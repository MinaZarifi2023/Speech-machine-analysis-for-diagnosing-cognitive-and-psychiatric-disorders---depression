{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Xa2oxAH1qCNp",
        "o3u3hicusOkw",
        "67LCCovDsSBu",
        "59ohhDsUp9Fr",
        "P-tEfCvlqbor",
        "Tgm9rN1o0bJ2",
        "IYkaod66mSdl",
        "bWpEUw3D4Kkd",
        "i254poJds7pa",
        "cYj-uulTAGkN",
        "jKl6HNRz6euW",
        "85jGPVaTtxdD",
        "2nSUfpyTmYyV",
        "3lvu2aL728Oz",
        "YY2oBKix3Aw6",
        "4dKUERTm3W3x",
        "7Scm9hA8_syY",
        "EZ3EcS67ED4j",
        "Nr8IAkdwyJgQ",
        "GW7p3eAUyT7Q",
        "fqH2h4f_yLAH",
        "jMaJW0d-nGHO",
        "sdBOLD556tbH",
        "E7s862os7AsU",
        "Mq6DONk_oH94",
        "10qvMJSQCpVR",
        "iAQDif1UBlbS",
        "5X-hfD5tBlbe",
        "SvNJLz7pZ2QC",
        "1lxBEw27XBc1",
        "nlUt_NxJV5ap",
        "dkJ9L1jpX2As",
        "2LCYY1fZZIzx",
        "s6MHkZtravQF",
        "tl0ID-XnTU4G",
        "m1E_EYIzIe38",
        "nI-71VIKZA2r",
        "u5B3rS_MJUeJ",
        "6-pqgulDrN9G",
        "Hd-uB-mA0ZIi",
        "sgX-GD_aK9qz",
        "MTKcDmxri1DL",
        "FYVW-WD6gd7J",
        "grZh8lnVwS6y",
        "0WAWx2o1-FLl",
        "Wm3chk0yHcaB",
        "WnBr4dXPiVgg",
        "hF2NhApCKum6",
        "VCXJSW955Aqp",
        "ZfT_e7tWHJQ4",
        "TDcfu6dNuiX2",
        "_boPR-SR5p3o",
        "s8tOqihZ6aEh",
        "_yemOeEM6aEs",
        "_CXYxAuI3p0q",
        "ac28-Ccj3tTg",
        "eM11OmMfIB9I",
        "FDgOG2Ogl0n1",
        "_PKGAKc0l0n3",
        "jVIrb1nngNV5",
        "zqCTW1xwl0n5",
        "F3OdR9Qpl0n7",
        "Ydws7phol0n9",
        "JsR4Y1Avl0oA",
        "29u5Li5K-1-a",
        "Pr7wNtpjFikr",
        "v9KhYClZFikw",
        "LBlUoJDiFikz",
        "Yq93B3q6lRnX",
        "yT_VuiI-qowH",
        "gop1yd7W2kXz",
        "j3nbppqHmEid",
        "kgDx2JGjrJmk",
        "SPCe_Xj1v3N7",
        "BMDDtpBMEh3g",
        "NW13AlPSEh3x",
        "rqrnSalL2sgB",
        "XSB5vEQt2sgC",
        "fjpneHyR2sgO",
        "yRX9NHK02sgY",
        "sy_1Mvzv2sgY",
        "hqt9B3qD2sgp",
        "fZpVH-4Q2sgx",
        "eKjWNcqp2sgx",
        "fz5mK69K2sg2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1.Create only image CSV"
      ],
      "metadata": {
        "id": "Xa2oxAH1qCNp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoZ3SExPo8mv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3u3hicusOkw"
      },
      "source": [
        "### mp3ToWav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiVamRv-BtYb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Set the path to the input MP3 folder\n",
        "input_folder = '/content/gdrive/MyDrive/prj_data/new_data/'   #/content/gdrive/MyDrive/prj_data/new data/\n",
        "\n",
        "# Set the path to the output WAV folder\n",
        "output_folder = '/content/gdrive/MyDrive/wav_files/new_data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HaZlLUGoIHy"
      },
      "outputs": [],
      "source": [
        "# # **************** run just one time for make wave files ***************\n",
        "\n",
        "# Get a list of all the MP3 files in the input folder\n",
        "mp3_files = [f for f in os.listdir(input_folder) if f.endswith('.mp3')]\n",
        "\n",
        "# Loop over each MP3 file and convert it to WAV\n",
        "for mp3_file in mp3_files:\n",
        "    mp3_path = os.path.join(input_folder, mp3_file)\n",
        "    wav_path = os.path.join(output_folder, os.path.splitext(mp3_file)[0] + '.wav')\n",
        "    subprocess.run(['ffmpeg', '-i', mp3_path, wav_path])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67LCCovDsSBu"
      },
      "source": [
        "## speech to text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOIg988GsUx2"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install g++ git make zlib1g-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-PYE9ggyjWW"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/kaldi-asr/kaldi.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2byPYewxym2b"
      },
      "outputs": [],
      "source": [
        "%cd kaldi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffpj7G6QyqBF"
      },
      "outputs": [],
      "source": [
        "!cd tools && make && cd ..\n",
        "!./configure --shared --mathlib=ATLAS --use-cuda=no\n",
        "!make depend -j $(nproc)\n",
        "!make -j $(nproc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "edZeoSvxy-dI"
      },
      "outputs": [],
      "source": [
        "# !wget 'https://github.com/rhasspy/fa_kaldi-rhasspy/releases/download/v1.0/vosk-model-small-fa-rhasspy-0.15.zip'\n",
        "# !unzip vosk-model-small-fa-rhasspy-0.15.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QhwqSzt0-89"
      },
      "outputs": [],
      "source": [
        "!pip install kaldi_io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GcjelcR3oHK"
      },
      "outputs": [],
      "source": [
        "!pip install vosk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUebLNLm4G0d"
      },
      "outputs": [],
      "source": [
        "!export KALDI_ROOT=/content/kaldi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtKvzXbG4Oea"
      },
      "outputs": [],
      "source": [
        "!source ~/.bashrc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHHHbY154skO"
      },
      "outputs": [],
      "source": [
        "!wget 'https://github.com/rhasspy/fa_kaldi-rhasspy/releases/download/v1.0/vosk-model-small-fa-rhasspy-0.15.zip'\n",
        "!unzip vosk-model-small-fa-rhasspy-0.15.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbPSMsTgplFX"
      },
      "outputs": [],
      "source": [
        "import wave\n",
        "import os\n",
        "import wave\n",
        "import json\n",
        "import kaldi_io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from vosk import Model, KaldiRecognizer\n",
        "\n",
        "# Set the paths to the acoustic and language models\n",
        "acoustic_model_path = \"/content/gdrive/MyDrive/prj_data/kaldi/vosk-model-small-fa-rhasspy-0.15\"\n",
        "lang_model_path = \"/content/gdrive/MyDrive/prj_data/kaldi/vosk-model-small-fa-rhasspy-0.15/graph/Gr.fst\"\n",
        "\n",
        "# Create an empty DataFrame to store the recognition results\n",
        "results_df = pd.DataFrame(columns=['file_name', 'transcript'])\n",
        "\n",
        "\n",
        "# Loop through each WAV file in the directory\n",
        "wav_dir = \"/content/gdrive/MyDrive/wav_files/new_data/\"\n",
        "for file_name in os.listdir(wav_dir):\n",
        "    if file_name.endswith(\".wav\"):\n",
        "        # Load the WAV file for recognition\n",
        "        audio_file_path = os.path.join(wav_dir, file_name)\n",
        "        audio_file = wave.open(audio_file_path, 'rb')\n",
        "\n",
        "        # Get the sample rate (framerate)\n",
        "        sample_rate = audio_file.getframerate()\n",
        "\n",
        "        # Load the acoustic and language models\n",
        "        acoustic_model = Model(acoustic_model_path)\n",
        "        lang_model = KaldiRecognizer(acoustic_model, sample_rate)\n",
        "\n",
        "        # Perform speech recognition on the audio file\n",
        "        while True:\n",
        "            # Read a chunk of audio data from the file\n",
        "            audio_data = audio_file.readframes(4000)\n",
        "\n",
        "            # Break out of the loop when there's no more audio data\n",
        "            if len(audio_data) == 0:\n",
        "                break\n",
        "\n",
        "            # Convert the audio data to a numpy array\n",
        "            audio_numpy = np.frombuffer(audio_data, dtype=np.int16)\n",
        "\n",
        "            # Feed the audio data to the recognizer\n",
        "            lang_model.AcceptWaveform(audio_numpy.tobytes())\n",
        "\n",
        "        # Get the final recognition results\n",
        "        result = lang_model.FinalResult()\n",
        "\n",
        "        # Append the recognition result to the DataFrame\n",
        "        results_df = results_df.append({'file_name': file_name, 'transcript': result}, ignore_index=True)\n",
        "\n",
        "# Print the recognition results\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.sort_values(\"file_name\", axis=0, ascending=True,inplace=True, na_position='first')\n",
        "results_df"
      ],
      "metadata": {
        "id": "xkvoXzogltRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=results_df\n",
        "df['transcript'] = df['transcript'].str.replace('{\\n  \"text\" : ', '')\n",
        "df['transcript'] = df['transcript'].str.replace('}', '')\n",
        "df['file_name'] = df['file_name'].str.replace('_story1_study1.wav', '')\n",
        "df"
      ],
      "metadata": {
        "id": "vpLgRkLp6bDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"/content/gdrive/MyDrive/out.csv\")"
      ],
      "metadata": {
        "id": "wjLywEjYlvjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #concat first data and new data\n",
        "# import pandas as pd\n",
        "# df = pd.read_csv(\"/content/gdrive/MyDrive/out.csv\", index_col=[0])\n",
        "# df2=pd.read_csv(\"/content/gdrive/MyDrive/first_out.csv\", index_col=[0])\n",
        "\n",
        "# frames = [df, df2]\n",
        "# dataset =  pd.concat(frames,ignore_index=True)\n",
        "# dataset.to_csv(\"/content/gdrive/MyDrive/project_dataset.csv\")\n",
        "# dataset.to_excel(\"/content/gdrive/MyDrive/project_dataset.xlsx\")\n",
        "# dataset"
      ],
      "metadata": {
        "id": "RmVrvQtK_V6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "split id and timeStamp from file name"
      ],
      "metadata": {
        "id": "4gOs8v8yXUP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.DataFrame(df['file_name'])\n",
        "df3 = df2['file_name'].str.split(\"_\", expand = True)\n",
        "df3.columns = ['STATUS_ID{}'.format(x+1) for x in df3.columns]\n",
        "# print (df3)\n",
        "#   STATUS_ID1 STATUS_ID2\n",
        "# 0  48xdti2f  1681027836\n",
        "# 1  48xdti2f  1681028046\n",
        "df[['id', 'TimeStamp']] = df3[['STATUS_ID1', 'STATUS_ID2']]\n",
        "df.to_csv(\"/content/gdrive/MyDrive/project_dataset_split.csv\")\n",
        "df.to_excel(\"/content/gdrive/MyDrive/project_dataset_split.xlsx\")\n",
        "df"
      ],
      "metadata": {
        "id": "b7uiAbRqP9rN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preproccess"
      ],
      "metadata": {
        "id": "59ohhDsUp9Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "GEPB5E4Nqu8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel(\"/content/gdrive/MyDrive/project_dataset_split_clean.xlsx\", index_col=[0])\n",
        "df"
      ],
      "metadata": {
        "id": "XkHSbIU_BjyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### find count of audios for delete incomplete data( handy)\n",
        "and then must get just audios of pictures\n"
      ],
      "metadata": {
        "id": "spCGWuoNAQNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# groupby by id\n",
        "# df.groupby(\"id\", group_keys=True).apply(lambda x: x)"
      ],
      "metadata": {
        "id": "hMxsRLor-6Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # groupby by id\n",
        "# gk = df.groupby(\"id\")\n",
        "# # show first row of each subject\n",
        "# gk.first()"
      ],
      "metadata": {
        "id": "xkZH_bT7aiQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #get one specific id\n",
        "# gk.get_group('00i15xzd')"
      ],
      "metadata": {
        "id": "XSFNofBhbFPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only run this cell\n",
        "# compute count of audio of every subject\n",
        "counter = df.groupby(\"id\")[\"TimeStamp\"].count()\n",
        "temp= pd.DataFrame(counter).reset_index()\n",
        "temp.rename(columns={\"TimeStamp\": \"count_of_audios\"}, inplace=True)\n",
        "print(temp)\n",
        "\n",
        "df = df.join(temp.set_index('id'), on='id')"
      ],
      "metadata": {
        "id": "vNMF0ye164Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  process uncomplete data\n",
        "counter = df.groupby(\"count_of_audios\")[\"id\"]\n",
        "counter.first()"
      ],
      "metadata": {
        "id": "NrCQ1G-v-s9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# counter = df.groupby(\"count_of_audios\")[\"id\"].count()\n",
        "# temp= pd.DataFrame(counter).reset_index()\n",
        "# temp['id']=temp['id']/temp['count_of_audios']\n",
        "# temp.rename(columns={\"id\": \"count_of_subject\"}, inplace=True)\n",
        "# temp\n",
        "# #(there is 106 person that they have 11 audio )\n",
        "# count_of_audios\tcount_of_subject\n",
        "# 0\t            3\t3.0\n",
        "# 1\t            4\t1.0\n",
        "# 2\t            5\t1.0\n",
        "# 3\t            7\t2.0\n",
        "# 4\t            8\t2.0\n",
        "# 5\t            9\t2.0\n",
        "# 6\t            10\t10.0\n",
        "# 7\t            11\t106.0\n",
        "# 8\t            13\t1.0\n",
        "# 9\t            14\t1.0\n",
        "# 10\t          22\t3.0"
      ],
      "metadata": {
        "id": "yQ_0N3MRDani"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # delete uncomplete data one by one\n",
        "# # groupby by count_of_audios\n",
        "# group_count = df.groupby(\"count_of_audios\")\n",
        "# # get one specific id\n",
        "# group_count.get_group(3) #2gwcrwvh ,33gukzxe,p8pzxt7r\n",
        "# group_count.get_group(4) #3bjlx7hv\n",
        "# group_count.get_group(5)  #iw7hcndg\n",
        "# group_count.get_group(7) #nng00uab ,ntmp3ned\n",
        "# group_count.get_group(8) #vqelw7vi , (no64afbg ok)\n",
        "# group_count.get_group(9) #r6gltob9 , (v4vf1g92 ok)\n",
        "# group_count.get_group(10) #ua32y4cb ,['08oi5ot0' '4gqdsfum' 'fb1t8fvc' 'g7gmcvts' 'it0clbqf' 'kzlrvd2x'  'qdolnbxs' 'suia995m' 'syy20boc' ] ok\n",
        "# group_count.get_group(22)\n",
        "\n",
        "# # How to list unique id in pd.unique(group_count.get_group(10)\n",
        "# len(pd.unique(group_count.get_group(8)['id']))"
      ],
      "metadata": {
        "id": "6AuuYJpgF2cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#delete 3first rows about past,naw,future and just keep 8 audios about pictures\n",
        "gb11 = df.groupby(\"count_of_audios\").get_group(11)\n",
        "only_images = gb11.drop(gb11.groupby(\"id\").head(3).index, axis=0)\n",
        "\n",
        "#add data with 8 audios\n",
        "gb8 = df.groupby(\"count_of_audios\").get_group(8)\n",
        "result = pd.concat([only_images, gb8])\n",
        "result.to_csv(\"/content/gdrive/MyDrive/project_dataset_split_clean_OnlyImages.csv\")\n",
        "result"
      ],
      "metadata": {
        "id": "TefYGc93NzEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### read only images csv"
      ],
      "metadata": {
        "id": "PUrMp8EJXKGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "vlZ1D_ojYRdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_excel(\"/content/gdrive/MyDrive/project_dataset_split_clean_OnlyImages1.xlsx\")\n",
        "df"
      ],
      "metadata": {
        "id": "zBoljge9fAL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.compute BDI"
      ],
      "metadata": {
        "id": "gdY1ugV94AYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "if you want to do it multi times, you must run first block and then next block, but if you want to run it just one times justrun first block\n",
        "## first block: read images csv and first BDI"
      ],
      "metadata": {
        "id": "P-tEfCvlqbor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "PlwXsFeZh0S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_excel(\"/content/gdrive/MyDrive/project_dataset_split_clean_OnlyImages1.xlsx\")\n",
        "df"
      ],
      "metadata": {
        "id": "viwB1ILTh0S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### read and clean BDI"
      ],
      "metadata": {
        "id": "Tgm9rN1o0bJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BDI1 =pd.read_csv(\"/content/BDI_story1.csv\",header=None)\n",
        "BDI1.head(2)"
      ],
      "metadata": {
        "id": "8KXokGSyh0S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get first character\n",
        "for i in range(2,23):\n",
        "  BDI1[i] = BDI1[i].str[0]\n",
        "\n",
        "# Converting Multiple columns to int\n",
        "for i in range (2,23):\n",
        "  BDI1 = BDI1.astype({i:\"int\"})\n",
        "\n",
        "#compute scores\n",
        "sum=0\n",
        "BDI1[\"BDI_score\"] = pd.Series(dtype='int')\n",
        "\n",
        "for j in range(1,BDI1.shape[0]-1):\n",
        "  for i in range(2,BDI1.shape[1]-1):\n",
        "    sum = sum + BDI1[i][j]\n",
        "  BDI1[\"BDI_score\"][j] = sum\n",
        "  sum=0\n",
        "\n",
        "#drop useless columns\n",
        "for i in range(2,23):\n",
        "  BDI1.drop([i], axis=1, inplace=True)\n",
        "BDI1 = BDI1.drop([0])\n",
        "BDI1.rename(columns = {0:'BDI_timestamp',1:'id' }, inplace = True)\n",
        "BDI1"
      ],
      "metadata": {
        "id": "zYS2OjlZh0TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#delete duplicated rows and null values raws\n",
        "temp = BDI1.sort_values('id', ascending=False)\n",
        "BDI1 = temp.drop_duplicates(subset='id', keep='first').dropna(axis=0)\n",
        "BDI1"
      ],
      "metadata": {
        "id": "_TP62eJkDBAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fix changed id Handy\n",
        "BDI1 = BDI1.replace('اذر', 'lsyku5px')\n",
        "BDI1 = BDI1.replace('مینا', '8kfkpopv')"
      ],
      "metadata": {
        "id": "Mk1LJNFsHDbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #see row from a specific id\n",
        "# BDI1.loc[BDI1['id']=='10xj3h72']"
      ],
      "metadata": {
        "id": "GaJdKpWW6POx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### merge df and BDI"
      ],
      "metadata": {
        "id": "XRcBoTD50mfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape , BDI1.shape)"
      ],
      "metadata": {
        "id": "A9vdrI2Jh0TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "left_merged1 = pd.merge(df, BDI1, how=\"left\", on=[\"id\"])\n",
        "left_merged1.shape"
      ],
      "metadata": {
        "id": "kIXgY7sah0TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##second block:read only second BDI csv"
      ],
      "metadata": {
        "id": "IYkaod66mSdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BDI =pd.read_csv(\"/content/BDI_bonyad01.csv\",header=None)\n",
        "BDI.head(2)"
      ],
      "metadata": {
        "id": "6pClSJyvgqbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2,23):\n",
        "  BDI[i] = BDI[i].str[0]\n",
        "\n",
        "# Converting Multiple columns to int\n",
        "for i in range (2,23):\n",
        "  BDI = BDI.astype({i:\"int\"})\n",
        "\n",
        "#compute BDI score\n",
        "sum=0\n",
        "BDI[\"BDI_score\"] = pd.Series(dtype='int')\n",
        "\n",
        "for j in range(1,BDI.shape[0]-1):\n",
        "  for i in range(2,BDI.shape[1]-1):\n",
        "    sum = sum + BDI[i][j]\n",
        "  BDI[\"BDI_score\"][j] = sum\n",
        "  sum=0\n",
        "\n",
        "#delete extra columns\n",
        "for i in range(2,23):\n",
        "  BDI.drop([i], axis=1, inplace=True)\n",
        "BDI = BDI.drop([0])\n",
        "BDI.rename(columns = {0:'BDI_timestamp',1:'id' }, inplace = True)\n",
        "BDI.shape"
      ],
      "metadata": {
        "id": "3ZRj8QgS2BVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = BDI.sort_values('id', ascending=False)\n",
        "BDI = temp.drop_duplicates(subset='id', keep='first')"
      ],
      "metadata": {
        "id": "Uln6JOKIET6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape , BDI.shape)"
      ],
      "metadata": {
        "id": "g6oLT62s4J33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "left_merged = pd.merge(df, BDI, how=\"left\", on=[\"id\"])\n",
        "left_merged.shape"
      ],
      "metadata": {
        "id": "neqDObFH2i75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with pd.option_context('display.max_colwidth', None):\n",
        "#   display(gk.get_group('zatfknoo'))"
      ],
      "metadata": {
        "id": "1bJmcJoJ-fH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### merge first subjects and second subjects and delete Nan from final df"
      ],
      "metadata": {
        "id": "tfmt3Mfv4JEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(left_merged1.shape , left_merged.shape) #(992, 7) (984, 7)\n",
        "finaldf = pd.concat([left_merged1, left_merged]).dropna(axis=0)\n",
        "finaldf"
      ],
      "metadata": {
        "id": "sTV0iDdl2Vvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IDs that is in df but is not in final df\n",
        "#they are not completed BDI -text them!\n",
        "df['id'][~df['id'].isin(finaldf['id'])].unique()\n",
        "\n",
        "# array(['10xj3h72', '2iaym4x3', '48xdti2f', '58ysvi8n', '8kfkpopv',\n",
        "#        '913dtzkj', 'c5l9xs7r', 'de2k1rwl', 'dvw4pmpg', 'evgj8hnz',\n",
        "#        'ff7sbzen', 'gssd0zh7', 'it0clbqf', 'kal5st0v', 'kegh0d5c',\n",
        "#        'lr4xk13r', 'lsyku5px', 'n2geu19i', 'qmodslv6', 'szngczd2',\n",
        "#        'v0vg2kj7', 'wijj2zsc', 'yq9g7cr4'], dtype=object)\n",
        "\n",
        "#null :913dtzkjوkal5st0vوkegh0d5cوlr4xk13r"
      ],
      "metadata": {
        "id": "qJIyKIrs90rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # see row from a specific id\n",
        "# with pd.option_context('display.max_colwidth', None):\n",
        "#   display(df.loc[df['id']=='qmodslv6'])"
      ],
      "metadata": {
        "id": "if7vUv3B7et_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###check count of audios\n",
        "check doubles row in BDI"
      ],
      "metadata": {
        "id": "1yar_pSUYdLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # only run this cell\n",
        "# # compute count of audio of every subject\n",
        "# counter = finaldf.groupby(\"id\")[\"TimeStamp\"].count()\n",
        "# temp= pd.DataFrame(counter).reset_index()\n",
        "# temp.rename(columns={\"TimeStamp\": \"count_of_audios\"}, inplace=True)\n",
        "# print(temp)\n",
        "# finaldf = finaldf.join(temp.set_index('id'), on='id')\n",
        "\n",
        "# #check if have duplicate id\n",
        "# counter = finaldf.groupby(\"count_of_audios\")[\"id\"]\n",
        "# counter.first()"
      ],
      "metadata": {
        "id": "JTKlvJbS9_8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finaldf.to_excel(\"/content/gdrive/MyDrive/project_dataset_with_BDIscores.xlsx\")\n",
        "finaldf.to_csv(\"/content/gdrive/MyDrive/project_dataset_with_BDIscores.csv\")"
      ],
      "metadata": {
        "id": "7efrAtSGTVJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. categorization"
      ],
      "metadata": {
        "id": "bWpEUw3D4Kkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "s_V2n4itrrcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##categories labels"
      ],
      "metadata": {
        "id": "i254poJds7pa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total Score ...................... Levels of Depression\n",
        "\n",
        "1-10____________________These ups and downs are considered normal\n",
        "\n",
        "\n",
        "11-16___________________ Mild mood disturbance\n",
        "\n",
        "\n",
        "17-20___________________Borderline clinical depression\n",
        "\n",
        "\n",
        "21-30___________________Moderate depression\n",
        "\n",
        "\n",
        "31-40___________________Severe depression\n",
        "\n",
        "\n",
        "over 40__________________Extreme depression"
      ],
      "metadata": {
        "id": "2Caz9y3eqv0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/project_dataset_with_BDIscores.csv')\n",
        "data"
      ],
      "metadata": {
        "id": "wMYgC8V8tUMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the bins and labels for categorization\n",
        "bins = [0, 10, 16, 20, 30, 40,63]  ###################check this later\n",
        "labels = ['normal', 'mild_dep', 'borderline_dep', 'moderate_dep', 'severe_dep','extreme_dep']\n",
        "\n",
        "# Use pd.cut() to categorize the continuous labels into the specified categories\n",
        "data['category'] = pd.cut(data['BDI_score'], bins=bins, labels=labels)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "data"
      ],
      "metadata": {
        "id": "K4nweGUw2lJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['category_numeric'] = pd.factorize(data['category'])[0]\n",
        "data.to_csv('/content/gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd.csv', index=False)\n",
        "# df=data"
      ],
      "metadata": {
        "id": "3UH80nvP32bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## groupby id"
      ],
      "metadata": {
        "id": "cYj-uulTAGkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"/content/gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd.csv\").drop(['Unnamed: 0','count_of_audios'], axis=1)\n",
        "df"
      ],
      "metadata": {
        "id": "veT-wYNK46Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by the 'id' column and compute the length of each group\n",
        "grouped_df = df.groupby('id').apply(lambda x: len(x)).reset_index()\n",
        "\n",
        "# Format the length column as xx.xx.xx\n",
        "# Group by the 'id' column and compute the sum of the 'length' column for each group\n",
        "grouped_df = df.groupby('id')['length'].apply(lambda x: x.astype(str).str.replace('.', '').astype(int).sum()).reset_index()\n",
        "grouped_df"
      ],
      "metadata": {
        "id": "XSctXX3QEM1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Merge the grouped DataFrame with the original DataFrame\n",
        "\n",
        "result_df = pd.merge(df, grouped_df, on='id')\n",
        "result_df"
      ],
      "metadata": {
        "id": "BK1Iix_EafOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert second to minute\n",
        "def converted(i):\n",
        "  print(i)\n",
        "  # Define the number of seconds\n",
        "  seconds = i\n",
        "  # Convert the seconds to hours, minutes, and seconds\n",
        "  hours, seconds = divmod(seconds, 60 ** 2)\n",
        "  minutes, seconds = divmod(seconds, 60)\n",
        "  # Format the resulting hours, minutes, and seconds as xx.xx.xx\n",
        "  formatted_time = '{:02d}.{:02d}.{:02d}'.format(hours, minutes, seconds)\n",
        "  return(formatted_time)"
      ],
      "metadata": {
        "id": "gMfo7G9wOSjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(grouped_df.shape[0]):\n",
        "  grouped_df.loc[j, 'total_length'] = converted(grouped_df.loc[j, 'length'])"
      ],
      "metadata": {
        "id": "JsHCt6f0aYN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_df"
      ],
      "metadata": {
        "id": "2wZJbupercWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = pd.merge(df, grouped_df, on='id')\n",
        "result_df.to_csv('/content/gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd_ComputedLength_MergedTranscripted_all.csv', index=False)\n",
        "result_df"
      ],
      "metadata": {
        "id": "xp4txvV2rcMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge transcripts together with \"|\"\n",
        "gk = result_df.groupby('id')['transcript'].apply(lambda x: '|'.join(x)).reset_index()\n",
        "temp = pd.merge(result_df, gk, on='id')\n",
        "categorized = temp.groupby(['id','total_length','category_numeric']).first().drop(columns=['file_name',\n",
        "                                                                             'length_x',\n",
        "                                                                             'BDI_timestamp',\n",
        "                                                                             'BDI_score',\n",
        "                                                                             'length_y',\n",
        "                                                                             'transcript_x'])\n",
        "categorized"
      ],
      "metadata": {
        "id": "IrW2dOJ1sanm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorized.to_csv('/content/gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd_ComputedLength_MergedTranscripted.csv', index=False)"
      ],
      "metadata": {
        "id": "SX6iND4LsO5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " result_df #is all\n",
        "\n",
        "\n",
        " categorized #is grouped_by"
      ],
      "metadata": {
        "id": "EE8sEt4Zs1Ea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###vectorised groupbied data"
      ],
      "metadata": {
        "id": "jKl6HNRz6euW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "# result_df =pd.read_csv('/content/gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd_ComputedLength_MergedTranscripted_all.csv')\n",
        "categorized =pd.read_csv('/content/gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd_ComputedLength_MergedTranscripted.csv')"
      ],
      "metadata": {
        "id": "lsSS5VGn5z3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "-tee45MA_op9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install python-pip\n",
        "!pip install hazm"
      ],
      "metadata": {
        "id": "5CRVNBRztZAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals\n",
        "from hazm import *"
      ],
      "metadata": {
        "id": "zMDZsxtcty9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "tiWW-9EIwa_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorized"
      ],
      "metadata": {
        "id": "8e5VqUnwGQsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load the ParsBERT tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "model = AutoModel.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "\n",
        "# Define a function to vectorize the text\n",
        "def vectorize_text(text):\n",
        "    # Tokenize the text\n",
        "    encoded_input = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "    # Pass the tokenized input through the model\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    # Extract the last hidden state of the model as the vector representation of the text\n",
        "    vector = model_output.last_hidden_state[:, 0, :].squeeze().tolist()\n",
        "    return vector\n"
      ],
      "metadata": {
        "id": "jqyoCJCQKf5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = categorized\n",
        "\n",
        "# Vectorize the text in the dataset\n",
        "vectors = []\n",
        "for text in dataset['transcript_y']:\n",
        "    vector = vectorize_text(text)\n",
        "    vectors.append(vector)\n",
        "\n",
        "# Add the vectors as columns to the dataset\n",
        "vector_columns = ['vector_' + str(i) for i in range(len(vectors[0]))]\n",
        "for i, column in enumerate(vector_columns):\n",
        "    dataset[column] = [vector[i] for vector in vectors]\n",
        "\n",
        "dataset.shape # shape : (118, 773)"
      ],
      "metadata": {
        "id": "olMuSPpr7fnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the dataset to a CSV file\n",
        "dataset.to_csv('/content/gdrive/MyDrive/vectorized_dataset.csv', index=False)\n",
        "#The size of the vector representation (embedding) generated by ParsBERT is 768."
      ],
      "metadata": {
        "id": "b2OjJkRnDoS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. vectorized - analysis"
      ],
      "metadata": {
        "id": "85jGPVaTtxdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from all"
      ],
      "metadata": {
        "id": "JwtqLpbkty1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "result_df =pd.read_csv('/content/gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd_ComputedLength_MergedTranscripted_all.csv')"
      ],
      "metadata": {
        "id": "l_ZFr2Q0ty1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = result_df.drop(columns=['file_name',\n",
        "                                    'BDI_timestamp',\n",
        "                                    'count_of_audios',\n",
        "                                    'length_y','Unnamed: 0'])\n",
        "result_df"
      ],
      "metadata": {
        "id": "OWwtjSScDqo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##vectorised with parsbert"
      ],
      "metadata": {
        "id": "zQfXxc1ZEbbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!sudo apt install python-pip\n",
        "!pip install hazm"
      ],
      "metadata": {
        "id": "adLdfxhKty1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import torch"
      ],
      "metadata": {
        "id": "01aaB2TZty1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use GPU\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load the ParsBERT tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "model = AutoModel.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")"
      ],
      "metadata": {
        "id": "M1QXI_vHty1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to vectorize the text\n",
        "def vectorize_text(text):\n",
        "    # Tokenize the text\n",
        "    encoded_input = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "    # Pass the tokenized input through the model\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    # Extract the last hidden state of the model as the vector representation of the text\n",
        "    vector = model_output.last_hidden_state[:, 0, :].squeeze().tolist()\n",
        "    return vector"
      ],
      "metadata": {
        "id": "yLHGSUTbFhqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = result_df\n",
        "\n",
        "# Vectorize the text in the dataset\n",
        "vectors = []\n",
        "for text in dataset['transcript']:\n",
        "    vector = vectorize_text(text)\n",
        "    vectors.append(vector)\n",
        "\n",
        "# Add the vectors as columns to the dataset\n",
        "vector_columns = ['vector_' + str(i) for i in range(len(vectors[0]))]\n",
        "for i, column in enumerate(vector_columns):\n",
        "    dataset[column] = [vector[i] for vector in vectors]\n",
        "\n",
        "dataset.shape"
      ],
      "metadata": {
        "id": "_Gv3m203ty1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the dataset to a CSV file\n",
        "vectorized_all_dataset = dataset\n",
        "vectorized_all_dataset.to_csv('/content/gdrive/MyDrive/vectorized_all_dataset.csv', index=False)\n",
        "#The size of the vector representation (embedding) generated by ParsBERT is 768."
      ],
      "metadata": {
        "id": "Wr--r9YKty1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarit Marix"
      ],
      "metadata": {
        "id": "2nSUfpyTmYyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "vectorized_all_dataset = pd.read_csv('/content/gdrive/MyDrive/vectorized_all_dataset.csv')"
      ],
      "metadata": {
        "id": "yNCBxabrjx7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "just_vectors = vectorized_all_dataset.iloc[:, 8:]\n",
        "just_vectors"
      ],
      "metadata": {
        "id": "k34XKs8ur2_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(just_vectors)\n",
        "\n",
        "# print the similarity matrix\n",
        "similarity_matrix.shape"
      ],
      "metadata": {
        "id": "5lrN5DGfj5ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(just_vectors)\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(similarity_matrix, cmap='coolwarm')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# set the x and y ticks every 80 rows and columns\n",
        "ax.set_xticks(np.arange(0, 952, 80))\n",
        "ax.set_yticks(np.arange(0, 952, 80))\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"Similarity Matrix\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "47wNQ-9Vnzym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Similarity Matrix sort by images"
      ],
      "metadata": {
        "id": "3lvu2aL728Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# generate the selected_rows arrays using a list comprehension\n",
        "selected_rows = [np.arange(i, similarity_matrix.shape[0], 8) for i in range(8)]\n",
        "# print the selected_rows arrays\n",
        "for i, rows in enumerate(selected_rows):\n",
        "    selected_rows[i]=rows\n",
        "\n",
        "# generate the selected_cols arrays using a list comprehension\n",
        "selected_cols = [np.arange(i, similarity_matrix.shape[0], 8) for i in range(8)]\n",
        "# print the selected_cols arrays\n",
        "for i, cols in enumerate(selected_cols):\n",
        "    selected_cols[i]=cols\n",
        "\n",
        "# generate the selected_matrix arrays using nested list comprehensions\n",
        "print('selected_rows:',selected_rows)\n",
        "print('selected_cols:',selected_cols)\n",
        "selected_matrix = [[similarity_matrix[rows][:, cols] for cols in selected_cols] for rows in selected_rows]\n",
        "# concatenate the selected_matrix arrays into a single DataFrame\n",
        "selected_matrix_df = pd.concat([pd.DataFrame(np.hstack(m)) for m in selected_matrix])\n",
        "print( 'selected_matrix_df.shape:', selected_matrix_df.shape)\n"
      ],
      "metadata": {
        "id": "C9owyv0NYWdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(selected_matrix_df,cmap='coolwarm')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# set the x and y ticks every 80 rows and columns\n",
        "ax.set_xticks(np.arange(0, 952, 119))\n",
        "ax.set_yticks(np.arange(0, 952, 119))\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"Similarity Matrix sort by images\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s7IxfwGqMex0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### drow lines"
      ],
      "metadata": {
        "id": "YY2oBKix3Aw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = selected_matrix_df\n",
        "\n",
        "# create a figure and axis object\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# plot the DataFrame\n",
        "im = ax.imshow(df, cmap='coolwarm')\n",
        "\n",
        "# set the x and y ticks and labels\n",
        "for i in range(8):\n",
        "    ax.axhline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.axvline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.text((i+0.5)*119, -50, f'pic{i+1}', ha='center', fontsize=10)\n",
        "    ax.text(-100, (i+0.5)*119, f'pic{i+1}', va='center', fontsize=10)\n",
        "\n",
        "# remove the x and y ticks\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "# add a colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title below the figure\n",
        "fig.suptitle('Similarity Matrix sort by pictures', fontsize=16, y=0.05, va='center')\n",
        "\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xFEni_HCwMT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sort just by bdi and then show the similarity matrix"
      ],
      "metadata": {
        "id": "4dKUERTm3W3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# create a sample DataFrame\n",
        "df = vectorized_all_dataset\n",
        "\n",
        "# sort the DataFrame by the 'salary' column in descending order\n",
        "df_sorted = df.sort_values(['BDI_score','id'])\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(df_sorted.iloc[:, 8:])\n",
        "\n",
        "# print the similarity matrix\n",
        "similarity_matrix.shape"
      ],
      "metadata": {
        "id": "KQfpSamz3YnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# create a sample DataFrame with 10 rows and 10 columns\n",
        "df = df_sorted[['BDI_score','id']]\n",
        "\n",
        "# set the display options to show all rows and columns\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# print the DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "id": "2l2eD3EP8mJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(similarity_matrix, cmap='coolwarm')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# set the x and y ticks every 80 rows and columns\n",
        "ax.set_xticks(np.arange(0, 952, 80))\n",
        "ax.set_yticks(np.arange(0, 952, 80))\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"Similarity Matrix sorted by BDI score from little to much\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-BKhZx_u4rNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sort by length of each audio and then show the similarity matrix"
      ],
      "metadata": {
        "id": "7Scm9hA8_syY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sorted.columns"
      ],
      "metadata": {
        "id": "IyG5uch-_29X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# create a sample DataFrame\n",
        "df = vectorized_all_dataset\n",
        "\n",
        "# sort the DataFrame by the 'salary' column in descending order\n",
        "df_sorted = df.sort_values(['length_x'])\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(df_sorted.iloc[:, 8:])\n",
        "\n",
        "# print the similarity matrix\n",
        "similarity_matrix.shape"
      ],
      "metadata": {
        "id": "8BWLDBD6_syZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(similarity_matrix, cmap='coolwarm')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# set the x and y ticks every 80 rows and columns\n",
        "ax.set_xticks(np.arange(0, 952, 80))\n",
        "ax.set_yticks(np.arange(0, 952, 80))\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"Similarity Matrix sorted by length from little to much\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qEPbHEFw_sya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sort by total length and then show the similarity matrix"
      ],
      "metadata": {
        "id": "EZ3EcS67ED4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sorted.columns"
      ],
      "metadata": {
        "id": "jDbE7m6jED4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# create a sample DataFrame\n",
        "df = vectorized_all_dataset\n",
        "\n",
        "# sort the DataFrame by the 'salary' column in descending order\n",
        "df_sorted = df.sort_values(['total_length' , 'id'])\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(df_sorted.iloc[:, 8:])\n",
        "\n",
        "# print the similarity matrix\n",
        "similarity_matrix.shape"
      ],
      "metadata": {
        "id": "ZZf6cHvxED4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a sample DataFrame with 10 rows and 10 columns\n",
        "df = df_sorted[['total_length','id']]\n",
        "\n",
        "# set the display options to show all rows and columns\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# print the DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "id": "0umN4HNeESTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(similarity_matrix, cmap='coolwarm')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# set the x and y ticks every 80 rows and columns\n",
        "ax.set_xticks(np.arange(0, 952, 80))\n",
        "ax.set_yticks(np.arange(0, 952, 80))\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"Similarity Matrix sorted by total length from little to much\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zCCU4_JkED4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pca -plotting the similarity matrix in this reduced 2D space"
      ],
      "metadata": {
        "id": "Nr8IAkdwyJgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# create a matrix of vectors\n",
        "vectors = just_vectors\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(vectors)\n",
        "\n",
        "# reduce the dimensionality of the similarity matrix using PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_matrix = pca.fit_transform(similarity_matrix)\n",
        "\n",
        "# plot the reduced matrix in a 2D scatter plot\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.scatter(pca_matrix[:, 0], pca_matrix[:, 1], s=10)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"Similarity Matrix (PCA)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5ZMOOVmpqcQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "np.set_printoptions(threshold=sys.maxsize)"
      ],
      "metadata": {
        "id": "9ZPcqvfUfPhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##analyze"
      ],
      "metadata": {
        "id": "GW7p3eAUyT7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(just_vectors)"
      ],
      "metadata": {
        "id": "by08TmE6ycKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Look for clusters:\n",
        "  shows how to perform k-means clustering on a similarity matrix:"
      ],
      "metadata": {
        "id": "fqH2h4f_yLAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "# perform k-means clustering on the similarity matrix\n",
        "kmeans = KMeans(n_clusters=4, random_state=0).fit(similarity_matrix)\n",
        "\n",
        "# get the cluster labels\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# print the cluster labels\n",
        "print(labels)\n"
      ],
      "metadata": {
        "id": "XAGET1ZoyL4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# perform k-means clustering on the similarity matrix\n",
        "kmeans = KMeans(n_clusters=4, random_state=0).fit(similarity_matrix)\n",
        "\n",
        "# get the cluster labels\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# reduce the dimensionality of the similarity matrix using PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_matrix = pca.fit_transform(similarity_matrix)\n",
        "\n",
        "# plot the reduced matrix in a 2D scatter plot with the clusters\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "scatter = ax.scatter(pca_matrix[:, 0], pca_matrix[:, 1], c=labels, s=10)\n",
        "legend = ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Clusters\")\n",
        "ax.add_artist(legend)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"Similarity Matrix (K-Means Clustering)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "#The x-axis corresponds to the first principal component, and the y-axis corresponds to the second principal component."
      ],
      "metadata": {
        "id": "GCanttymzjPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will create a plot of the WCSS as a function of the number of clusters, where the \"elbow\" of the curve indicates the optimal number of clusters. here is 2"
      ],
      "metadata": {
        "id": "pYSf2AB7lP92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# create a dataset\n",
        "X = pca_matrix\n",
        "\n",
        "# calculate the within-cluster sum of squares (WCSS) for a range of cluster numbers\n",
        "wcss = []\n",
        "for i in range(1, 8):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
        "    kmeans.fit(X)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# plot the WCSS as a function of the number of clusters\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.plot(range(1, 8), wcss)\n",
        "ax.set_title('Elbow Method')\n",
        "ax.set_xlabel('Number of Clusters')\n",
        "ax.set_ylabel('WCSS')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vLKtERC92-Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# perform k-means clustering on the similarity matrix\n",
        "kmeans = KMeans(n_clusters=2, random_state=0).fit(similarity_matrix)\n",
        "\n",
        "# get the cluster labels\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# reduce the dimensionality of the similarity matrix using PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_matrix = pca.fit_transform(similarity_matrix)\n",
        "\n",
        "# plot the reduced matrix in a 2D scatter plot with the clusters\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "scatter = ax.scatter(pca_matrix[:, 0], pca_matrix[:, 1], c=labels, s=10)\n",
        "legend = ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Clusters\")\n",
        "ax.add_artist(legend)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"Similarity Matrix (K-Means Clustering)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "#The x-axis corresponds to the first principal component, and the y-axis corresponds to the second principal component."
      ],
      "metadata": {
        "id": "TCHT430xjjed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.Identify outliers:\n",
        " shows how to perform Local Outlier Factor LOF outlier detection on a similarity matrix:"
      ],
      "metadata": {
        "id": "jMaJW0d-nGHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "# # perform LOF outlier detection on the similarity matrix\n",
        "# lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
        "# outliers = lof.fit_predict(similarity_matrix)"
      ],
      "metadata": {
        "id": "HQrttF51lzAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.Calculate statistics:\n",
        " shows how to calculate the mean and standard deviation of a similarity matrix:"
      ],
      "metadata": {
        "id": "sdBOLD556tbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # calculate the mean and standard deviation of the similarity matrix\n",
        "# mean = np.mean(similarity_matrix)\n",
        "# std = np.std(similarity_matrix)\n",
        "\n",
        "# # print the mean and standard deviation\n",
        "# print(\"Mean:\", mean)\n",
        "# print(\"Standard deviation:\", std)\n"
      ],
      "metadata": {
        "id": "aFvzD7kS6wQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Compare subsets:\n",
        " shows how to compare two subsets of a similarity matrix:"
      ],
      "metadata": {
        "id": "E7s862os7AsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # select the first 500 rows and columns of the similarity matrix\n",
        "# subset1 = similarity_matrix[:500, :500]\n",
        "\n",
        "# # select the last 500 rows and columns of the similarity matrix\n",
        "# subset2 = similarity_matrix[-500:, -500:]\n",
        "\n",
        "# # calculate the mean similarity of each subset\n",
        "# mean1 = np.mean(subset1)\n",
        "# mean2 = np.mean(subset2)\n",
        "\n",
        "# # print the mean similarity of each subset\n",
        "# print(\"Mean similarity of subset 1:\", mean1)\n",
        "# print(\"Mean similarity of subset 2:\", mean2)\n"
      ],
      "metadata": {
        "id": "dDI-ETAY5dVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xfV0mevU_A8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. similarity matrix"
      ],
      "metadata": {
        "id": "Mq6DONk_oH94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**read data_with_linguistic_features**"
      ],
      "metadata": {
        "id": "vKvpqc_EgVhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# import pandas as pd\n",
        "\n",
        "data_with_linguistic_features = pd.read_csv('/content/gdrive/MyDrive/data_with_linguistic_features.csv')"
      ],
      "metadata": {
        "id": "rr2F8irRCdQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_with_linguistic_features.shape"
      ],
      "metadata": {
        "id": "ZopS7AmDoW0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. jump and SM and Corr"
      ],
      "metadata": {
        "id": "10qvMJSQCpVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##prepare data"
      ],
      "metadata": {
        "id": "4iDMV-Fch72V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # join without vectores\n",
        "# df1 = pd.read_csv('/content/gdrive/MyDrive/final_data_with_linguistic_features.csv')\n",
        "# jump = pd.read_csv('/content/gdrive/MyDrive/jump.csv', index_col=[0])\n",
        "\n",
        "# merged_df = pd.merge(df1 , jump, on=[\"file_name\",'id',] , how='left')\n",
        "# merged_df = merged_df.drop('transcript_y',axis=1)\n",
        "# merged_df.rename(columns = {'transcript_x':'transcript'}, inplace = True)\n",
        "\n",
        "# merged_df.to_csv('/content/gdrive/MyDrive/final_data_with_linguistic_features.csv', index=False)"
      ],
      "metadata": {
        "id": "N4KZBLtZVLeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5xRdqVega14"
      },
      "outputs": [],
      "source": [
        "# # join with vectores\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "jump =pd.read_csv('/content/gdrive/MyDrive/jump.csv', index_col=[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split id and timeStamp from file name\n",
        "df2 = pd.DataFrame(jump['file_name'])\n",
        "df3 = df2['file_name'].str.split(\"_\", expand = True)\n",
        "df3.columns = ['STATUS_ID{}'.format(x+1) for x in df3.columns]\n",
        "# print (df3)\n",
        "#   STATUS_ID1 STATUS_ID2\n",
        "# 0  48xdti2f  1681027836\n",
        "# 1  48xdti2f  1681028046\n",
        "jump[['id', 'TimeStamp']] = df3[['STATUS_ID1', 'STATUS_ID2']]\n",
        "jump"
      ],
      "metadata": {
        "id": "_H1dA4Pep910"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized_all_dataset = pd.read_csv('/content/gdrive/MyDrive/vectorized_all_dataset.csv')\n",
        "vectorized_all_dataset"
      ],
      "metadata": {
        "id": "GIyWwQ9hpbFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jump['TimeStamp']=jump['TimeStamp'].str.replace(r'.wav', '', regex=True)\n",
        "\n",
        "jump['TimeStamp']=jump['TimeStamp'].astype(int)\n",
        "vectorized_all_dataset['TimeStamp']=vectorized_all_dataset['TimeStamp'].astype(int)"
      ],
      "metadata": {
        "id": "uJnD4VZGtmZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.merge(jump, vectorized_all_dataset , on=[\"TimeStamp\",'id'])\n",
        "merged_df"
      ],
      "metadata": {
        "id": "Plxh4F6Lx8_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# which one of jump exist that not be in merged_df\n",
        "jump['TimeStamp'][~jump['TimeStamp'].isin(merged_df['TimeStamp'])]"
      ],
      "metadata": {
        "id": "_zY3wrXTA4Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merged_df columns:\n",
        "# transcript_x    id  file_name  euclidean_mean  euclidean_std  euclidean_sum  cosine_mean  cosine_std  cosine_sum\n",
        "# TimeStamp    transcript_y  length_x  BDI_score   category  category_numeric total_length vector_0  vector_1\n",
        "# vector_2  vector_3  vector_4  vector_5  vector_6  vector_7  vector_8  vector_9  vector_10  vector_11  vector_12  vector_13  vector_14  vector_15  vector_16  vector_17  vector_18  vector_19  vector_20  vector_21  vector_22  vector_23  vector_24  vector_25  vector_26  vector_27  vector_28  vector_29  vector_30  vector_31  vector_32  vector_33  vector_34  vector_35  vector_36  vector_37  vector_38  vector_39  vector_40  vector_41  vector_42  vector_43  vector_44  vector_45  vector_46  vector_47  vector_48  vector_49  vector_50  vector_51  vector_52  vector_53  vector_54  vector_55  vector_56  vector_57  vector_58  vector_59  vector_60  vector_61  vector_62  vector_63  vector_64  vector_65  vector_66  vector_67  vector_68  vector_69  vector_70  vector_71  vector_72  vector_73  vector_74  vector_75  vector_76  vector_77  vector_78  ve"
      ],
      "metadata": {
        "id": "lKkPn1VQH-e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##sorting and show similarity matrix"
      ],
      "metadata": {
        "id": "Sw7D68aSaosI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sort by euclidean_mean feature and then show the similarity matrix\n"
      ],
      "metadata": {
        "id": "iAQDif1UBlbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.iloc[0:2, 16:]"
      ],
      "metadata": {
        "id": "wDHx5PpKQ6FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# create a sample DataFrame\n",
        "temp_df = merged_df\n",
        "\n",
        "# sort the DataFrame by the 'salary' column in descending order\n",
        "df_sorted = temp_df.sort_values(['euclidean_mean','id'])\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(df_sorted.iloc[:, 16:])\n",
        "\n",
        "# print the similarity matrix\n",
        "similarity_matrix.shape"
      ],
      "metadata": {
        "id": "auO79_6IBlbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# create a sample DataFrame with 10 rows and 10 columns\n",
        "df = df_sorted[['euclidean_mean','id']]\n",
        "\n",
        "# set the display options to show all rows and columns\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# print the DataFrame\n",
        "print(df)\n",
        "# df['euclidean_mean'].unique()"
      ],
      "metadata": {
        "id": "jO-p4G0MBlbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(similarity_matrix, cmap='coolwarm')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "plt.xticks(rotation = 90)\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"Similarity Matrix sorted by euclidean_mean from little to much\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "U3R8WrXzBlbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###compare subset"
      ],
      "metadata": {
        "id": "5X-hfD5tBlbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compare subset\n",
        "import numpy as np\n",
        "\n",
        "# select the first 741 rows and columns of the similarity matrix\n",
        "subset1 = similarity_matrix[:10, :10]\n",
        "\n",
        "# select the last 74 rows and columns of the similarity matrix\n",
        "subset2 = similarity_matrix[-10:, -10:]\n",
        "\n",
        "# calculate the mean similarity of each subset\n",
        "mean1 = np.mean(subset1)\n",
        "mean2 = np.mean(subset2)\n",
        "\n",
        "# print the mean similarity of each subset\n",
        "print(\"Mean similarity of subset (euclidean_mean range 10 ):\", mean1)\n",
        "print(\"Mean similarity of subset (euclidean_mean range 18):\", mean2)"
      ],
      "metadata": {
        "id": "fCRqTymOBlbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sort by euclidean_std feature and then show the similarity matrix\n"
      ],
      "metadata": {
        "id": "SvNJLz7pZ2QC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# create a sample DataFrame\n",
        "temp_df = merged_df\n",
        "\n",
        "# sort the DataFrame by the 'salary' column in descending order\n",
        "df_sorted = temp_df.sort_values(['euclidean_std','id'])\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(df_sorted.iloc[:, 16:])\n",
        "\n",
        "# print the similarity matrix\n",
        "similarity_matrix.shape"
      ],
      "metadata": {
        "id": "qCNrJsiMZ2QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# create a sample DataFrame with 10 rows and 10 columns\n",
        "df = df_sorted[['euclidean_std','id']].dropna()\n",
        "\n",
        "# set the display options to show all rows and columns\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# print the DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "id": "Ym6r8fokZ2QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "D5x0yfEoaBle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(similarity_matrix, cmap='coolwarm')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "plt.xticks(rotation = 90)\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"Similarity Matrix sorted by euclidean_std from little to much\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yyGrOjm3Z2QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###compare subset"
      ],
      "metadata": {
        "id": "ec7iIGDnZ2QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compare subset\n",
        "import numpy as np\n",
        "\n",
        "# select the first 741 rows and columns of the similarity matrix\n",
        "subset1 = similarity_matrix[:100, :100]\n",
        "\n",
        "# select the last 74 rows and columns of the similarity matrix\n",
        "subset2 = similarity_matrix[-100:, -100:]\n",
        "\n",
        "# calculate the mean similarity of each subset\n",
        "mean1 = np.mean(subset1)\n",
        "mean2 = np.mean(subset2)\n",
        "\n",
        "# print the mean similarity of each subset\n",
        "print(\"Mean similarity of subset (euclidean_std range 100 ):\", mean1)\n",
        "print(\"Mean similarity of subset (euclidean_std range 100):\", mean2)"
      ],
      "metadata": {
        "id": "h1Kqi9H1Z2QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sort by euclidean_sum feature and then show the similarity matrix\n"
      ],
      "metadata": {
        "id": "1lxBEw27XBc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# create a sample DataFrame\n",
        "temp_df = merged_df\n",
        "\n",
        "# sort the DataFrame by the 'salary' column in descending order\n",
        "df_sorted = temp_df.sort_values(['euclidean_sum','id'])\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(df_sorted.iloc[:, 16:])\n",
        "\n",
        "# print the similarity matrix\n",
        "similarity_matrix.shape"
      ],
      "metadata": {
        "id": "VIl4i59HXBdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# create a sample DataFrame with 10 rows and 10 columns\n",
        "df = df_sorted[['euclidean_sum','id']]\n",
        "\n",
        "# set the display options to show all rows and columns\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# print the DataFrame\n",
        "df\n",
        "# df['euclidean_mean'].unique()"
      ],
      "metadata": {
        "id": "DnxFxha6XBdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(similarity_matrix, cmap='coolwarm')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "plt.xticks(rotation = 90)\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"Similarity Matrix sorted by euclidean_sum from little to much\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Rho-WUv3XBdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###compare subset"
      ],
      "metadata": {
        "id": "edCtIdwjXBda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compare subset\n",
        "import numpy as np\n",
        "\n",
        "# select the first 741 rows and columns of the similarity matrix\n",
        "subset1 = similarity_matrix[:100, :100]\n",
        "\n",
        "# select the last 74 rows and columns of the similarity matrix\n",
        "subset2 = similarity_matrix[-100:, -100:]\n",
        "\n",
        "# calculate the mean similarity of each subset\n",
        "mean1 = np.mean(subset1)\n",
        "mean2 = np.mean(subset2)\n",
        "\n",
        "# print the mean similarity of each subset\n",
        "print(\"Mean similarity of subset (100 first euclidean_sum):\", mean1)\n",
        "print(\"Mean similarity of subset (100 last euclidean_sum):\", mean2)"
      ],
      "metadata": {
        "id": "_qmU_b83XBdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sort by cosine_mean feature and then show the similarity matrix\n"
      ],
      "metadata": {
        "id": "nlUt_NxJV5ap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# create a sample DataFrame\n",
        "temp_df = merged_df\n",
        "\n",
        "# sort the DataFrame by the 'salary' column in descending order\n",
        "df_sorted = temp_df.sort_values(['cosine_mean','id'])\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(df_sorted.iloc[:, 16:])\n",
        "\n",
        "# print the similarity matrix\n",
        "similarity_matrix.shape"
      ],
      "metadata": {
        "id": "y_VGUxQlV5ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# create a sample DataFrame with 10 rows and 10 columns\n",
        "df = df_sorted[['cosine_mean','id']]\n",
        "\n",
        "# set the display options to show all rows and columns\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# print the DataFrame\n",
        "df\n",
        "# df['euclidean_mean'].unique()"
      ],
      "metadata": {
        "id": "LnH9g8hSV5as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(similarity_matrix, cmap='coolwarm')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "plt.xticks(rotation = 90)\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"Similarity Matrix sorted by cosine_mean from little to much\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AsJSK2bbV5as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###compare subset"
      ],
      "metadata": {
        "id": "0WqCuKV6V5at"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compare subset\n",
        "import numpy as np\n",
        "\n",
        "# select the first 741 rows and columns of the similarity matrix (negation_count == 0)\n",
        "subset1 = similarity_matrix[:100, :100]###############?\n",
        "\n",
        "# select the last 74 rows and columns of the similarity matrix (negation_count >= 2)\n",
        "subset2 = similarity_matrix[-100:, -100:] ############?\n",
        "\n",
        "# calculate the mean similarity of each subset\n",
        "mean1 = np.mean(subset1)\n",
        "mean2 = np.mean(subset2)\n",
        "\n",
        "# print the mean similarity of each subset\n",
        "print(\"Mean similarity of subset (100 first cosine_mean):\", mean1)\n",
        "print(\"Mean similarity of subset (100 last cosine_mean):\", mean2)"
      ],
      "metadata": {
        "id": "yVTPnZZsV5at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sort by cosine_std feature and then show the similarity matrix\n"
      ],
      "metadata": {
        "id": "dkJ9L1jpX2As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# create a sample DataFrame\n",
        "temp_df = merged_df\n",
        "\n",
        "# sort the DataFrame by the 'salary' column in descending order\n",
        "df_sorted = temp_df.sort_values(['cosine_std','id'])\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(df_sorted.iloc[:, 16:])\n",
        "\n",
        "# print the similarity matrix\n",
        "similarity_matrix.shape"
      ],
      "metadata": {
        "id": "pHbQZtj-X2At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "df = df_sorted[['cosine_std','id']].dropna()\n",
        "\n",
        "# set the display options to show all rows and columns\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# print the DataFrame\n",
        "print(df)\n",
        "# df['euclidean_mean'].unique()"
      ],
      "metadata": {
        "id": "SN6NxOK7X2At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(similarity_matrix, cmap='coolwarm')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# # set the x and y ticks every 80 rows and columns\n",
        "# ax.set_xticks(sum)\n",
        "# ax.set_yticks(sum)\n",
        "\n",
        "# for i in range(len(sum)):\n",
        "#   plt.axvline(x=sum[i], color='black', linewidth=0.4)\n",
        "#   plt.axhline(y=sum[i], color='black', linewidth=0.4)\n",
        "# Add a vertical line at x=3\n",
        "\n",
        "plt.xticks(rotation = 90)\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"Similarity Matrix sorted by cosine_std from little to much\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "u4a0ldTCX2At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###compare subset"
      ],
      "metadata": {
        "id": "TacIlQ2GX2Au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compare subset\n",
        "import numpy as np\n",
        "\n",
        "# select the first 741 rows and columns of the similarity matrix\n",
        "subset1 = similarity_matrix[:200, :200]\n",
        "\n",
        "# select the last 74 rows and columns of the similarity matrix\n",
        "subset2 = similarity_matrix[-200:, -200:]\n",
        "\n",
        "# calculate the mean similarity of each subset\n",
        "mean1 = np.mean(subset1)\n",
        "mean2 = np.mean(subset2)\n",
        "\n",
        "# print the mean similarity of each subset\n",
        "print(\"Mean similarity of subset (200 first cosine_std ):\", mean1)\n",
        "print(\"Mean similarity of subset (200 last cosine_std ):\", mean2)"
      ],
      "metadata": {
        "id": "brgpHSujX2Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FcfvA-w3ZIt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sort by cosine_sum feature and then show the similarity matrix\n"
      ],
      "metadata": {
        "id": "2LCYY1fZZIzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# create a sample DataFrame\n",
        "temp_df = merged_df\n",
        "\n",
        "# sort the DataFrame by the 'salary' column in descending order\n",
        "df_sorted = temp_df.sort_values(['cosine_sum','id'])\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(df_sorted.iloc[:, 16:])\n",
        "\n",
        "# print the similarity matrix\n",
        "similarity_matrix.shape"
      ],
      "metadata": {
        "id": "i4VHHjdbZIzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# create a sample DataFrame with 10 rows and 10 columns\n",
        "df = df_sorted[['cosine_sum','id']]\n",
        "\n",
        "# set the display options to show all rows and columns\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# print the DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "id": "hWZpEAozZIzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(similarity_matrix, cmap='coolwarm')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "plt.xticks(rotation = 90)\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"Similarity Matrix sorted by cosine_sum from little to much\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ta73NwXdZIz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###compare subset"
      ],
      "metadata": {
        "id": "VyGUrOotZIz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compare subset\n",
        "import numpy as np\n",
        "\n",
        "# select the first 741 rows and columns of the similarity matrix\n",
        "subset1 = similarity_matrix[:200, :200]\n",
        "\n",
        "# select the last 74 rows and columns of the similarity matrix\n",
        "subset2 = similarity_matrix[-200:, -200:]\n",
        "\n",
        "# calculate the mean similarity of each subset\n",
        "mean1 = np.mean(subset1)\n",
        "mean2 = np.mean(subset2)\n",
        "\n",
        "# print the mean similarity of each subset\n",
        "print(\"Mean similarity of subset (cosine_sum range 200 ):\", mean1)\n",
        "print(\"Mean similarity of subset (cosine_sum range 200):\", mean2)"
      ],
      "metadata": {
        "id": "MFheyP5RZIz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## correlation"
      ],
      "metadata": {
        "id": "s6MHkZtravQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.columns"
      ],
      "metadata": {
        "id": "2mRTkHlbbpc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cor_df = merged_df[['euclidean_mean','euclidean_std' , 'euclidean_sum' , 'cosine_mean' , 'cosine_std' , 'cosine_sum','BDI_score','TimeStamp']]\n",
        "cor_df"
      ],
      "metadata": {
        "id": "xUm_BxxBb6PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "euclidean_mean  euclidean_std  euclidean_sum  cosine_mean  cosine_std  cosine_sum"
      ],
      "metadata": {
        "id": "u1kEviDxc8wI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['euclidean_mean']. corr(merged_df['BDI_score'])"
      ],
      "metadata": {
        "id": "xjZ4oYDobkB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['euclidean_std']. corr(merged_df['BDI_score'])"
      ],
      "metadata": {
        "id": "tXSpnwbyc1ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['euclidean_sum']. corr(merged_df['BDI_score'])"
      ],
      "metadata": {
        "id": "RYxyhVFhdBHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['cosine_mean']. corr(merged_df['BDI_score'])"
      ],
      "metadata": {
        "id": "ZqteDbSzdBH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['cosine_std']. corr(merged_df['BDI_score'])"
      ],
      "metadata": {
        "id": "ayzEPjxIdCZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['cosine_sum']. corr(merged_df['BDI_score'])"
      ],
      "metadata": {
        "id": "iMYN43bGdCZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "VT2i3COECV1w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl0ID-XnTU4G"
      },
      "source": [
        "#7. scatter plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i270iH81adDF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4GVoG9uf9Qf"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/data_with_linguistic_features.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVBXEwSbPvZD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(df.iloc[:, 13:])\n",
        "\n",
        "# print the similarity matrix\n",
        "similarity_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMaBPShYhen3"
      },
      "outputs": [],
      "source": [
        "# compare all the rows of the first and second column\n",
        "# pic1 subj1 & pic2 subj1\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Generate a random similarity matrix\n",
        "\n",
        "# Create a scatter plot matrix of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax.scatter(similarity_matrix[:, 0], similarity_matrix[:, 1], s=1)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7SvYxLLl3kb"
      },
      "outputs": [],
      "source": [
        "# pic1 subj1 & pic1 subj2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create a scatter plot matrix of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax.scatter(similarity_matrix[:, 0], similarity_matrix[:, 8], s=1)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VaRoJdRhzlW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(df.iloc[:, 13:])\n",
        "\n",
        "# Perform PCA on the similarity matrix\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(similarity_matrix)\n",
        "transformed_matrix = pca.transform(similarity_matrix)\n",
        "third_variable = df.BDI_score\n",
        "# third_variable = np.random.rand(952)\n",
        "\n",
        "# Create a scatter plot of the first two principal components, with color based on the third variable\n",
        "fig, ax = plt.subplots(figsize=(19, 8))\n",
        "scatter = ax.scatter(transformed_matrix[:, 0], transformed_matrix[:, 1], c=third_variable, s=30, cmap='coolwarm', vmin=np.min(third_variable), vmax=np.max(third_variable))\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "# Add labels and title to the plot\n",
        "ax.set_xlabel('Principal Component 1')\n",
        "ax.set_ylabel('Principal Component 2')\n",
        "ax.set_title('Scatter Plot with Color Based on BDI_score')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. unsupervised"
      ],
      "metadata": {
        "id": "m1E_EYIzIe38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/final_data_with_linguistic_features.csv')"
      ],
      "metadata": {
        "id": "gCpzewpWcImL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features from the transcript column using TF-IDF vectorization\n",
        "tfidf = TfidfVectorizer()\n",
        "X = tfidf.fit_transform(df['transcript'])"
      ],
      "metadata": {
        "id": "vvnwt043cn1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L7wHSIlbcdq"
      },
      "outputs": [],
      "source": [
        "# Cluster the text data\n",
        "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
        "\n",
        "# Evaluate the clustering results\n",
        "labels = kmeans.labels_\n",
        "score = silhouette_score(X, labels)\n",
        "print('Silhouette score:', score)\n",
        "\n",
        "# Manually inspect the clusters\n",
        "cluster1 = df[labels == 0]\n",
        "cluster2 = df[labels == 1]\n",
        "print('Cluster 1:', cluster1)\n",
        "print('Cluster 2:', cluster2)\n",
        "\n",
        "# Visualize the results\n",
        "# TODO: Add code to visualize the results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize the clustering results using all the columns of the TF-IDF matrix, you can use dimensionality reduction techniques such as principal component analysis (PCA) or t-distributed stochastic neighbor embedding (t-SNE) to project the high-dimensional data onto a lower-dimensional space that can be easily visualized."
      ],
      "metadata": {
        "id": "BXzQgRDJitdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Cluster the text data\n",
        "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
        "\n",
        "# Reduce the dimensionality of the data using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "X_tsne = tsne.fit_transform(X.toarray())\n",
        "\n",
        "# Evaluate the clustering results\n",
        "labels = kmeans.labels_\n",
        "score = silhouette_score(X, labels)\n",
        "print('Silhouette score:', score)\n",
        "\n",
        "# Visualize the clustering results\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, s=50, cmap='viridis')\n",
        "legend = ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Clusters\")\n",
        "ax.add_artist(legend)\n",
        "plt.title('Clustering Results')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R5QtErx1f3d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reduce the dimensionality of the data using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X.toarray())\n",
        "\n",
        "# Evaluate the clustering results\n",
        "labels = kmeans.labels_\n",
        "score = silhouette_score(X, labels)\n",
        "print('Silhouette score:', score)\n",
        "\n",
        "# Visualize the clustering results\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, s=50, cmap='viridis')\n",
        "legend = ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Clusters\")\n",
        "ax.add_artist(legend)\n",
        "plt.title('Clustering Results')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TFE8nKo_jC2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "امتیاز سیلوئت یک معیار از جدایی خوب خوشه‌ها در نتیجه خوشه‌بندی است. این امتیاز از -1 تا 1 متغیر است، به طوری که امتیاز 1 نشان دهنده جدایی کامل خوشه‌ها، امتیاز 0 نشان دهنده همپوشانی خوشه‌ها و امتیاز -1 نشان دهنده این است که نمونه‌ها به خوشه‌های اشتباهی اختصاص داده شده‌اند.\n",
        "\n",
        "در مورد ما امتیاز سیلوئت 0.005 نشان می‌دهد که خوشه‌ا بسیار نزدیک به هم و همپوشانی دارند، به این معنی که نتیجه خوشه‌بندی خوبی نیست. این امتیا نشان می‌دهد که الگوریتم خوشه‌بندی قادر به جداسازی ده‌ها به خوشه‌های مجزا نیست.\n"
      ],
      "metadata": {
        "id": "LDVSNM_FkFfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cluster the text data\n",
        "kmeans = KMeans(n_clusters=3, random_state=0).fit(X)\n",
        "\n",
        "# Reduce the dimensionality of the data using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X.toarray())\n",
        "\n",
        "# Evaluate the clustering results\n",
        "labels = kmeans.labels_\n",
        "score = silhouette_score(X, labels)\n",
        "print('Silhouette score:', score)\n",
        "\n",
        "# Visualize the clustering results\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, s=50, cmap='viridis')\n",
        "legend = ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Clusters\")\n",
        "ax.add_artist(legend)\n",
        "plt.title('Clustering Results')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LTrA03makKDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cluster the text data\n",
        "kmeans = KMeans(n_clusters=5, random_state=0).fit(X)\n",
        "\n",
        "# Reduce the dimensionality of the data using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X.toarray())\n",
        "\n",
        "# Evaluate the clustering results\n",
        "labels = kmeans.labels_\n",
        "score = silhouette_score(X, labels)\n",
        "print('Silhouette score:', score)\n",
        "\n",
        "# Visualize the clustering results\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, s=50, cmap='viridis', alpha=0.5)\n",
        "legend = ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Clusters\")\n",
        "ax.add_artist(legend)\n",
        "plt.title('Clustering Results')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Np_m3cBilJW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Perform hierarchical clustering on the data\n",
        "Z = linkage(X.toarray(), method='ward')\n",
        "\n",
        "# Plot the dendrogram\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(Z)\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DJwF1FGlnzYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. feature extraction\n"
      ],
      "metadata": {
        "id": "nI-71VIKZA2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##sentiment analysis"
      ],
      "metadata": {
        "id": "u5B3rS_MJUeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/final_data_with_linguistic_features.csv')"
      ],
      "metadata": {
        "id": "mFcIvpyOdW4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the sentiment analysis model\n",
        "classifier = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "# Define a function to extract sentiment from text\n",
        "def get_sentiment(text):\n",
        "    result = classifier(text)[0]\n",
        "    sentiment_score = result['score']\n",
        "    return sentiment_score\n",
        "\n",
        "# Apply the function to the text column and create a new column for sentiment\n",
        "df['sentiment_transformers'] = df['transcript'].apply(get_sentiment)"
      ],
      "metadata": {
        "id": "wjGqSqaN3afC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm\n",
        "#then restart run time"
      ],
      "metadata": {
        "id": "ZFt56ydrdsdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flair"
      ],
      "metadata": {
        "id": "ofiRL5ZAdBnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from hazm import sent_tokenize, word_tokenize, Normalizer\n",
        "from flair.models import TextClassifier\n",
        "from flair.data import Sentence\n",
        "\n",
        "# Initialize the Hazm library tools and the Flair sentiment classifier\n",
        "normalizer = Normalizer()\n",
        "classifier = TextClassifier.load('sentiment')\n",
        "\n",
        "# Define a function to extract sentiment from text\n",
        "def get_sentiment(text):\n",
        "    normalized_text = normalizer.normalize(text)\n",
        "    sentences = sent_tokenize(normalized_text)\n",
        "    sentiment_scores = []\n",
        "    for sentence in sentences:\n",
        "        sentence = Sentence(sentence)\n",
        "        classifier.predict(sentence)\n",
        "        sentiment_scores.append(sentence.labels[0].score)\n",
        "    sentiment_score = sum(sentiment_scores) / len(sentiment_scores)\n",
        "    return sentiment_score\n",
        "\n",
        "# Apply the function to the text column and create a new column for sentiment\n",
        "df['sentiment_flair'] = df['transcript'].apply(get_sentiment)"
      ],
      "metadata": {
        "id": "_BsSPbUNbU9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Speeh Graph Analysis"
      ],
      "metadata": {
        "id": "6-pqgulDrN9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WC/\n",
        "nodes/\n",
        "edges/\n",
        "RE/\n",
        "PE/\n",
        "L1/\n",
        "L2/\n",
        "L3/\n",
        "LCC/\n",
        "LSC_/\n",
        "ATD_list/density/\n",
        "diameter/\n",
        "CC/"
      ],
      "metadata": {
        "id": "0XvxR5DUJaTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import hazm # for Persian text processing\n",
        "\n",
        "# Create empty lists to store graph attributes\n",
        "WC_list = []\n",
        "nodes_list = []\n",
        "edges_list = []\n",
        "RE_list = []\n",
        "PE_list = []\n",
        "L1_list = []\n",
        "L2_list = []\n",
        "L3_list = []\n",
        "LCC_list = []\n",
        "LSC_list = []\n",
        "ATD_list =density_list = []\n",
        "diameter_list = []\n",
        "ASP_list = []\n",
        "CC_list = []\n",
        "\n",
        "# Iterate over the rows of the dataset\n",
        "for index, row in df.iterrows():\n",
        "    # Tokenize the Persian text using Hazm\n",
        "    tokens = hazm.word_tokenize(row['transcript'])\n",
        "    # Create an undirected graph from the tokens\n",
        "    G = nx.Graph()\n",
        "    for token in tokens:\n",
        "        G.add_node(token)\n",
        "    for i in range(len(tokens)-1):\n",
        "        G.add_edge(tokens[i], tokens[i+1])\n",
        "    # Calculate graph attributes\n",
        "    WC = len(tokens)\n",
        "    nodes = len(G.nodes())\n",
        "    edges = len(G.edges())\n",
        "    RE = edges / nodes\n",
        "    PE = edges / (nodes * (nodes - 1))\n",
        "    L1 = nx.average_shortest_path_length(G)\n",
        "    L2 = nx.algebraic_connectivity(G)\n",
        "    # L3 = nx.spectral_radius(G)\n",
        "    LCC = nx.average_clustering(G)\n",
        "    LSC = nx.average_shortest_path_length(nx.line_graph(G))\n",
        "    # ATD = nx.average_degree_connectivity(G)\n",
        "    density = nx.density(G)\n",
        "    diameter = nx.diameter(G)\n",
        "    ASP = nx.average_shortest_path_length(G)\n",
        "    CC = nx.transitivity(G)\n",
        "    # Append graph attributes to lists\n",
        "    WC_list.append(WC)\n",
        "    nodes_list.append(nodes)\n",
        "    edges_list.append(edges)\n",
        "    RE_list.append(RE)\n",
        "    PE_list.append(PE)\n",
        "    L1_list.append(L1)\n",
        "    L2_list.append(L2)\n",
        "    # L3_list.append(L3)\n",
        "    LCC_list.append(LCC)\n",
        "    LSC_list.append(LSC)\n",
        "    # ATD_list.append(ATD)\n",
        "    density_list.append(density)\n",
        "    diameter_list.append(diameter)\n",
        "    ASP_list.append(ASP)\n",
        "    CC_list.append(CC)\n",
        "\n",
        "# Add graph attributes as columns to the DataFrame\n",
        "df['WC'] = WC_list\n",
        "df['nodes'] = nodes_list\n",
        "df['edges'] = edges_list\n",
        "df['RE'] = RE_list\n",
        "df['PE'] = PE_list\n",
        "df['L1'] = L1_list\n",
        "df['L2'] = L2_list\n",
        "# df['L3'] = L3_list\n",
        "df['LCC'] = LCC_list\n",
        "df['LSC'] = LSC_list\n",
        "# df['ATD'] = ATD_list\n",
        "df['density'] = density_list\n",
        "df['diameter'] = diameter_list\n",
        "df['ASP'] = ASP_list\n",
        "df['CC'] = CC_list\n"
      ],
      "metadata": {
        "id": "jxJi1FPegnG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the updated DataFrame to a CSV file\n",
        "df.to_csv('/content/gdrive/MyDrive/persian_dataset_with_graph_attributes_sentiment_jumping.csv', index=False)"
      ],
      "metadata": {
        "id": "mqNnhTVjwEN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##correlation"
      ],
      "metadata": {
        "id": "Hd-uB-mA0ZIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "wDp7pOkd9rU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scatter plot for repetitions and word_rate\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['sentiment_flair'],df['sentiment_transformers'], c=df['BDI_score'], cmap='coolwarm', alpha=0.8)\n",
        "plt.colorbar(label='BDI Score')\n",
        "plt.xlabel('sentiment_flair')\n",
        "plt.ylabel('sentiment_transformers')\n",
        "plt.title('Scatter Plot of sentiment_flair vs. sentiment_transformers')\n",
        "#  The cmap parameter is used to specify the color map for the BDI scores.\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HcTEYMCKhyne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correlation_df = pd.DataFrame()\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# calculate the correlation between each feature and the label\n",
        "corr = df.drop(['category_numeric', 'file_name', 'transcript', 'id', 'TimeStamp','category', 'lengthBySeconds', 'length', 'total_length',\n",
        "       'euclidean_mean', 'euclidean_std', 'euclidean_sum', 'cosine_mean',\n",
        "       'cosine_std', 'cosine_sum',], axis=1).corr()['BDI_score']\n",
        "\n",
        "# create a new dataframe with the correlation values\n",
        "corr_df = pd.DataFrame({'correlation': corr})\n",
        "\n",
        "# set the index to the feature names\n",
        "corr_df.index = corr_df.index.str.replace('feature', 'feat')\n",
        "\n",
        "# print the resulting dataframe\n",
        "print(corr_df)"
      ],
      "metadata": {
        "id": "8Qi5dxoP2HkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(corr_df[1:], annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Correlation with BDI Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6z5kXbNd221O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Convert y_train to a NumPy array\n",
        "# y_train_np = y_train.cpu().detach().numpy()\n",
        "\n",
        "# # Truncate the feature array to match the number of samples in the target array\n",
        "# features_truncated = features[:target.shape[0]]\n",
        "\n",
        "# # Concatenate the truncated feature array and the target array\n",
        "# data = np.concatenate((features_truncated, target), axis=1)\n",
        "\n",
        "# # Calculate the correlations\n",
        "# correlations = np.corrcoef(data, rowvar=False)\n",
        "\n",
        "# # Get the first three correlation values and their corresponding feature names\n",
        "# correlation_values = correlations[-1, :-1][:3]\n",
        "# feature_names = feature_names[:3]\n",
        "\n",
        "# # Create a DataFrame with feature names and their correlations with BDI score\n",
        "# correlation_df = pd.DataFrame(correlation_values, index=feature_names, columns=['Correlation with BDI Score'])\n",
        "\n",
        "# # Print the correlation table\n",
        "# print(correlation_df)\n",
        "\n",
        "# # Create a heatmap of the correlations\n",
        "# sns.heatmap(correlation_df, annot=True, cmap='coolwarm', center=0)\n",
        "# plt.title('Correlation with BDI Score')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "kaO-QMeNaHXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. SVM - RF- DT - balance - LOO"
      ],
      "metadata": {
        "id": "sgX-GD_aK9qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/persian_dataset_with_graph_attributes_sentiment_jumping.csv')"
      ],
      "metadata": {
        "id": "BPI4e4LRiW4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TFIDF with 2 class"
      ],
      "metadata": {
        "id": "MTKcDmxri1DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoTokenizer, AutoModel"
      ],
      "metadata": {
        "id": "Sc8sfKefu-sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "qZXEvdRhiO1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bdi<14 is normal and >14 is depress\n",
        "label14 =[]\n",
        "for i in range(data.shape[0]):\n",
        "  if data.iloc[i]['BDI_score'] < 14:\n",
        "    # print(data.iloc[i]['BDI_score'] , '//' ,data.iloc[i]['label9'])\n",
        "    label14.append(0)\n",
        "  else :\n",
        "    label14.append(1)\n",
        "\n",
        "data['label14'] = label14\n",
        "\n",
        "y = data['label14']"
      ],
      "metadata": {
        "id": "7MXS6qiYNo-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bdi<9 is normal and >9 is depress\n",
        "label9 =[]\n",
        "for i in range(data.shape[0]):\n",
        "  if data.iloc[i]['BDI_score'] < 9:\n",
        "    # print(data.iloc[i]['BDI_score'] , '//' ,data.iloc[i]['label9'])\n",
        "    label9.append(0)\n",
        "  else :\n",
        "    label9.append(1)\n",
        "\n",
        "data['label9'] = label9\n",
        "\n",
        "y = data['label9']"
      ],
      "metadata": {
        "id": "mZ3ChSu5H2ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "4EqrwI-RIEsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "SwN99DnIfEo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns that are not needed for prediction\n",
        "data = data.drop(['file_name','id', 'TimeStamp', 'category', 'total_length','lengthBySeconds'], axis=1)\n",
        "\n",
        "# Convert 'category_numeric' column to numeric values\n",
        "data['category_numeric'] = pd.to_numeric(data['category_numeric'], errors='coerce')\n",
        "\n",
        "# Preprocess the 'length' column to convert it into a numeric format (seconds)\n",
        "def convert_length_to_seconds(length):\n",
        "    parts = length.split('.')\n",
        "    if len(parts) == 3:\n",
        "        # HH.MM.SS format\n",
        "        hours = int(parts[0])\n",
        "        minutes = int(parts[1])\n",
        "        seconds = int(parts[2])\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid time format: {length}\")\n",
        "    return hours * 3600 + minutes * 60 + seconds\n",
        "\n",
        "data['length_seconds'] = data['length'].apply(convert_length_to_seconds)\n",
        "\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = data[['transcript','euclidean_mean', 'euclidean_std', 'euclidean_sum', 'cosine_mean',\n",
        "       'cosine_std', 'cosine_sum', 'sentiment_transformers', 'sentiment_flair',\n",
        "       'WC', 'nodes', 'edges', 'RE', 'PE', 'L1', 'L2', 'LCC', 'LSC', 'density',\n",
        "       'diameter', 'ASP', 'CC', 'length_seconds' ]]\n",
        "# y = data['BDI_score']"
      ],
      "metadata": {
        "id": "Nxd5npmGjjbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(data['BDI_score'])"
      ],
      "metadata": {
        "id": "Rdy_D5QZC4NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(y)"
      ],
      "metadata": {
        "id": "381VySubPInc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features from the transcript column using TF-IDF vectorization\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(X['transcript'])\n",
        "X_tfidf.shape"
      ],
      "metadata": {
        "id": "tJ6JFADCJTjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=20)\n",
        "pca.fit(X_tfidf.toarray())"
      ],
      "metadata": {
        "id": "qLYh92qHIfhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "# from sklearn.datasets import load_iris\n",
        "\n",
        "\n",
        "\n",
        "# Plot scree plot\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, 'ro-', linewidth=2)\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Proportion of Variance Explained')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ECor-bv7QJ09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "so 7 is The \"elbow\" point"
      ],
      "metadata": {
        "id": "5dDJUm_3YNli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Assuming tfidf_matrix is your TF-IDF matrix\n",
        "pca = PCA(n_components=7) # You can choose the number of components you want to reduce to\n",
        "pca.fit(X_tfidf.toarray())\n",
        "\n",
        "# Transform the matrix\n",
        "tfidf_pca = pca.transform(X_tfidf.toarray())\n",
        "\n",
        "# Print the shape of the new matrix\n",
        "print(tfidf_pca.shape)"
      ],
      "metadata": {
        "id": "yJA04JcTRWJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.columns"
      ],
      "metadata": {
        "id": "M3FNNDOgJ1aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X.drop(['transcript', 'length_seconds'], axis=1))\n",
        "\n",
        "# Concatenate the TF-IDF features with the scaled numerical features\n",
        "X_final = np.concatenate((tfidf_pca, X_scaled), axis=1)"
      ],
      "metadata": {
        "id": "YG-sM0DutSJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values from the training data\n",
        "mask = ~np.isnan(X_final).any(axis=1)\n",
        "X_final = X_final[mask]\n",
        "y = y[mask]"
      ],
      "metadata": {
        "id": "xueGnF8iuJHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_final.shape"
      ],
      "metadata": {
        "id": "Xq4uNLeWNvNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the NumPy arrays to PyTorch tensors and move them to the GPU\n",
        "X_final = torch.tensor(X_final, dtype=torch.float32).to(device)\n",
        "y = torch.tensor(y.values, dtype=torch.float32).unsqueeze(1).to(device)"
      ],
      "metadata": {
        "id": "D56VREq-LcI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_final.shape"
      ],
      "metadata": {
        "id": "y11BL6WeLCq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tfidf_pca & jumping & SGA & sentiment\n",
        "# LOO\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "X= X_final\n",
        "y= y\n",
        "\n",
        "loo = LeaveOneOut()\n",
        "mse_list = []\n",
        "accuracy_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "f1_list = []\n",
        "y_pred_list = []\n",
        "y_test_list= []\n",
        "\n",
        "for train_index, test_index in loo.split(X):\n",
        "\n",
        "\n",
        "    # print(loo.split(X) , '/' ,train_index , '/' ,test_index)\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    clf = SVC(kernel='linear')\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    y_pred_list.append(y_pred)\n",
        "    y_test_list.append(y_test)\n",
        "    mse_list.append(mse)\n",
        "    accuracy_list.append(accuracy)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)\n",
        "    f1_list.append(f1)"
      ],
      "metadata": {
        "id": "j8jQDMvHKjRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_mse = sum(mse_list) / len(mse_list)\n",
        "mean_accuracy = sum(accuracy_list) / len(accuracy_list)\n",
        "mean_precision = sum(precision_list) / len(precision_list)\n",
        "mean_recall = sum(recall_list) / len(recall_list)\n",
        "mean_f1 = sum(f1_list) / len(f1_list)\n",
        "\n",
        "print(\"Mean MSE:\", mean_mse)\n",
        "print(\"Mean accuracy:\", mean_accuracy)\n",
        "print(\"Mean precision:\", mean_precision)\n",
        "print(\"Mean recall:\", mean_recall)\n",
        "print(\"Mean F1 score:\", mean_f1);\n",
        "\n",
        "# goalllll\n",
        "# Mean MSE: 0.416195856873823\n",
        "# Mean accuracy: 0.583804143126177\n",
        "# Mean precision: 0.583804143126177\n",
        "# Mean recall: 0.583804143126177\n",
        "# Mean F1 score: 0.583804143126177\n",
        "\n",
        "#label14\n",
        "# Mean MSE: 0.4218455743879473\n",
        "# Mean accuracy: 0.5781544256120528\n",
        "# Mean precision: 0.5781544256120528\n",
        "# Mean recall: 0.5781544256120528\n",
        "# Mean F1 score: 0.5781544256120528"
      ],
      "metadata": {
        "id": "-sAQzxLyNHdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Create a table comparing actual and predicted scores\n",
        "comparison_table = pd.DataFrame({'Actual BDI Score': y_test_list, 'Predicted BDI Score': y_pred_list})\n",
        "table = tabulate(comparison_table, headers='keys', tablefmt='psql')\n",
        "print(table)\n"
      ],
      "metadata": {
        "id": "cgrzjlrr9nYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert tensor to list\n",
        "y_true_converted = []\n",
        "for elem in y_test_list:\n",
        "    y_true_converted.append(int(elem[0].item()))\n",
        "\n",
        "# print(y_true_converted)"
      ],
      "metadata": {
        "id": "JqkVqHEIZ0_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Confusion Matrix:"
      ],
      "metadata": {
        "id": "4x3K_vfWY16K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Obtain the predictions from your classification model\n",
        "y_pred = y_pred_list\n",
        "\n",
        "# Obtain the true labels for the test data\n",
        "y_true = y_true_converted\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Define class names and colormap\n",
        "classes = ['Class 0', 'Class 1']\n",
        "cmap = plt.cm.Blues\n",
        "\n",
        "# Normalize the confusion matrix\n",
        "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Plot the confusion matrix using seaborn heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm_norm, annot=cm, fmt='d', cmap=cmap, xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "545IHE0Zc9IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. ROC Curve:"
      ],
      "metadata": {
        "id": "f4kTdcUxbFGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ROC (Receiver Operating Characteristic) curve is a graphical representation of the performance of a binary classification model. It plots the true positive rate (sensitivity) against the false positive rate (1-specificity) at various thresholds.\n",
        "\n",
        "An ideal ROC curve should have a steep upward slope and then a sharp turn toward the upper-left corner, followed by a gentle flattening out toward the top-right corner. This means that the model has high sensitivity (true positive rate) and low false positive rate across all possible threshold values, indicating that it is able to accurately distinguish between positive and negative cases. In other words, an ideal ROC curve would have an AUC (Area Under the Curve) value of 1.0, which represents perfect predictive ability."
      ],
      "metadata": {
        "id": "nmXRKSmWeXMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Obtain the predicted probabilities from your classification model\n",
        "y_scores = y_pred_list\n",
        "\n",
        "# Obtain the true labels for the test data\n",
        "y_true = y_true_converted\n",
        "\n",
        "# Compute the false positive rate, true positive rate, and threshold values\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "y2VmqC0vY1IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Precision-Recall Curve:"
      ],
      "metadata": {
        "id": "uS2fm1mMbKTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Compute the precision, recall, and threshold values\n",
        "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
        "\n",
        "# Plot the precision-recall curve\n",
        "plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall Curve')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8wc1SdvMY1FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scatter Plot between features:\n"
      ],
      "metadata": {
        "id": "FYVW-WD6gd7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the tensors to CPU memory\n",
        "X_train_final_cpu = X_train_final.cpu()\n",
        "y_train_cpu = y_train.cpu()\n",
        "\n",
        "# Scatter plot for repetitions and word_rate\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_train_final_cpu[:, 0].numpy(), X_train_final_cpu[:, 1].numpy(), c=y_train_cpu.squeeze(), cmap='coolwarm', alpha=0.8)\n",
        "plt.colorbar(label='BDI Score')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Repetitions')\n",
        "plt.title('Scatter Plot of Number of Words vs. Repetitions')\n",
        "#  The cmap parameter is used to specify the color map for the BDI scores.\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "I-F8uyfUhBKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the tensors to CPU memory\n",
        "X_train_final_cpu = X_train_final.cpu()\n",
        "y_train_cpu = y_train.cpu()\n",
        "\n",
        "# Scatter plot for Repetitions and word_rate\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_train_final_cpu[:, 1].numpy(), X_train_final_cpu[:,2].numpy(), c=y_train_cpu.squeeze(), cmap='coolwarm', alpha=0.8)\n",
        "plt.colorbar(label='BDI Score')\n",
        "plt.xlabel('Repetitions')\n",
        "plt.ylabel('Word Rate')\n",
        "plt.title('Scatter Plot of Repetitions vs. Word Rate')\n",
        "#  The cmap parameter is used to specify the color map for the BDI scores.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Enaw1NgDG92s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the tensors to CPU memory\n",
        "X_train_final_cpu = X_train_final.cpu()\n",
        "y_train_cpu = y_train.cpu()\n",
        "\n",
        "# Scatter plot for Number of Words and word_rate\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_train_final_cpu[:, 0].numpy(), X_train_final_cpu[:,2].numpy(), c=y_train_cpu.squeeze(), cmap='coolwarm', alpha=0.8)\n",
        "plt.colorbar(label='BDI Score')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Word Rate')\n",
        "plt.title('Scatter Plot of Number of Words vs. Word Rate')\n",
        "#  The cmap parameter is used to specify the color map for the BDI scores.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SKNLKgdGUyMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the tensors to CPU memory\n",
        "X_train_final_cpu = X_train_final.cpu()\n",
        "y_train_cpu = y_train.cpu()\n",
        "\n",
        "# Scatter plot for\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_train_final_cpu[:, 6].numpy(), X_train_final_cpu[:,8].numpy(), c=y_train_cpu.squeeze(), cmap='coolwarm', alpha=0.8)\n",
        "plt.colorbar(label='BDI Score')\n",
        "plt.xlabel('cosine mean')\n",
        "plt.ylabel('cosine sum')\n",
        "plt.title('Scatter Plot of cosine mean vs. cosine sum')\n",
        "#  The cmap parameter is used to specify the color map for the BDI scores.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HvmCgyxKHShJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Move the tensors to CPU memory\n",
        "# X_train_final_cpu = X_train_final.cpu()\n",
        "# y_train_cpu = y_train.cpu()\n",
        "\n",
        "# # Scatter plot\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.scatter(X_train_final_cpu[:, 0].numpy(), X_train_final_cpu[:,5].numpy(), c=y_train_cpu.squeeze(), cmap='coolwarm', alpha=0.8)\n",
        "# plt.colorbar(label='BDI Score')\n",
        "# plt.xlabel('Number of Words')\n",
        "# plt.ylabel('total length')\n",
        "# plt.title('Scatter Plot of Number of Words vs. total length')\n",
        "# #  The cmap parameter is used to specify the color map for the BDI scores.\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "BOnXAKLsIqdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### size"
      ],
      "metadata": {
        "id": "9V3YDRwcltJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the tensors to CPU memory\n",
        "X_train_final_cpu = X_train_final.cpu()\n",
        "y_train_cpu = y_train.cpu()\n",
        "\n",
        "# Scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_train_final_cpu[:, 1].numpy(), X_train_final_cpu[:,2].numpy(), c=y_train_cpu.squeeze(),s=y_train_cpu.squeeze() , cmap='coolwarm', alpha=0.8)\n",
        "plt.colorbar(label='BDI Score')\n",
        "plt.xlabel('repetition')\n",
        "plt.ylabel('word rate')\n",
        "plt.title('Scatter Plot of repetition vs. word rate')\n",
        "#  The cmap parameter is used to specify the color map for the BDI scores.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sDhdLrQRIr8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TFIDF with 63 class _ balance"
      ],
      "metadata": {
        "id": "grZh8lnVwS6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "CcoSUWIAwS6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "# from transformers import AutoTokenizer, AutoModel\n",
        "from torch import nn\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "W3e71I9owS6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/persian_dataset_with_graph_attributes_sentiment_jumping.csv')"
      ],
      "metadata": {
        "id": "iE6RTQX3wS6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### balanced data before dropna"
      ],
      "metadata": {
        "id": "0WAWx2o1-FLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load your dataframe into a pandas DataFrame object\n",
        "df1 =data\n",
        "\n",
        "# Define the bins for the 'bdi' column\n",
        "bins = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55]\n",
        "\n",
        "# Use pandas cut function to bin the 'bdi' values into different ranges\n",
        "df1['bdi_range'] = pd.cut(df1['BDI_score'], bins=bins)\n",
        "\n",
        "# Count the number of rows falling in each range\n",
        "count = df1.groupby('bdi_range')['BDI_score'].count()\n",
        "\n",
        "# Identify the ranges with large values\n",
        "large_ranges = count[count > 16].index.tolist()\n",
        "\n",
        "# Randomly delete half of the rows in the large ranges\n",
        "for r in large_ranges:\n",
        "    # print(r)\n",
        "    rows_to_delete = df1[df1['bdi_range'] == r].sample(n= len(df1[df1['bdi_range'] == r]) - 16 ).index\n",
        "    # rows_to_delete = np.random.choice(df1.index, size=16, replace=False)\n",
        "    df1.drop(rows_to_delete, inplace=True)\n",
        "\n",
        "data= df1\n",
        "data = data.reset_index().drop(['index'],axis=1)"
      ],
      "metadata": {
        "id": "i__r3Ljh7Ykg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows_to_delete = df1[df1['bdi_range'] == count[count > 16].index.tolist()[0]].sample(n=  16 ).index\n",
        "# rows_to_delete = np.random.choice(df1.index, size=16, replace=False)\n",
        "df1.drop(rows_to_delete, inplace=True)"
      ],
      "metadata": {
        "id": "KDn8SdG3ejD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(df1['BDI_score'])"
      ],
      "metadata": {
        "id": "vRw20rJDFn2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### continue..."
      ],
      "metadata": {
        "id": "Wm3chk0yHcaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Drop columns that are not needed for prediction\n",
        "data = data.drop(['file_name','id', 'TimeStamp', 'category', 'total_length','lengthBySeconds'], axis=1)\n",
        "# data=df1\n",
        "\n",
        "# Convert 'category_numeric' column to numeric values\n",
        "data['category_numeric'] = pd.to_numeric(data['category_numeric'], errors='coerce')\n",
        "\n",
        "# Preprocess the 'length' column to convert it into a numeric format (seconds)\n",
        "def convert_length_to_seconds(length):\n",
        "    parts = length.split('.')\n",
        "    if len(parts) == 3:\n",
        "        # HH.MM.SS format\n",
        "        hours = int(parts[0])\n",
        "        minutes = int(parts[1])\n",
        "        seconds = int(parts[2])\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid time format: {length}\")\n",
        "    return hours * 3600 + minutes * 60 + seconds\n",
        "\n",
        "data['length_seconds'] = data['length'].apply(convert_length_to_seconds)\n",
        "\n",
        "\n",
        "\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = data[['transcript','euclidean_mean', 'euclidean_std', 'euclidean_sum', 'cosine_mean',\n",
        "       'cosine_std', 'cosine_sum', 'sentiment_transformers', 'sentiment_flair',\n",
        "       'WC', 'nodes', 'edges', 'RE', 'PE', 'L1', 'L2', 'LCC', 'LSC', 'density',\n",
        "       'diameter', 'ASP', 'CC', 'length_seconds' ]]\n",
        "y = data['BDI_score']"
      ],
      "metadata": {
        "id": "87NZkf6OwS61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(y)"
      ],
      "metadata": {
        "id": "5jVG6LB6wS61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features from the transcript column using TF-IDF vectorization\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(X['transcript'])\n",
        "X_tfidf.shape"
      ],
      "metadata": {
        "id": "EGsLoTACwS62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Assuming tfidf_matrix is your TF-IDF matrix\n",
        "pca = PCA(n_components=7) # You can choose the number of components you want to reduce to\n",
        "pca.fit(X_tfidf.toarray())\n",
        "\n",
        "# Transform the matrix\n",
        "tfidf_pca = pca.transform(X_tfidf.toarray())\n",
        "\n",
        "# Print the shape of the new matrix\n",
        "print(tfidf_pca.shape)"
      ],
      "metadata": {
        "id": "4LLzyJ-WwS62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X.drop(['transcript', 'length_seconds'], axis=1))\n",
        "\n",
        "# Concatenate the TF-IDF features with the scaled numerical features\n",
        "X_final = np.concatenate((tfidf_pca, X_scaled), axis=1)"
      ],
      "metadata": {
        "id": "vyqgC1OCwS63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values from the training data\n",
        "mask = ~np.isnan(X_final).any(axis=1)\n",
        "X_final = X_final[mask]\n",
        "y = y[mask]"
      ],
      "metadata": {
        "id": "XTRH-_8EwS63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the NumPy arrays to PyTorch tensors and move them to the GPU\n",
        "X_final = torch.tensor(X_final, dtype=torch.float32).to(device)\n",
        "y = torch.tensor(y.values, dtype=torch.float32).unsqueeze(1).to(device)"
      ],
      "metadata": {
        "id": "u68eRNjdwS64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###continue"
      ],
      "metadata": {
        "id": "WnBr4dXPiVgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tfidf_pca & jumping & SGA & sentiment\n",
        "# LOO\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "X= X_final\n",
        "y= y\n",
        "\n",
        "loo = LeaveOneOut()\n",
        "mse_list = []\n",
        "accuracy_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "f1_list = []\n",
        "y_pred_list = []\n",
        "y_test_list= []\n",
        "\n",
        "for train_index, test_index in loo.split(X):\n",
        "\n",
        "\n",
        "    # print(loo.split(X) , '/' ,train_index , '/' ,test_index)\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    clf = SVC(kernel='linear')\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    y_pred_list.append(y_pred)\n",
        "    y_test_list.append(y_test)\n",
        "    mse_list.append(mse)\n",
        "    accuracy_list.append(accuracy)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)\n",
        "    f1_list.append(f1)"
      ],
      "metadata": {
        "id": "lWJ0wewSwS64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_mse = sum(mse_list) / len(mse_list)\n",
        "mean_accuracy = sum(accuracy_list) / len(accuracy_list)\n",
        "mean_precision = sum(precision_list) / len(precision_list)\n",
        "mean_recall = sum(recall_list) / len(recall_list)\n",
        "mean_f1 = sum(f1_list) / len(f1_list)\n",
        "\n",
        "print(\"Mean MSE:\", mean_mse)\n",
        "print(\"Mean accuracy:\", mean_accuracy)\n",
        "print(\"Mean precision:\", mean_precision)\n",
        "print(\"Mean recall:\", mean_recall)\n",
        "print(\"Mean F1 score:\", mean_f1);\n",
        "\n",
        "# label9:\n",
        "# Mean MSE: 532.8807339449542\n",
        "# Mean accuracy: 0.1743119266055046\n",
        "# Mean precision: 0.1743119266055046\n",
        "# Mean recall: 0.1743119266055046\n",
        "# Mean F1 score: 0.1743119266055046\n",
        "\n",
        "# label14\n",
        "# Mean MSE: 381.71844660194176\n",
        "# Mean accuracy: 0.23300970873786409\n",
        "# Mean precision: 0.23300970873786409\n",
        "# Mean recall: 0.23300970873786409\n",
        "# Mean F1 score: 0.23300970873786409"
      ],
      "metadata": {
        "id": "77shFd16wS65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Create a table comparing actual and predicted scores\n",
        "comparison_table = pd.DataFrame({'Actual BDI Score': y_test_list, 'Predicted BDI Score': y_pred_list})\n",
        "table = tabulate(comparison_table, headers='keys', tablefmt='psql')\n",
        "print(table)\n"
      ],
      "metadata": {
        "id": "QbZloaRMwS65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert tensor to list\n",
        "y_true_converted = []\n",
        "for elem in y_test_list:\n",
        "    y_true_converted.append(int(elem[0].item()))\n",
        "\n",
        "# print(y_true_converted)"
      ],
      "metadata": {
        "id": "3ccyIeQawS66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "modify the size and color an size of the points based on Length"
      ],
      "metadata": {
        "id": "Nx1I_3iEwS68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_test_series = np.array(y_test_list)\n",
        "subset_data = data.loc[pd.DataFrame(y_test_list).index]\n",
        "\n",
        "# Plot actual and predicted scores with size variation\n",
        "plt.scatter(y_test_series, y_pred_list,  alpha=0.5 ,s=subset_data['length_seconds'],c=subset_data['length_seconds']) #s=subset_data['length_seconds']/60\n",
        "plt.plot([min(y_test_series), max(y_test_series)], [min(y_test_series), max(y_test_series)], color='red', linestyle='--')\n",
        "plt.colorbar(label='length_seconds')\n",
        "plt.xlabel('Actual BDI Score')\n",
        "plt.ylabel('Predicted BDI Score')\n",
        "plt.title('Actual vs Predicted BDI Score with Size Variation, size:length_seconds')\n",
        "plt.show();\n"
      ],
      "metadata": {
        "id": "hnsoW3wzwS68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Residual Plot:\n",
        "This plot shows the residuals (the differences between the actual and predicted values) against the predicted values. It helps to identify any patterns or trends in the residuals, which can provide insights into the model's performance."
      ],
      "metadata": {
        "id": "1xN3ZKqswS69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution of Residuals:\n",
        "This histogram shows the distribution of the residuals. It helps to check if the residuals follow a normal distribution, which is desired for a well-performing model."
      ],
      "metadata": {
        "id": "N47l-VIdwS69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "residuals =  y_true_converted -  np.array(y_pred_list)\n",
        "\n",
        "plt.hist(residuals, bins=20, alpha=0.5)\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Residuals')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pZ6zkEx3wS69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#visualized similarity matrix"
      ],
      "metadata": {
        "id": "hF2NhApCKum6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/gdrive/MyDrive/finaldata_with_vectors.csv')\n",
        "vectors= data.iloc[:, -768:]\n",
        "vectors"
      ],
      "metadata": {
        "id": "o9Gsp9Y1bQ-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(vectors)\n",
        "\n",
        "# print the similarity matrix\n",
        "similarity_matrix.shape"
      ],
      "metadata": {
        "id": "CzDoGCpIK6YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(vectors)\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(similarity_matrix, cmap='coolwarm')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# set the x and y ticks every 80 rows and columns\n",
        "ax.set_xticks(np.arange(0, 952, 80))\n",
        "ax.set_yticks(np.arange(0, 952, 80))\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"Similarity Matrix\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UPTjbTOILBvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sort images after sm"
      ],
      "metadata": {
        "id": "WnasE4F8f_SU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# generate the selected_rows arrays using a list comprehension\n",
        "selected_rows = [np.arange(i, similarity_matrix.shape[0], 8) for i in range(8)]\n",
        "# print the selected_rows arrays\n",
        "for i, rows in enumerate(selected_rows):\n",
        "    selected_rows[i]=rows\n",
        "\n",
        "# generate the selected_cols arrays using a list comprehension\n",
        "selected_cols = [np.arange(i, similarity_matrix.shape[0], 8) for i in range(8)]\n",
        "# print the selected_cols arrays\n",
        "for i, cols in enumerate(selected_cols):\n",
        "    selected_cols[i]=cols\n",
        "\n",
        "# generate the selected_matrix arrays using nested list comprehensions\n",
        "# print('selected_rows:',selected_rows)\n",
        "# print('selected_cols:',selected_cols)\n",
        "selected_matrix = [[similarity_matrix[rows][:, cols] for cols in selected_cols] for rows in selected_rows]\n",
        "# concatenate the selected_matrix arrays into a single DataFrame\n",
        "selected_matrix_df = pd.concat([pd.DataFrame(np.hstack(m)) for m in selected_matrix])\n",
        "# print( 'selected_matrix_df.shape:', selected_matrix_df.shape)"
      ],
      "metadata": {
        "id": "u4hkGXmmrhG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = selected_matrix_df\n",
        "\n",
        "# create a figure and axis object\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# plot the DataFrame\n",
        "im = ax.imshow(df, cmap='coolwarm')\n",
        "\n",
        "# set the x and y ticks and labels\n",
        "for i in range(8):\n",
        "    ax.axhline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.axvline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.text((i+0.5)*119, -50, f'pic{i+1}', ha='center', fontsize=10)\n",
        "    ax.text(-100, (i+0.5)*119, f'pic{i+1}', va='center', fontsize=10)\n",
        "\n",
        "# remove the x and y ticks\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "# add a colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title below the figure\n",
        "fig.suptitle('Similarity Matrix sort by pictures', fontsize=16, y=0.05, va='center')\n",
        "\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FA0iwRZoLXjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sort by images and BDI-score"
      ],
      "metadata": {
        "id": "VCXJSW955Aqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a sample DataFrame with shape 952 x 799 and random scores\n",
        "df = data\n",
        "\n",
        "# Define the indices of the groups of rows in the desired order - for images sorting\n",
        "group_indices = [[i+j for i in range(0, 952, 8)] for j in range(8)]\n",
        "\n",
        "# Sort each group of rows based on the \"score\" column separately - for feature sorting\n",
        "sorted_groups = []\n",
        "for group in group_indices:\n",
        "    sorted_group = df.iloc[group].sort_values('BDI_score')\n",
        "    sorted_groups.append(sorted_group)\n",
        "\n",
        "# Concatenate the sorted groups and the remaining unsorted rows\n",
        "sorted_df_by_images_and_BDI = pd.concat(sorted_groups + [df.drop(sum(group_indices, []))])\n",
        "\n",
        "# Display the sorted DataFrame\n",
        "print(sorted_df_by_images_and_BDI)\n"
      ],
      "metadata": {
        "id": "z3ucExac6i9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##compute SM"
      ],
      "metadata": {
        "id": "ZfT_e7tWHJQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(sorted_df_by_images_and_BDI.iloc[:, -768:])\n",
        "\n",
        "# print the similarity matrix\n",
        "print( 'sorted_df.shape:', sorted_df_by_images_and_BDI.shape,'similarity_matrix.shape:',similarity_matrix.shape)"
      ],
      "metadata": {
        "id": "sm4NbgL2HJRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(similarity_matrix,cmap='coolwarm')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# set the x and y ticks every 80 rows and columns\n",
        "ax.set_xticks(np.arange(0, 952, 119))\n",
        "ax.set_yticks(np.arange(0, 952, 119))\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"Similarity Matrix sort by images and BDI on them\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zqUCJB8vHJRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a figure and axis object\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# plot the DataFrame\n",
        "im = ax.imshow(similarity_matrix, cmap='coolwarm')\n",
        "\n",
        "# set the x and y ticks and labels\n",
        "for i in range(8):\n",
        "    ax.axhline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.axvline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.text((i+0.5)*119, -50, f'pic{i+1}', ha='center', fontsize=10)\n",
        "    ax.text(-100, (i+0.5)*119, f'pic{i+1}', va='center', fontsize=10)\n",
        "\n",
        "# remove the x and y ticks\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "# add a colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title below the figure\n",
        "fig.suptitle('Similarity Matrix sort by pictures and BDI', fontsize=16, y=0.05, va='center')\n",
        "\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "36c7IBFVHJRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## percentile - use MS"
      ],
      "metadata": {
        "id": "TDcfu6dNuiX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "# assume sim_matrix is your similarity matrix\n",
        "n_items = similarity_matrix.shape[0]\n",
        "\n",
        "# flatten the similarity matrix and compute ranks\n",
        "ranks = rankdata(similarity_matrix.flatten())\n",
        "\n",
        "# reshape the ranks back into the original matrix shape\n",
        "ranks_matrix = ranks.reshape((n_items, n_items))\n",
        "\n",
        "# compute percentiles by dividing ranks by total number of scores and multiplying by 100\n",
        "percentile_matrix = (ranks_matrix / len(ranks)) * 100\n",
        "\n",
        "# print the resulting percentile matrix\n",
        "percentile_matrix\n"
      ],
      "metadata": {
        "id": "ozvsf1JVuM0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(percentile_matrix,cmap='inferno')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# set the x and y ticks every 80 rows and columns\n",
        "ax.set_xticks(np.arange(0, 952, 119))\n",
        "ax.set_yticks(np.arange(0, 952, 119))\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"equalized Similarity Matrix sort by images and BDI on them\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dHB_rAV3uYFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a figure and axis object\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# plot the DataFrame\n",
        "im = ax.imshow(percentile_matrix, cmap='inferno')\n",
        "\n",
        "# set the x and y ticks and labels\n",
        "for i in range(8):\n",
        "    ax.axhline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.axvline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.text((i+0.5)*119, -50, f'pic{i+1}', ha='center', fontsize=10)\n",
        "    ax.text(-100, (i+0.5)*119, f'pic{i+1}', va='center', fontsize=10)\n",
        "\n",
        "# remove the x and y ticks\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "# add a colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title below the figure\n",
        "fig.suptitle('equalized Similarity Matrix sort by pictures and BDI on them', fontsize=16, y=0.05, va='center')\n",
        "\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FfplENgvLg5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # select the first 500 rows and columns of the similarity matrix\n",
        "# subset1 = percentile_matrix[:119, :119]\n",
        "\n",
        "# # select the last 500 rows and columns of the similarity matrix\n",
        "# subset2 = percentile_matrix[:119, 119:238]\n",
        "\n",
        "# # calculate the mean similarity of each subset\n",
        "# mean1 = np.mean(subset1)\n",
        "# mean2 = np.mean(subset2)\n",
        "\n",
        "# # print the mean similarity of each subset\n",
        "# print(\"Mean similarity of subset 1:\", mean1)\n",
        "# print(\"Mean similarity of subset 2:\", mean2)"
      ],
      "metadata": {
        "id": "nZZJmrQkvF8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sub plots"
      ],
      "metadata": {
        "id": "_boPR-SR5p3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a random matrix with shape (952, 952)\n",
        "matrix = percentile_matrix\n",
        "\n",
        "# Define the sub-matrix size\n",
        "sub_size = 119\n",
        "\n",
        "# Iterate over the matrix in steps of sub_size\n",
        "for i in range(0, matrix.shape[0], sub_size):\n",
        "    for j in range(0, matrix.shape[1], sub_size):\n",
        "        # Extract the sub-matrix\n",
        "        sub_matrix = matrix[i:i+sub_size, j:j+sub_size]\n",
        "\n",
        "        # Plot the sub-matrix\n",
        "        plt.imshow(sub_matrix, cmap=\"inferno\")\n",
        "\n",
        "        # Add a title to the plot\n",
        "        plt.title(f\"Sub-matrix ({int(i/119 )+1},{int(j/119)+1})\")\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "CnpQdbaCwniI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a random matrix with shape (952, 952)\n",
        "matrix = percentile_matrix\n",
        "\n",
        "# Define the sub-matrix size\n",
        "sub_sizei = 119\n",
        "sub_sizej = 952\n",
        "\n",
        "# Iterate over the matrix in steps of sub_size\n",
        "for i in range(0, matrix.shape[0], sub_sizei):\n",
        "    for j in range(0, matrix.shape[1], sub_sizej):\n",
        "        # Extract the sub-matrix\n",
        "        sub_matrix = matrix[i:i+sub_sizei, j:j+sub_sizej]\n",
        "\n",
        "        # Plot the sub-matrix\n",
        "        plt.imshow(sub_matrix, cmap=\"inferno\")\n",
        "\n",
        "        # Add a title to the plot\n",
        "        plt.title(f\"Sub-matrix ({int(i/119 )+1},all)\")\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "qGXX-ov97b-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns[:31]"
      ],
      "metadata": {
        "id": "VY4-Fyzk6rNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a sample DataFrame with shape 952 x 799 and random scores\n",
        "df = data\n",
        "\n",
        "# Define the indices of the groups of rows in the desired order - for images sorting\n",
        "group_indices = [[i+j for i in range(0, 952, 8)] for j in range(8)]\n",
        "\n",
        "# Sort each group of rows based on the \"score\" column separately - for feature sorting\n",
        "sorted_groups = []\n",
        "for group in group_indices:\n",
        "    sorted_group = df.iloc[group].sort_values('BDI_score')\n",
        "    sorted_groups.append(sorted_group)\n",
        "\n",
        "# Concatenate the sorted groups and the remaining unsorted rows\n",
        "sorted_df_by_images_and_BDI = pd.concat(sorted_groups + [df.drop(sum(group_indices, []))])\n",
        "\n",
        "# Display the sorted DataFrame\n",
        "print(sorted_df_by_images_and_BDI)\n"
      ],
      "metadata": {
        "id": "ezpcSe566aEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##compute SM"
      ],
      "metadata": {
        "id": "s8tOqihZ6aEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(sorted_df_by_images_and_BDI.iloc[:, -768:])\n",
        "\n",
        "# print the similarity matrix\n",
        "print( 'sorted_df.shape:', sorted_df_by_images_and_BDI.shape,'similarity_matrix.shape:',similarity_matrix.shape)"
      ],
      "metadata": {
        "id": "yRmuBtks6aEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(similarity_matrix,cmap='coolwarm')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# set the x and y ticks every 80 rows and columns\n",
        "ax.set_xticks(np.arange(0, 952, 119))\n",
        "ax.set_yticks(np.arange(0, 952, 119))\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"Similarity Matrix sort by images and BDI on them\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xd94jZ366aEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a figure and axis object\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# plot the DataFrame\n",
        "im = ax.imshow(similarity_matrix, cmap='coolwarm')\n",
        "\n",
        "# set the x and y ticks and labels\n",
        "for i in range(8):\n",
        "    ax.axhline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.axvline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.text((i+0.5)*119, -50, f'pic{i+1}', ha='center', fontsize=10)\n",
        "    ax.text(-100, (i+0.5)*119, f'pic{i+1}', va='center', fontsize=10)\n",
        "\n",
        "# remove the x and y ticks\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "# add a colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title below the figure\n",
        "fig.suptitle('Similarity Matrix sort by pictures and BDI', fontsize=16, y=0.05, va='center')\n",
        "\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nm5d3Ezz6aEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## percentile - use MS"
      ],
      "metadata": {
        "id": "_yemOeEM6aEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "# assume sim_matrix is your similarity matrix\n",
        "n_items = similarity_matrix.shape[0]\n",
        "\n",
        "# flatten the similarity matrix and compute ranks\n",
        "ranks = rankdata(similarity_matrix.flatten())\n",
        "\n",
        "# reshape the ranks back into the original matrix shape\n",
        "ranks_matrix = ranks.reshape((n_items, n_items))\n",
        "\n",
        "# compute percentiles by dividing ranks by total number of scores and multiplying by 100\n",
        "percentile_matrix = (ranks_matrix / len(ranks)) * 100\n",
        "\n",
        "# print the resulting percentile matrix\n",
        "percentile_matrix\n"
      ],
      "metadata": {
        "id": "0EycJK6U6aEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(percentile_matrix,cmap='inferno')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# set the x and y ticks every 80 rows and columns\n",
        "ax.set_xticks(np.arange(0, 952, 119))\n",
        "ax.set_yticks(np.arange(0, 952, 119))\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"equalized Similarity Matrix sort by images and BDI on them\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_JgH8ate6aEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a figure and axis object\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# plot the DataFrame\n",
        "im = ax.imshow(percentile_matrix, cmap='inferno')\n",
        "\n",
        "# set the x and y ticks and labels\n",
        "for i in range(8):\n",
        "    ax.axhline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.axvline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.text((i+0.5)*119, -50, f'pic{i+1}', ha='center', fontsize=10)\n",
        "    ax.text(-100, (i+0.5)*119, f'pic{i+1}', va='center', fontsize=10)\n",
        "\n",
        "# remove the x and y ticks\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "# add a colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title below the figure\n",
        "fig.suptitle('equalized Similarity Matrix sort by pictures and BDI on them', fontsize=16, y=0.05, va='center')\n",
        "\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hl0NA4up6aEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # select the first 500 rows and columns of the similarity matrix\n",
        "# subset1 = percentile_matrix[:119, :119]\n",
        "\n",
        "# # select the last 500 rows and columns of the similarity matrix\n",
        "# subset2 = percentile_matrix[:119, 119:238]\n",
        "\n",
        "# # calculate the mean similarity of each subset\n",
        "# mean1 = np.mean(subset1)\n",
        "# mean2 = np.mean(subset2)\n",
        "\n",
        "# # print the mean similarity of each subset\n",
        "# print(\"Mean similarity of subset 1:\", mean1)\n",
        "# print(\"Mean similarity of subset 2:\", mean2)"
      ],
      "metadata": {
        "id": "Wg9CYKt-6aEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sub plots"
      ],
      "metadata": {
        "id": "T9PC1Vr36aEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a random matrix with shape (952, 952)\n",
        "matrix = percentile_matrix\n",
        "\n",
        "# Define the sub-matrix size\n",
        "sub_size = 119\n",
        "\n",
        "# Iterate over the matrix in steps of sub_size\n",
        "for i in range(0, matrix.shape[0], sub_size):\n",
        "    for j in range(0, matrix.shape[1], sub_size):\n",
        "        # Extract the sub-matrix\n",
        "        sub_matrix = matrix[i:i+sub_size, j:j+sub_size]\n",
        "\n",
        "        # Plot the sub-matrix\n",
        "        plt.imshow(sub_matrix, cmap=\"inferno\")\n",
        "\n",
        "        # Add a title to the plot\n",
        "        plt.title(f\"Sub-matrix ({int(i/119 )+1},{int(j/119)+1})\")\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "g5xVCtRJ6aEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11. jump of idees & correlation:"
      ],
      "metadata": {
        "id": "bM-42AT4k8Ot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# jump of idees - tocken based embedding"
      ],
      "metadata": {
        "id": "_CXYxAuI3p0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "3m7Q_xcNJXY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJsirBRW2KAX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import statistics\n",
        "import numpy as np\n",
        "from transformers import BertModel, BertTokenizerFast\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/persian_dataset_with_graph_attributes_sentiment_jumping.csv') #data.csv\n",
        "df = df[['transcript', 'id', 'file_name','BDI_score']]"
      ],
      "metadata": {
        "id": "H8TIqm8dn6o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **28*28 tfidf**"
      ],
      "metadata": {
        "id": "WX6_nM2CMyVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def cal_single_metrics(name, embeddings):\n",
        "    if name=='euclidean':\n",
        "        return np.linalg.norm(embeddings[0]-embeddings[1])\n",
        "    elif name=='cosine':\n",
        "        return cosine_similarity(embeddings[0].reshape(1,-1), embeddings[1].reshape(1,-1))[0][0]\n",
        "\n",
        "df = df[['transcript', 'id', 'file_name','BDI_score']]\n",
        "data = df.to_numpy()\n",
        "out_data = []\n",
        "\n",
        "# initialize TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "for i in range(0, len(data), 8):\n",
        "    tmp_euclidean = np.zeros((8, 8))\n",
        "    tmp_cosine = np.zeros((8, 8))\n",
        "    sentences = [data[i+j][0] for j in range(8)]\n",
        "    embs = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    for j in range(8):\n",
        "        for k in range(j+1, 8):\n",
        "            tmp_euclidean[j][k] = cal_single_metrics('euclidean', [embs[j].toarray(), embs[k].toarray()])\n",
        "            tmp_cosine[j][k] = cal_single_metrics('cosine', [embs[j].toarray(), embs[k].toarray()])\n",
        "\n",
        "    tmp_euclidean = tmp_euclidean[np.triu_indices(8, k=1)]\n",
        "    tmp_cosine = tmp_cosine[np.triu_indices(8, k=1)]\n",
        "\n",
        "    out_data.append([tmp_euclidean, tmp_cosine, data[i][1]])\n",
        "\n",
        "df_out28 = pd.DataFrame(out_data, columns=['28xeuclidean', '28xcosine', 'id'])\n",
        "\n",
        "df_out28.to_csv('/content/gdrive/MyDrive/28x-tfidf.csv')\n",
        "\n",
        "df_out28"
      ],
      "metadata": {
        "id": "0QXn73sljjBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out28['28xcosine'][118]"
      ],
      "metadata": {
        "id": "G39ow1EbsjtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **emb with tfidf** for SM"
      ],
      "metadata": {
        "id": "QmICfWD8Ml4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "data = df.to_numpy()\n",
        "out_data = []\n",
        "\n",
        "# Initializing TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "# Fitting the vectorizer on the data\n",
        "tfidf.fit(data[:, 0])\n",
        "\n",
        "# Transforming the sentences into TF-IDF embeddings\n",
        "embeddings = tfidf.transform(data[:, 0]).toarray()\n",
        "\n",
        "for i in range(len(data)):\n",
        "    out_data.append([data[i][0], data[i][1], data[i][2], embeddings[i]])"
      ],
      "metadata": {
        "id": "Y-g2k5CHHTON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "E7s_AEGn57MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out = pd.DataFrame(out_data, columns=['transcript', 'id', 'file_name', 'emb'])\n",
        "df_out"
      ],
      "metadata": {
        "id": "9cGaW32iop6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out.to_csv('/content/gdrive/MyDrive/tfidgvector.csv') #labse"
      ],
      "metadata": {
        "id": "VC6s8FEpqdVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.shape"
      ],
      "metadata": {
        "id": "amBf-XypJJCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **corr**"
      ],
      "metadata": {
        "id": "MyWho8CmjgEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/persian_dataset_with_graph_attributes_sentiment_jumping.csv') #data.csv\n",
        "df = df[['transcript', 'id', 'file_name','BDI_score']]\n",
        "\n",
        "data = df.to_numpy()\n",
        "out_data = []\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "for i in tqdm(data):\n",
        "    i[0] = i[0].replace('\"', '')\n",
        "    i[0] = i[0].replace('\\n', '')\n",
        "    tmp = i[0].split('.')\n",
        "    tmp = [j for j in tmp if j!='']\n",
        "    if len(tmp)>1:\n",
        "        emb = vectorizer.fit_transform(tmp)\n",
        "        euclidean = np.linalg.norm(emb.toarray()[1:] - emb.toarray()[:-1], axis=1).mean(), np.linalg.norm(emb.toarray()[1:] - emb.toarray()[:-1], axis=1).std()\n",
        "        euclidean_sum = np.linalg.norm(emb.toarray()[1:] - emb.toarray()[:-1], axis=1).sum()\n",
        "        cosine = cosine_similarity(emb.toarray()[1:], emb.toarray()[:-1]).mean(), cosine_similarity(emb.toarray()[1:], emb.toarray()[:-1]).std()\n",
        "        cosine_sum = cosine_similarity(emb.toarray()[1:], emb.toarray()[:-1]).sum()\n",
        "\n",
        "        out_data.append([i[0], i[1], i[2], euclidean[0], euclidean[1], euclidean_sum, cosine[0], cosine[1], cosine_sum])\n",
        "    else:\n",
        "        out_data.append([i[0], i[1], i[2], None, None, None, None, None, None])\n"
      ],
      "metadata": {
        "id": "9u1KqramO4uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out = pd.DataFrame(out_data, columns=['transcript', 'id', 'file_name', 'euclidean_mean', 'euclidean_std', 'euclidean_sum', 'cosine_mean', 'cosine_std', 'cosine_sum'])\n",
        "# df_out.to_csv('jump.csv')\n",
        "df_out"
      ],
      "metadata": {
        "id": "KKe2-_KFhOgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = pd.read_csv('/content/gdrive/MyDrive/persian_dataset_with_graph_attributes_sentiment_jumping.csv')\n",
        "df_final"
      ],
      "metadata": {
        "id": "RnL7d82OGj-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.columns"
      ],
      "metadata": {
        "id": "q08CxLlNPsBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged = pd.merge(df_final, df_out, on='file_name', how='inner')\n",
        "df_merged = df_merged.reset_index()\n",
        "\n",
        "df_merged"
      ],
      "metadata": {
        "id": "3X3SWUTOG2z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "good_features = []\n",
        "\n",
        "for j in range(8):\n",
        "    print(f'Image {j+1}')\n",
        "    corr = df_merged.loc[j::8][['BDI_score', 'euclidean_mean_y', 'euclidean_std_y', 'euclidean_sum_y', 'cosine_mean_y', 'cosine_std_y', 'cosine_sum_y']].corr(method='pearson', numeric_only=True)\n",
        "    tmp = df_merged.loc[j::8][['BDI_score', 'euclidean_mean_y', 'euclidean_std_y', 'euclidean_sum_y', 'cosine_mean_y', 'cosine_std_y', 'cosine_sum_y']].count()\n",
        "\n",
        "    for i in ['euclidean_mean_y', 'euclidean_std_y', 'euclidean_sum_y', 'cosine_mean_y', 'cosine_std_y', 'cosine_sum_y']:\n",
        "        print(i)\n",
        "        print('corr: ', corr['BDI_score'][i])\n",
        "        print('significant: ', 2/math.sqrt(tmp[i]))\n",
        "        if corr['BDI_score'][i]>=2/math.sqrt(tmp[i]):\n",
        "            good_features.append((f'Image {j+1}', i, corr['BDI_score'][i]-2/math.sqrt(tmp[i])))\n",
        "        print('--------------')"
      ],
      "metadata": {
        "id": "BrcpnCOtUNer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "good_features"
      ],
      "metadata": {
        "id": "35mdrHDEnkX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_merged.loc[1::8][['id_x', 'euclidean_std_y']].to_csv('feature_1.csv')"
      ],
      "metadata": {
        "id": "nh-aGe5Wt_j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_merged.loc[4::8][['id_x', 'euclidean_std_y']].to_csv('feature_2.csv')"
      ],
      "metadata": {
        "id": "o8vgRofluay6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_merged.loc[4::8][['id_x', 'euclidean_mean_y']].to_csv('feature_3.csv')\n",
        "# df_merged.loc[1::8][['id_x', 'cosine_sum_y']].to_csv('feature_4.csv')\n",
        "# df_merged.loc[::8][['id_x', 'euclidean_std_y']].to_csv('feature_5.csv')"
      ],
      "metadata": {
        "id": "nHtFHy2OgpEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_merged.loc[df_merged['BDI_score'] == 31, 'index'].iloc[0]"
      ],
      "metadata": {
        "id": "2Tsh3XtagPsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_merged[df_merged['BDI_score'] != 31]"
      ],
      "metadata": {
        "id": "Yfu_8vJpg0E0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt_array = df_merged.loc[1::8][['BDI_score', 'euclidean_std_y']].to_numpy()\n",
        "x = plt_array.T[0]\n",
        "y = plt_array.T[1]\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.xlabel('BDI Score')\n",
        "plt.ylabel('Euclidean Standard Deviation')\n",
        "plt.title('Relationship between BDI Score and Euclidean Standard Deviation in picture 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0gIveADntUBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_merged[df_merged['BDI_score'] != 31]\n",
        "plt_array = df_merged.loc[3::8][['BDI_score', 'euclidean_std_y']].to_numpy()\n",
        "x = plt_array.T[0]\n",
        "y = plt_array.T[1]\n",
        "plt.scatter(x, y)\n",
        "plt.xlabel('BDI Score')\n",
        "plt.ylabel('Euclidean Standard Deviation')\n",
        "plt.title('Relationship between BDI Score and Euclidean Standard Deviation in picture 4')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8NVVhDAEuWJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bounder"
      ],
      "metadata": {
        "id": "9L9_HaWObJRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt_array = df_merged.loc[4::8][['BDI_score', 'euclidean_mean_y']].to_numpy()\n",
        "x = plt_array.T[0]\n",
        "y = plt_array.T[1]\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.xlabel('BDI Score')\n",
        "plt.ylabel('Euclidean mean')\n",
        "plt.title('Relationship between BDI Score and Euclidean mean in picture 5')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9LPilCyybJqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt_array = df_merged.loc[1::8][['BDI_score', 'cosine_sum_y']].to_numpy()\n",
        "x = plt_array.T[0]\n",
        "y = plt_array.T[1]\n",
        "plt.scatter(x, y)\n",
        "plt.xlabel('BDI Score')\n",
        "plt.ylabel('cosine_sum')\n",
        "plt.title('Relationship between BDI Score and cosine_sum in picture 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kNFQLQzkbJqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt_array = df_merged.loc[0::8][['BDI_score', 'euclidean_std_y']].to_numpy()\n",
        "x = plt_array.T[0]\n",
        "y = plt_array.T[1]\n",
        "plt.scatter(x, y)\n",
        "plt.xlabel('BDI Score')\n",
        "plt.ylabel('euclidean_std')\n",
        "plt.title('Relationship between BDI Score and euclidean_std in picture 1')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cvh94AVkbxwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# jump of idees - mean word based embedding"
      ],
      "metadata": {
        "id": "ac28-Ccj3tTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!sudo apt install python-pip\n",
        "# !pip install hazm\n",
        "#restart runtime"
      ],
      "metadata": {
        "id": "4yyId9Rr_vBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals\n",
        "# from hazm import *\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import torch"
      ],
      "metadata": {
        "id": "BEUysYsJ_rfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use GPU\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# # Load the ParsBERT tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "model = AutoModel.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")"
      ],
      "metadata": {
        "id": "Y0vVLaUr_raz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import hazm\n",
        "import transformers\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/persian_dataset_with_graph_attributes_sentiment_jumping.csv') #data.csv\n",
        "data = data[['transcript', 'id', 'file_name','BDI_score']]\n",
        "data"
      ],
      "metadata": {
        "id": "L9yRPDVI4VZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## start"
      ],
      "metadata": {
        "id": "r7SfEGknK3nK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### vectorized all for similarity matrix"
      ],
      "metadata": {
        "id": "yLXAlLS6itAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to vectorize the text\n",
        "def get_emb(text):\n",
        "    # Tokenize the text\n",
        "    encoded_input = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "    # Pass the tokenized input through the model\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    # Extract the last hidden state of the model as the vector representation of the text\n",
        "    last_hidden_states = model_output.last_hidden_state\n",
        "    sentence_embeddings = np.mean(last_hidden_states.numpy(), axis=1)\n",
        "    mean_embedding = np.mean(sentence_embeddings, axis=0).tolist()\n",
        "    return mean_embedding"
      ],
      "metadata": {
        "id": "9XvB-TBvI8EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "data = data.to_numpy()\n",
        "out_data = []\n",
        "\n",
        "for i in tqdm(data):\n",
        "    i[0] = i[0].replace('\"', '')\n",
        "    i[0] = i[0].replace('\\n', '')\n",
        "    tmp = i[0].split('.')\n",
        "    emb = get_emb(i[0])\n",
        "    out_data.append([i[0], i[1], i[2], emb])"
      ],
      "metadata": {
        "id": "IshqSFcZJhcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get emb 952\n",
        "df_out = pd.DataFrame(out_data, columns=['transcript', 'id', 'file_name', 'emb'])\n",
        "df_out"
      ],
      "metadata": {
        "id": "HSNMXB_tJrDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cal_single_metrics(name, embeddings):\n",
        "    if name=='euclidean':\n",
        "        return np.linalg.norm(embeddings[0]-embeddings[1])\n",
        "\n",
        "    elif name=='cosine':\n",
        "        return dot(embeddings[0], embeddings[1])/(norm(embeddings[0])*norm(embeddings[1]))"
      ],
      "metadata": {
        "id": "oopCusn8Jq_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gwt emb 28*28\n",
        "out_data = []\n",
        "\n",
        "\n",
        "for i in tqdm(data):\n",
        "    i[0] = i[0].replace('\"', '')\n",
        "    i[0] = i[0].replace('\\n', '')\n",
        "\n",
        "for i in tqdm(range(0, len(data), 8)):\n",
        "    tmp_euclidean = np.zeros((8, 8))\n",
        "    tmp_cosine = np.zeros((8, 8))\n",
        "    embs = []\n",
        "    for j in range(8):\n",
        "        embs.append(get_emb(data[i+j][0]))\n",
        "\n",
        "    for j in range(8):\n",
        "        for k in range(8):\n",
        "            tmp_euclidean[j][k] = cal_single_metrics('euclidean', [embs[j][0],embs[k][0]])\n",
        "            tmp_cosine[j][k] = cal_single_metrics('cosine', [embs[j][0],embs[k][0]])\n",
        "    tmp_euclidean = tmp_euclidean[np.triu_indices(8, k=1)]\n",
        "    tmp_cosine = tmp_cosine[np.triu_indices(8, k=1)]\n",
        "\n",
        "    out_data.append([tmp_euclidean, tmp_cosine, data[i][1]])\n"
      ],
      "metadata": {
        "id": "-zJlbEV7Jq8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get emb 28 * 28\n",
        "df_out= pd.DataFrame(out_data, columns=['28xeuclidean', '28xcosine', 'id'])\n",
        "df_out.to_csv('/content/gdrive/MyDrive/28parsbertx.csv')\n",
        "df_out"
      ],
      "metadata": {
        "id": "Xy1l80OYKpzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**compute features**"
      ],
      "metadata": {
        "id": "KAhZKT16h9BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_metrics(name, embeddings):\n",
        "    if name=='euclidean':\n",
        "        tmp = []\n",
        "        for i in range(len(embeddings)-1):\n",
        "            tmp.append(np.linalg.norm(embeddings[i]-embeddings[i+1]))\n",
        "        if len(tmp)>1:\n",
        "            return statistics.mean(tmp), statistics.stdev(tmp)\n",
        "        else:\n",
        "            return statistics.mean(tmp), None\n",
        "\n",
        "    elif name=='cosine':\n",
        "        tmp = []\n",
        "        for i in range(len(embeddings)-1):\n",
        "            tmp.append(dot(embeddings[i], embeddings[i+1])/(norm(embeddings[i])*norm(embeddings[i+1])))\n",
        "        if len(tmp)>1:\n",
        "            return statistics.mean(tmp), statistics.stdev(tmp)\n",
        "        else:\n",
        "            return statistics.mean(tmp), None\n",
        "\n",
        "    elif name=='euclidean_sum':\n",
        "        s = 0\n",
        "        for i in range(len(embeddings)-1):\n",
        "            s += np.linalg.norm(embeddings[i]-embeddings[i+1])\n",
        "        return s\n",
        "\n",
        "    elif name=='cosine_sum':\n",
        "        s = 0\n",
        "        for i in range(len(embeddings)-1):\n",
        "            s += dot(embeddings[i], embeddings[i+1])/(norm(embeddings[i])*norm(embeddings[i+1]))\n",
        "        return s\n",
        "\n",
        "\n",
        "def get_emb(sentences):\n",
        "    sent_inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**sent_inputs)\n",
        "\n",
        "    embeddings = outputs.pooler_output\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "lMNqEz9hKpvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "\n",
        "out_data = []\n",
        "\n",
        "for i in tqdm(data):\n",
        "    i[0] = i[0].replace('\"', '')\n",
        "    i[0] = i[0].replace('\\n', '')\n",
        "    tmp = i[0].split('.')\n",
        "    tmp = [j for j in tmp if j!='']\n",
        "    if len(tmp)>1:\n",
        "        emb = get_emb(tmp)\n",
        "        euclidean = cal_metrics('euclidean', emb)\n",
        "        euclidean_sum = cal_metrics('euclidean_sum', emb)\n",
        "        cosine = cal_metrics('cosine', emb)\n",
        "        cosine_sum = cal_metrics('cosine_sum', emb)\n",
        "\n",
        "        out_data.append([i[0], i[1], i[2], euclidean[0], euclidean[1], euclidean_sum, cosine[0], cosine[1], cosine_sum])\n",
        "    else:\n",
        "        out_data.append([i[0], i[1], i[2], None, None, None, None, None, None])"
      ],
      "metadata": {
        "id": "6qK2KwiJOQNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **corr**"
      ],
      "metadata": {
        "id": "PAdeipyXihID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_out = pd.DataFrame(out_data, columns=['transcript', 'id', 'file_name', 'euclidean_mean', 'euclidean_std', 'euclidean_sum', 'cosine_mean', 'cosine_std', 'cosine_sum'])\n",
        "df_out.to_csv('/content/gdrive/MyDrive/parsbertjump.csv')\n",
        "df_out"
      ],
      "metadata": {
        "id": "5C5D5BSkOQKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final =pd.read_csv('/content/gdrive/MyDrive/persian_dataset_with_graph_attributes_sentiment_jumping.csv')\n",
        "df_final"
      ],
      "metadata": {
        "id": "GPVWdUPTOQHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged = pd.merge(df_final, df_out, on='file_name', how='inner')\n",
        "df_merged = df_merged.reset_index()\n",
        "\n",
        "df_merged"
      ],
      "metadata": {
        "id": "XeW-jSMePR3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "good_features = []\n",
        "\n",
        "for j in range(8):\n",
        "    print(f'Image {j+1}')\n",
        "    corr = df_merged.loc[j::8][['BDI_score', 'euclidean_mean_y', 'euclidean_std_y', 'euclidean_sum_y', 'cosine_mean_y', 'cosine_std_y', 'cosine_sum_y']].corr(method='pearson', numeric_only=True)\n",
        "    tmp = df_merged.loc[j::8][['BDI_score', 'euclidean_mean_y', 'euclidean_std_y', 'euclidean_sum_y', 'cosine_mean_y', 'cosine_std_y', 'cosine_sum_y']].count()\n",
        "\n",
        "    for i in ['euclidean_mean_y', 'euclidean_std_y', 'euclidean_sum_y', 'cosine_mean_y', 'cosine_std_y', 'cosine_sum_y']:\n",
        "        print(i)\n",
        "        print('corr: ', corr['BDI_score'][i])\n",
        "        print('significant: ', 2/math.sqrt(tmp[i]))\n",
        "        if corr['BDI_score'][i]>=2/math.sqrt(tmp[i]):\n",
        "            good_features.append((f'Image {j+1}', i, corr['BDI_score'][i]-2/math.sqrt(tmp[i])))\n",
        "        print('--------------')"
      ],
      "metadata": {
        "id": "4gHfr_YBPR0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "good_features"
      ],
      "metadata": {
        "id": "QxXnMtiiPRv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt_array = df_merged.loc[7::8][['BDI_score', 'euclidean_std_y']].to_numpy()\n",
        "x = plt_array.T[0]\n",
        "y = plt_array.T[1]\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.xlabel('BDI Score')\n",
        "plt.ylabel('Euclidean Standard Deviation')\n",
        "plt.title('Relationship between BDI Score and Euclidean Standard Deviation in picture 8')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QzJ-7EPoPehF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt_array = df_merged.loc[7::8][['BDI_score', 'cosine_std_y']].to_numpy()\n",
        "x = plt_array.T[0]\n",
        "y = plt_array.T[1]\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.xlabel('BDI Score')\n",
        "plt.ylabel('cosine Standard Deviation')\n",
        "plt.title('Relationship between BDI Score and cosine Standard Deviation in picture 8')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "89EB9XLoPeaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### vectorize for similarity function"
      ],
      "metadata": {
        "id": "4cag5JRzKqnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Define a function to vectorize the text\n",
        "def vectorize_text(text):\n",
        "    # Tokenize the text\n",
        "    encoded_input = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "    # Pass the tokenized input through the model\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    # Extract the last hidden state of the model as the vector representation of the text\n",
        "    last_hidden_states = model_output.last_hidden_state\n",
        "    sentence_embeddings = np.mean(last_hidden_states.numpy(), axis=1)\n",
        "    mean_embedding = np.mean(sentence_embeddings, axis=0).tolist()\n",
        "    return mean_embedding\n"
      ],
      "metadata": {
        "id": "wbfMoCdkAMtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = data\n",
        "\n",
        "# Vectorize the text in the dataset\n",
        "vectors = []\n",
        "for text in dataset['transcript']:\n",
        "    vector = vectorize_text(text)\n",
        "    vectors.append(vector)\n",
        "\n",
        "# Add the vectors as columns to the dataset\n",
        "vector_columns = ['vector_' + str(i) for i in range(len(vectors[0]))]\n",
        "for i, column in enumerate(vector_columns):\n",
        "    dataset[column] = [vector[i] for vector in vectors]\n",
        "\n",
        "vectorized_all_dataset = dataset"
      ],
      "metadata": {
        "id": "XnAaTnyjGYBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized_all_dataset"
      ],
      "metadata": {
        "id": "6UKFW-aZBwnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized_all_dataset.to_csv('/content/gdrive/MyDrive/vectorized_all_dataset_mean_sentence-based_Parsbert.csv', index=False)"
      ],
      "metadata": {
        "id": "XKgsdO-vEHWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dPaZjnMTkrcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# jump of idees - sentence based embedding"
      ],
      "metadata": {
        "id": "eM11OmMfIB9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "W32hvbP1kr7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pQO3pWakr7e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import statistics\n",
        "import numpy as np\n",
        "from transformers import BertModel, BertTokenizerFast\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"setu4993/LaBSE\")\n",
        "model = BertModel.from_pretrained(\"setu4993/LaBSE\")\n",
        "model = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_emb(sentences):\n",
        "    sent_inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**sent_inputs)\n",
        "\n",
        "    embeddings = outputs.pooler_output\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "NTSvsNLRo2zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data.csv')\n",
        "df = df[['transcript', 'id', 'file_name']]\n",
        "data = df.to_numpy()\n",
        "out_data = []\n",
        "\n",
        "for i in tqdm(data):\n",
        "    i[0] = i[0].replace('\"', '')\n",
        "    i[0] = i[0].replace('\\n', '')\n",
        "    tmp = i[0].split('.')\n",
        "    emb = get_emb(i[0])\n",
        "    out_data.append([i[0], i[1], i[2], emb])\n"
      ],
      "metadata": {
        "id": "GHf9R6oxkr7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out = pd.DataFrame(out_data, columns=['transcript', 'id', 'file_name', 'emb'])\n",
        "df_out"
      ],
      "metadata": {
        "id": "yF9H7u59kr7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out.to_csv('labse.csv')"
      ],
      "metadata": {
        "id": "EewV_rz1kr7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_single_metrics(name, embeddings):\n",
        "    if name=='euclidean':\n",
        "        return np.linalg.norm(embeddings[0]-embeddings[1])\n",
        "\n",
        "    elif name=='cosine':\n",
        "        return dot(embeddings[0], embeddings[1])/(norm(embeddings[0])*norm(embeddings[1]))\n",
        "\n",
        "\n",
        "def get_emb(sentences):\n",
        "    sent_inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**sent_inputs)\n",
        "\n",
        "    embeddings = outputs.pooler_output\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "STkSs-xaAB2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data.csv')\n",
        "df = df[['transcript', 'id', 'file_name']]\n",
        "data = df.to_numpy()\n",
        "out_data = []\n",
        "\n",
        "\n",
        "for i in tqdm(data):\n",
        "    i[0] = i[0].replace('\"', '')\n",
        "    i[0] = i[0].replace('\\n', '')\n",
        "\n",
        "for i in tqdm(range(0, len(data), 8)):\n",
        "    tmp_euclidean = np.zeros((8, 8))\n",
        "    tmp_cosine = np.zeros((8, 8))\n",
        "    embs = []\n",
        "    for j in range(8):\n",
        "        embs.append(get_emb(data[i+j][0]))\n",
        "\n",
        "    for j in range(8):\n",
        "        for k in range(8):\n",
        "            tmp_euclidean[j][k] = cal_single_metrics('euclidean', [embs[j][0],embs[k][0]])\n",
        "            tmp_cosine[j][k] = cal_single_metrics('cosine', [embs[j][0],embs[k][0]])\n",
        "    tmp_euclidean = tmp_euclidean[np.triu_indices(8, k=1)]\n",
        "    tmp_cosine = tmp_cosine[np.triu_indices(8, k=1)]\n",
        "\n",
        "    out_data.append([tmp_euclidean, tmp_cosine, data[i][1]])\n"
      ],
      "metadata": {
        "id": "bODq18bd5jQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out = pd.DataFrame(out_data, columns=['28xeuclidean', '28xcosine', 'id'])\n",
        "df_out"
      ],
      "metadata": {
        "id": "tRT0H3hp_n-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out.to_csv('28x.csv')"
      ],
      "metadata": {
        "id": "D-CGfKup_42H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_metrics(name, embeddings):\n",
        "    if name=='euclidean':\n",
        "        tmp = []\n",
        "        for i in range(len(embeddings)-1):\n",
        "            tmp.append(np.linalg.norm(embeddings[i]-embeddings[i+1]))\n",
        "        if len(tmp)>1:\n",
        "            return statistics.mean(tmp), statistics.stdev(tmp)\n",
        "        else:\n",
        "            return statistics.mean(tmp), None\n",
        "\n",
        "    elif name=='cosine':\n",
        "        tmp = []\n",
        "        for i in range(len(embeddings)-1):\n",
        "            tmp.append(dot(embeddings[i], embeddings[i+1])/(norm(embeddings[i])*norm(embeddings[i+1])))\n",
        "        if len(tmp)>1:\n",
        "            return statistics.mean(tmp), statistics.stdev(tmp)\n",
        "        else:\n",
        "            return statistics.mean(tmp), None\n",
        "\n",
        "    elif name=='euclidean_sum':\n",
        "        s = 0\n",
        "        for i in range(len(embeddings)-1):\n",
        "            s += np.linalg.norm(embeddings[i]-embeddings[i+1])\n",
        "        return s\n",
        "\n",
        "    elif name=='cosine_sum':\n",
        "        s = 0\n",
        "        for i in range(len(embeddings)-1):\n",
        "            s += dot(embeddings[i], embeddings[i+1])/(norm(embeddings[i])*norm(embeddings[i+1]))\n",
        "        return s\n",
        "\n",
        "\n",
        "def get_emb(sentences):\n",
        "    sent_inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**sent_inputs)\n",
        "\n",
        "    embeddings = outputs.pooler_output\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "MUiWp9-fGgfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data.csv')\n",
        "df = df[['transcript', 'id', 'file_name']]\n",
        "data = df.to_numpy()\n",
        "out_data = []\n",
        "\n",
        "for i in tqdm(data):\n",
        "    i[0] = i[0].replace('\"', '')\n",
        "    i[0] = i[0].replace('\\n', '')\n",
        "    tmp = i[0].split('.')\n",
        "    tmp = [j for j in tmp if j!='']\n",
        "    if len(tmp)>1:\n",
        "        emb = get_emb(tmp)\n",
        "        euclidean = cal_metrics('euclidean', emb)\n",
        "        euclidean_sum = cal_metrics('euclidean_sum', emb)\n",
        "        cosine = cal_metrics('cosine', emb)\n",
        "        cosine_sum = cal_metrics('cosine_sum', emb)\n",
        "\n",
        "        out_data.append([i[0], i[1], i[2], euclidean[0], euclidean[1], euclidean_sum, cosine[0], cosine[1], cosine_sum])\n",
        "    else:\n",
        "        out_data.append([i[0], i[1], i[2], None, None, None, None, None, None])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cXNz9T9AJkJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_out = pd.DataFrame(out_data, columns=['transcript', 'id', 'file_name', 'euclidean_mean', 'euclidean_std', 'euclidean_sum', 'cosine_mean', 'cosine_std', 'cosine_sum'])\n",
        "df_out.to_csv('jump.csv')\n",
        "df_out"
      ],
      "metadata": {
        "id": "Zf-WXqpQkr7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = pd.read_csv('final_data.csv')\n",
        "df_final"
      ],
      "metadata": {
        "id": "U-jRwoAIkr7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged = pd.merge(df_final, df_out, on='file_name', how='inner')\n",
        "df_merged = df_merged.reset_index()\n",
        "\n",
        "df_merged"
      ],
      "metadata": {
        "id": "jfEiFKCAkr7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "good_features = []\n",
        "\n",
        "for j in range(8):\n",
        "    print(f'Image {j+1}')\n",
        "    corr = df_merged.loc[j::8][['BDI_score', 'euclidean_mean_y', 'euclidean_std_y', 'euclidean_sum_y', 'cosine_mean_y', 'cosine_std_y', 'cosine_sum_y']].corr(method='pearson', numeric_only=True)\n",
        "    tmp = df_merged.loc[j::8][['BDI_score', 'euclidean_mean_y', 'euclidean_std_y', 'euclidean_sum_y', 'cosine_mean_y', 'cosine_std_y', 'cosine_sum_y']].count()\n",
        "\n",
        "    for i in ['euclidean_mean_y', 'euclidean_std_y', 'euclidean_sum_y', 'cosine_mean_y', 'cosine_std_y', 'cosine_sum_y']:\n",
        "        print(i)\n",
        "        print('corr: ', corr['BDI_score'][i])\n",
        "        print('significant: ', 2/math.sqrt(tmp[i]))\n",
        "        if corr['BDI_score'][i]>=2/math.sqrt(tmp[i]):\n",
        "            good_features.append((f'Image {j+1}', i, corr['BDI_score'][i]-2/math.sqrt(tmp[i])))\n",
        "        print('--------------')"
      ],
      "metadata": {
        "id": "t-cpiWrTkr7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "good_features"
      ],
      "metadata": {
        "id": "IyitMqm3kr7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.loc[::8][['id_x', 'euclidean_std_y']].to_csv('feature_1.csv')"
      ],
      "metadata": {
        "id": "6aIIGCKnkr7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.loc[6::8][['id_x', 'euclidean_std_y']].to_csv('feature_2.csv')"
      ],
      "metadata": {
        "id": "uz8mrVWnkr7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt_array = df_merged.loc[0::8][['BDI_score', 'euclidean_std_y']].to_numpy()\n",
        "x = plt_array.T[0]\n",
        "y = plt_array.T[1]\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gpWOwLLkkr7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt_array = df_merged.loc[6::8][['BDI_score', 'euclidean_std_y']].to_numpy()\n",
        "x = plt_array.T[0]\n",
        "y = plt_array.T[1]\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Naa9OoIXkr7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt_array = df_merged.loc[3::8][['BDI_score', 'euclidean_std_y']].to_numpy()\n",
        "x = plt_array.T[0]\n",
        "y = plt_array.T[1]\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CWsZKgonuY3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt_array = df_merged.loc[4::8][['BDI_score', 'cosine_std_y']].to_numpy()\n",
        "x = plt_array.T[0]\n",
        "y = plt_array.T[1]\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "noOLq8VXuh2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. percentile similarity matrix :"
      ],
      "metadata": {
        "id": "uqkGYHCilFfe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDgOG2Ogl0n1"
      },
      "source": [
        "#percentile similarity matrix : sort by images and BDI on tocken based - tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_vBYpNvl0n2"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/gdrive/MyDrive/finaldata_with_vectors.csv')\n",
        "vectors= data.iloc[:, -768:]\n",
        "vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K3jpMMsl0n2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(vectors)\n",
        "\n",
        "# print the similarity matrix\n",
        "similarity_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keEve2R0l0n3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(vectors)\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(similarity_matrix, cmap='coolwarm')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# set the x and y ticks every 80 rows and columns\n",
        "ax.set_xticks(np.arange(0, 952, 80))\n",
        "ax.set_yticks(np.arange(0, 952, 80))\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"Similarity Matrix\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PKGAKc0l0n3"
      },
      "source": [
        "## sort images after sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8IKBHfwl0n3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# generate the selected_rows arrays using a list comprehension\n",
        "selected_rows = [np.arange(i, similarity_matrix.shape[0], 8) for i in range(8)]\n",
        "# print the selected_rows arrays\n",
        "for i, rows in enumerate(selected_rows):\n",
        "    selected_rows[i]=rows\n",
        "\n",
        "# generate the selected_cols arrays using a list comprehension\n",
        "selected_cols = [np.arange(i, similarity_matrix.shape[0], 8) for i in range(8)]\n",
        "# print the selected_cols arrays\n",
        "for i, cols in enumerate(selected_cols):\n",
        "    selected_cols[i]=cols\n",
        "\n",
        "# generate the selected_matrix arrays using nested list comprehensions\n",
        "# print('selected_rows:',selected_rows)\n",
        "# print('selected_cols:',selected_cols)\n",
        "selected_matrix = [[similarity_matrix[rows][:, cols] for cols in selected_cols] for rows in selected_rows]\n",
        "# concatenate the selected_matrix arrays into a single DataFrame\n",
        "selected_matrix_df = pd.concat([pd.DataFrame(np.hstack(m)) for m in selected_matrix])\n",
        "# print( 'selected_matrix_df.shape:', selected_matrix_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CZEV_MAl0n4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = selected_matrix_df\n",
        "\n",
        "# create a figure and axis object\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# plot the DataFrame\n",
        "im = ax.imshow(df, cmap='coolwarm')\n",
        "\n",
        "# set the x and y ticks and labels\n",
        "for i in range(8):\n",
        "    ax.axhline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.axvline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.text((i+0.5)*119, -50, f'pic{i+1}', ha='center', fontsize=10)\n",
        "    ax.text(-100, (i+0.5)*119, f'pic{i+1}', va='center', fontsize=10)\n",
        "\n",
        "# remove the x and y ticks\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "# add a colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title below the figure\n",
        "fig.suptitle('Similarity Matrix sort by pictures', fontsize=16, y=0.05, va='center')\n",
        "\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVIrb1nngNV5"
      },
      "source": [
        "## sort images before sm in main data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMcde42iLHdZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import pandas as pd\n",
        "\n",
        "# # Create a sample DataFrame with shape 952 x 799\n",
        "# df = data\n",
        "\n",
        "# # Define the indices of the groups of rows in the desired order\n",
        "# group_indices = [[i+j for i in range(0, 952, 8)] for j in range(8)]\n",
        "\n",
        "# # Concatenate the rows in the desired order\n",
        "# sorted_df = pd.concat([df.iloc[group] for group in group_indices] + [df.drop(sum(group_indices, []))])\n",
        "\n",
        "# # Display the sorted DataFrame\n",
        "# print(sorted_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqCTW1xwl0n5"
      },
      "source": [
        "##  .sort by images and BDI-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SivdvYyl0n6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a sample DataFrame with shape 952 x 799 and random scores\n",
        "df = data\n",
        "\n",
        "# Define the indices of the groups of rows in the desired order - for images sorting\n",
        "group_indices = [[i+j for i in range(0, 952, 8)] for j in range(8)]\n",
        "\n",
        "# Sort each group of rows based on the \"score\" column separately - for feature sorting\n",
        "sorted_groups = []\n",
        "for group in group_indices:\n",
        "    sorted_group = df.iloc[group].sort_values('BDI_score')\n",
        "    sorted_groups.append(sorted_group)\n",
        "\n",
        "# Concatenate the sorted groups and the remaining unsorted rows\n",
        "sorted_df_by_images_and_BDI = pd.concat(sorted_groups + [df.drop(sum(group_indices, []))])\n",
        "\n",
        "# Display the sorted DataFrame\n",
        "print(sorted_df_by_images_and_BDI)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3OdR9Qpl0n7"
      },
      "source": [
        "##compute SM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbZ-I0a_l0n7"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(sorted_df_by_images_and_BDI.iloc[:, -768:])\n",
        "\n",
        "# print the similarity matrix\n",
        "print( 'sorted_df.shape:', sorted_df_by_images_and_BDI.shape,'similarity_matrix.shape:',similarity_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQhlWJ5kl0n8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(similarity_matrix,cmap='coolwarm')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# set the x and y ticks every 80 rows and columns\n",
        "ax.set_xticks(np.arange(0, 952, 119))\n",
        "ax.set_yticks(np.arange(0, 952, 119))\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"Similarity Matrix sort by images and BDI on them\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOdpkqbtl0n9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a figure and axis object\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# plot the DataFrame\n",
        "im = ax.imshow(similarity_matrix, cmap='coolwarm')\n",
        "\n",
        "# set the x and y ticks and labels\n",
        "for i in range(8):\n",
        "    ax.axhline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.axvline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.text((i+0.5)*119, -50, f'pic{i+1}', ha='center', fontsize=10)\n",
        "    ax.text(-100, (i+0.5)*119, f'pic{i+1}', va='center', fontsize=10)\n",
        "\n",
        "# remove the x and y ticks\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "# add a colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title below the figure\n",
        "fig.suptitle('Similarity Matrix sort by pictures and BDI', fontsize=16, y=0.05, va='center')\n",
        "\n",
        "\n",
        "# # show the plot\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydws7phol0n9"
      },
      "source": [
        "## percentile - use MS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUHt3X1cl0n-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "# assume sim_matrix is your similarity matrix\n",
        "n_items = similarity_matrix.shape[0]\n",
        "\n",
        "# flatten the similarity matrix and compute ranks\n",
        "ranks = rankdata(similarity_matrix.flatten())\n",
        "\n",
        "# reshape the ranks back into the original matrix shape\n",
        "ranks_matrix = ranks.reshape((n_items, n_items))\n",
        "\n",
        "# compute percentiles by dividing ranks by total number of scores and multiplying by 100\n",
        "percentile_matrix = (ranks_matrix / len(ranks)) * 100\n",
        "\n",
        "# print the resulting percentile matrix\n",
        "percentile_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5Y8_YNkl0n-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(percentile_matrix,cmap='inferno')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# set the x and y ticks every 80 rows and columns\n",
        "ax.set_xticks(np.arange(0, 952, 119))\n",
        "ax.set_yticks(np.arange(0, 952, 119))\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"equalized Similarity Matrix sort by images and BDI on them\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64qfdGgTl0n_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a figure and axis object\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# plot the DataFrame\n",
        "im = ax.imshow(percentile_matrix, cmap='inferno')\n",
        "\n",
        "# set the x and y ticks and labels\n",
        "for i in range(8):\n",
        "    ax.axhline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.axvline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.text((i+0.5)*119, -50, f'pic{i+1}', ha='center', fontsize=10)\n",
        "    ax.text(-100, (i+0.5)*119, f'pic{i+1}', va='center', fontsize=10)\n",
        "\n",
        "# remove the x and y ticks\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "# add a colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title below the figure\n",
        "fig.suptitle('equalized Similarity Matrix sort by pictures and BDI on them', fontsize=16, y=0.05, va='center')\n",
        "\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL-mIFopl0oA"
      },
      "outputs": [],
      "source": [
        "# # select the first 500 rows and columns of the similarity matrix\n",
        "# subset1 = percentile_matrix[:119, :119]\n",
        "\n",
        "# # select the last 500 rows and columns of the similarity matrix\n",
        "# subset2 = percentile_matrix[:119, 119:238]\n",
        "\n",
        "# # calculate the mean similarity of each subset\n",
        "# mean1 = np.mean(subset1)\n",
        "# mean2 = np.mean(subset2)\n",
        "\n",
        "# # print the mean similarity of each subset\n",
        "# print(\"Mean similarity of subset 1:\", mean1)\n",
        "# print(\"Mean similarity of subset 2:\", mean2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsR4Y1Avl0oA"
      },
      "source": [
        "## sub plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9fer5Ykl0oB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a random matrix with shape (952, 952)\n",
        "matrix = percentile_matrix\n",
        "\n",
        "# Define the sub-matrix size\n",
        "sub_size = 119\n",
        "\n",
        "# Iterate over the matrix in steps of sub_size\n",
        "for i in range(0, matrix.shape[0], sub_size):\n",
        "    for j in range(0, matrix.shape[1], sub_size):\n",
        "        # Extract the sub-matrix\n",
        "        sub_matrix = matrix[i:i+sub_size, j:j+sub_size]\n",
        "\n",
        "        # Plot the sub-matrix\n",
        "        plt.imshow(sub_matrix, cmap=\"inferno\", vmin=percentile_matrix.min(), vmax=percentile_matrix.max())\n",
        "        # Add a color bar to the plot\n",
        "        plt.colorbar()\n",
        "\n",
        "\n",
        "        # Add a title to the plot\n",
        "        plt.title(f\"Sub-matrix ({int(i/119 )+1},{int(j/119)+1})\")\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29u5Li5K-1-a"
      },
      "source": [
        "#percentile similarity matrix : sort by images and BDI on sentence based - labse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h14wWwgbB_OF"
      },
      "outputs": [],
      "source": [
        "!pip install tensor\n",
        "from torch import tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6YjZeRc-1-c"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/gdrive/MyDrive/labse.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJH4itt1Cx5P"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqhFeM4uBF1H"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from torch import tensor\n",
        "\n",
        "# assume your DataFrame is named \"df\"\n",
        "new_cols = data['emb'].apply(lambda x: tuple(tensor(eval(x)).numpy()))\n",
        "df_new =pd.concat([data.drop('emb', axis=1), new_cols], axis=1)\n",
        "print(df_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwwmycDuA3n2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# assume your DataFrame is named \"df\"\n",
        "num_cols = len(df_new['emb'].iloc[0][0])\n",
        "col_names = [f\"col{i+1}\" for i in range(num_cols)]\n",
        "df_new1 = pd.DataFrame(df_new['emb'].apply(lambda x: x[0]).to_list(), columns=col_names)\n",
        "\n",
        "# concatenate the new columns with the original DataFrame\n",
        "df_final = pd.concat([df_new.drop('emb', axis=1), df_new1], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kiWrCq2Ek8U"
      },
      "outputs": [],
      "source": [
        "df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOwyT3w1EzoN"
      },
      "outputs": [],
      "source": [
        "vectors= df_final[]\n",
        "vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9pjw3WB-1-d"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(vectors)\n",
        "\n",
        "# print the similarity matrix\n",
        "similarity_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy5J7pcg-1-r"
      },
      "source": [
        "## percentile - use MS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AetpFxC0-1-s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "# assume sim_matrix is your similarity matrix\n",
        "n_items = similarity_matrix.shape[0]\n",
        "\n",
        "# flatten the similarity matrix and compute ranks\n",
        "ranks = rankdata(similarity_matrix.flatten())\n",
        "\n",
        "# reshape the ranks back into the original matrix shape\n",
        "ranks_matrix = ranks.reshape((n_items, n_items))\n",
        "\n",
        "# compute percentiles by dividing ranks by total number of scores and multiplying by 100\n",
        "percentile_matrix = (ranks_matrix / len(ranks)) * 100\n",
        "\n",
        "# print the resulting percentile matrix\n",
        "percentile_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KROvUpT6-1-t"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(percentile_matrix,cmap='inferno')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# set the x and y ticks every 80 rows and columns\n",
        "ax.set_xticks(np.arange(0, 952, 119))\n",
        "ax.set_yticks(np.arange(0, 952, 119))\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"equalized Similarity Matrix sort by images and BDI on them - laBSE\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IazvUDxc-1-v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a figure and axis object\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# plot the DataFrame\n",
        "im = ax.imshow(percentile_matrix, cmap='inferno')\n",
        "\n",
        "# set the x and y ticks and labels\n",
        "for i in range(8):\n",
        "    ax.axhline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.axvline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.text((i+0.5)*119, -50, f'pic{i+1}', ha='center', fontsize=10)\n",
        "    ax.text(-100, (i+0.5)*119, f'pic{i+1}', va='center', fontsize=10)\n",
        "\n",
        "# remove the x and y ticks\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "# add a colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title below the figure\n",
        "fig.suptitle('equalized Similarity Matrix sort by pictures and BDI on them _ LaBSE', fontsize=16, y=0.05, va='center')\n",
        "\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pXNRolK-1-w"
      },
      "outputs": [],
      "source": [
        "# # select the first 500 rows and columns of the similarity matrix\n",
        "# subset1 = percentile_matrix[:119, :119]\n",
        "\n",
        "# # select the last 500 rows and columns of the similarity matrix\n",
        "# subset2 = percentile_matrix[:119, 119:238]\n",
        "\n",
        "# # calculate the mean similarity of each subset\n",
        "# mean1 = np.mean(subset1)\n",
        "# mean2 = np.mean(subset2)\n",
        "\n",
        "# # print the mean similarity of each subset\n",
        "# print(\"Mean similarity of subset 1:\", mean1)\n",
        "# print(\"Mean similarity of subset 2:\", mean2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R89SupEC-1-x"
      },
      "source": [
        "### sub plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XbRaXSq9-1-y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a random matrix with shape (952, 952)\n",
        "matrix = percentile_matrix\n",
        "\n",
        "# Define the sub-matrix size\n",
        "sub_size = 119\n",
        "\n",
        "# Iterate over the matrix in steps of sub_size\n",
        "for i in range(0, matrix.shape[0], sub_size):\n",
        "    for j in range(0, matrix.shape[1], sub_size):\n",
        "        # Extract the sub-matrix\n",
        "        sub_matrix = matrix[i:i+sub_size, j:j+sub_size]\n",
        "\n",
        "        # Plot the sub-matrix\n",
        "        plt.imshow(sub_matrix, cmap=\"inferno\")\n",
        "\n",
        "        # Add a title to the plot\n",
        "        plt.title(f\"Sub-matrix ({int(i/119 )+1},{int(j/119)+1})\")\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr7wNtpjFikr"
      },
      "source": [
        "# percentile similarity matrix : sort by images and BDI on mean word based embedding - parsbert\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5n0GpwGKFiks"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/vectorized_all_dataset_mean_sentence-based_Parsbert.csv')\n",
        "# data\n",
        "vectors= data.iloc[:, -768:]\n",
        "vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSpjNv6JFikt"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# import numpy as np\n",
        "\n",
        "# # calculate cosine similarity between vectors\n",
        "# similarity_matrix = cosine_similarity(vectors)\n",
        "\n",
        "# # print the similarity matrix\n",
        "# similarity_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9KhYClZFikw"
      },
      "source": [
        "##  .sort by images and BDI-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j62MqDyvFikx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a sample DataFrame with shape 952 x 799 and random scores\n",
        "df = data\n",
        "\n",
        "# Define the indices of the groups of rows in the desired order - for images sorting\n",
        "group_indices = [[i+j for i in range(0, 952, 8)] for j in range(8)]\n",
        "\n",
        "# Sort each group of rows based on the \"score\" column separately - for feature sorting\n",
        "sorted_groups = []\n",
        "for group in group_indices:\n",
        "    sorted_group = df.iloc[group].sort_values('BDI_score')\n",
        "    sorted_groups.append(sorted_group)\n",
        "\n",
        "# Concatenate the sorted groups and the remaining unsorted rows\n",
        "sorted_df_by_images_and_BDI = pd.concat(sorted_groups + [df.drop(sum(group_indices, []))])\n",
        "\n",
        "# Display the sorted DataFrame\n",
        "# print(sorted_df_by_images_and_BDI)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZVdXAT_Fiky"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# calculate cosine similarity between vectors\n",
        "similarity_matrix = cosine_similarity(sorted_df_by_images_and_BDI.iloc[:, -768:])\n",
        "\n",
        "# print the similarity matrix\n",
        "print( 'sorted_df.shape:', sorted_df_by_images_and_BDI.shape,'similarity_matrix.shape:',similarity_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBlUoJDiFikz"
      },
      "source": [
        "## percentile - use MS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEofEl4wFik0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "# assume sim_matrix is your similarity matrix\n",
        "n_items = similarity_matrix.shape[0]\n",
        "\n",
        "# flatten the similarity matrix and compute ranks\n",
        "ranks = rankdata(similarity_matrix.flatten())\n",
        "\n",
        "# reshape the ranks back into the original matrix shape\n",
        "ranks_matrix = ranks.reshape((n_items, n_items))\n",
        "\n",
        "# compute percentiles by dividing ranks by total number of scores and multiplying by 100\n",
        "percentile_matrix = (ranks_matrix / len(ranks)) * 100\n",
        "\n",
        "# print the resulting percentile matrix\n",
        "# percentile_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTo26M8QFik1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a heatmap of the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(percentile_matrix,cmap='inferno')\n",
        "\n",
        "# rotate the tick labels and set their alignment\n",
        "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# set the x and y ticks every 80 rows and columns\n",
        "ax.set_xticks(np.arange(0, 952, 119))\n",
        "ax.set_yticks(np.arange(0, 952, 119))\n",
        "\n",
        "# add colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title and show the plot\n",
        "ax.set_title(\"equalized Similarity Matrix sort by images and BDI on them - based on sentense - mean - pars bert\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zct-zTtlFik1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a figure and axis object\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# plot the DataFrame\n",
        "im = ax.imshow(percentile_matrix, cmap='inferno')\n",
        "\n",
        "# set the x and y ticks and labels\n",
        "for i in range(8):\n",
        "    ax.axhline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.axvline((i+1)*119-0.5, color='black', linewidth=0.8)\n",
        "    ax.text((i+0.5)*119, -50, f'pic{i+1}', ha='center', fontsize=10)\n",
        "    ax.text(-100, (i+0.5)*119, f'pic{i+1}', va='center', fontsize=10)\n",
        "\n",
        "# remove the x and y ticks\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "# add a colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# set the title below the figure\n",
        "fig.suptitle('equalized Similarity Matrix sort by images and BDI on them - based on sentense - mean - pars bert', fontsize=16, y=0.05, va='center')\n",
        "\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgyrVyVWFik2"
      },
      "outputs": [],
      "source": [
        "# # select the first 500 rows and columns of the similarity matrix\n",
        "# subset1 = percentile_matrix[:119, :119]\n",
        "\n",
        "# # select the last 500 rows and columns of the similarity matrix\n",
        "# subset2 = percentile_matrix[:119, 119:238]\n",
        "\n",
        "# # calculate the mean similarity of each subset\n",
        "# mean1 = np.mean(subset1)\n",
        "# mean2 = np.mean(subset2)\n",
        "\n",
        "# # print the mean similarity of each subset\n",
        "# print(\"Mean similarity of subset 1:\", mean1)\n",
        "# print(\"Mean similarity of subset 2:\", mean2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4nHgVdTFik3"
      },
      "source": [
        "## sub plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywH7O_2DFik3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a random matrix with shape (952, 952)\n",
        "matrix = percentile_matrix\n",
        "\n",
        "# Define the sub-matrix size\n",
        "sub_size = 119\n",
        "\n",
        "# Iterate over the matrix in steps of sub_size\n",
        "for i in range(0, matrix.shape[0], sub_size):\n",
        "    for j in range(0, matrix.shape[1], sub_size):\n",
        "        # Extract the sub-matrix\n",
        "        sub_matrix = matrix[i:i+sub_size, j:j+sub_size]\n",
        "\n",
        "        # Plot the sub-matrix\n",
        "        plt.imshow(sub_matrix, cmap=\"inferno\")\n",
        "\n",
        "        # Add a title to the plot\n",
        "        plt.title(f\"Sub-matrix ({int(i/119 )+1},{int(j/119)+1})\")\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13. prediction SVM:"
      ],
      "metadata": {
        "id": "ruY1hofznxni"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq93B3q6lRnX"
      },
      "source": [
        "#prediction on 28*28 sentence based\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## euclidean - labse"
      ],
      "metadata": {
        "id": "yT_VuiI-qowH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EwHEbOelRnY"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PgY89f3lRnZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "# from transformers import AutoTokenizer, AutoModel\n",
        "from torch import nn\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLgOSRjzlRnZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/28x.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viswItZfmDgh"
      },
      "outputs": [],
      "source": [
        "groupbyid_df = pd.read_csv('/content/gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd_ComputedLength_MergedTranscripted_all.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQu3KLd5lq_H"
      },
      "outputs": [],
      "source": [
        "merg = pd.merge(groupbyid_df[['id', 'BDI_score']], data, on=['id']).drop(['Unnamed: 0'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcT4UqkdrPZx"
      },
      "outputs": [],
      "source": [
        "grouped = merg.groupby('id')\n",
        "df = pd.DataFrame(grouped.first())\n",
        "df= df.reset_index()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGxiit4eXk1Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Split euclidean values into lists of numbers\n",
        "df['28xeuclidean'] = df['28xeuclidean'].str.split()\n",
        "\n",
        "# Apply pd.Series to euclidean column\n",
        "df_euclidean = df['28xeuclidean'].apply(pd.Series)\n",
        "\n",
        "# Rename columns\n",
        "df_euclidean.columns = ['euclidean_{}'.format(i+1) for i in range(len(df_euclidean.columns))]\n",
        "\n",
        "# Concatenate original DataFrame and new euclidean DataFrame\n",
        "df_concatenated = pd.concat([df[['id', 'BDI_score']], df_euclidean], axis=1)\n",
        "\n",
        "\n",
        "# Apply str.replace() only to string columns\n",
        "for col in df_concatenated.columns:\n",
        "    if df_concatenated[col].dtype == 'object':\n",
        "        df_concatenated[col] = df_concatenated[col].str.replace('[','').str.replace(']','')\n",
        "df_concatenated = df_concatenated.drop(['euclidean_29'],axis=1)\n",
        "df_concatenated;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS0-c0uziha4"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # Split euclidean values into lists of numbers\n",
        "# df['28xcosine'] = df['28xcosine'].str.split()\n",
        "\n",
        "# # Apply pd.Series to euclidean column\n",
        "# df_euclidean = df['28xcosine'].apply(pd.Series)\n",
        "\n",
        "# # Rename columns\n",
        "# df_euclidean.columns = ['cosine_{}'.format(i+1) for i in range(len(df_euclidean.columns))]\n",
        "\n",
        "# # Concatenate original DataFrame and new euclidean DataFrame\n",
        "# df_concatenated = pd.concat([df_concatenated, df_euclidean], axis=1)\n",
        "\n",
        "# # Apply str.replace() only to string columns\n",
        "# for col in df_concatenated.columns:\n",
        "#     if df_concatenated[col].dtype == 'object':\n",
        "#         df_concatenated[col] = df_concatenated[col].str.replace('[','').str.replace(']','')\n",
        "# df_concatenated = df_concatenated.drop(['cosine_29','cosine_30'],axis=1)\n",
        "# df_concatenated;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89iCLSHBp8jl"
      },
      "outputs": [],
      "source": [
        "# replace empty strings with NaN and drop rows containing NaN\n",
        "df_concatenated.replace('', np.nan, inplace=True)\n",
        "df_concatenated.dropna(inplace=True)\n",
        "\n",
        "# print the updated dataframe\n",
        "df_concatenated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ccm9_bHj9qe"
      },
      "outputs": [],
      "source": [
        "# select all columns except 'col3' and 'col4'\n",
        "cols_to_exclude = ['id', 'BDI_score']\n",
        "X = df_concatenated.loc[:, ~df_concatenated.columns.isin(cols_to_exclude)]\n",
        "\n",
        "y= df_concatenated['BDI_score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoosQEEmlJGI"
      },
      "outputs": [],
      "source": [
        "# Convert all columns to numeric, raising errors on non-numeric values\n",
        "X = X.apply(pd.to_numeric, errors='raise')\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MukX13xtl8n1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an SVM model and fit it to the training data\n",
        "svm = SVR(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Use the model to make predictions on the test data\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Compute R^2 and RMSE\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print('R^2:', r2)\n",
        "print('RMSE:', rmse)\n",
        "\n",
        "#just with cosine\n",
        "# R^2: 0.09913889255840802\n",
        "# RMSE: 11.043210965336455\n",
        "\n",
        "#with just euclidean\n",
        "# R^2: 0.23771517212190119\n",
        "# RMSE: 8.960692041903147"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzaLLViwz3-Q"
      },
      "source": [
        "حدود 10 درصد از واریانس در داده ها توسط مدل توضیح داده شده است"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euwn_AD0lRnp"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Create a table comparing actual and predicted scores\n",
        "comparison_table = pd.DataFrame({'Actual BDI Score': y_test, 'Predicted BDI Score': y_pred})\n",
        "table = tabulate(comparison_table, headers='keys', tablefmt='psql')\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBmplMTYlRnq"
      },
      "source": [
        "modify the size and color an size of the points based on Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCCu1HbclRnr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_test_series = np.array(y_test)\n",
        "subset_data = data.loc[pd.DataFrame(y_test).index]\n",
        "\n",
        "# Plot actual and predicted scores with size variation\n",
        "plt.scatter(y_test, y_pred,  alpha=0.5) #s=subset_data['length_seconds']/60\n",
        "plt.plot([min(y_test_series), max(y_test_series)], [min(y_test_series), max(y_test_series)], color='red', linestyle='--')\n",
        "# plt.colorbar(label='length_seconds')\n",
        "plt.xlabel('Actual BDI Score')\n",
        "plt.ylabel('Predicted BDI Score')\n",
        "plt.title('Actual vs Predicted BDI Score with cosine')\n",
        "plt.show();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gop1yd7W2kXz"
      },
      "source": [
        "## cosine - labse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGZHpATs2kX-"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWcJeucm2kX_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "# from transformers import AutoTokenizer, AutoModel\n",
        "from torch import nn\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ1nx7kD2kYA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/28x.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm2lefDT2kYA"
      },
      "outputs": [],
      "source": [
        "groupbyid_df = pd.read_csv('/content/gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd_ComputedLength_MergedTranscripted_all.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0r0d10DL2kYA"
      },
      "outputs": [],
      "source": [
        "merg = pd.merge(groupbyid_df[['id', 'BDI_score']], data, on=['id']).drop(['Unnamed: 0'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iy5y0bL2kYA"
      },
      "outputs": [],
      "source": [
        "grouped = merg.groupby('id')\n",
        "df = pd.DataFrame(grouped.first())\n",
        "df= df.reset_index()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpxHtQAw2kYB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Split euclidean values into lists of numbers\n",
        "df['28xcosine'] = df['28xcosine'].str.split()\n",
        "\n",
        "# Apply pd.Series to euclidean column\n",
        "df_euclidean = df['28xcosine'].apply(pd.Series)\n",
        "\n",
        "# Rename columns\n",
        "df_euclidean.columns = ['cosine_{}'.format(i+1) for i in range(len(df_euclidean.columns))]\n",
        "\n",
        "# Concatenate original DataFrame and new euclidean DataFrame\n",
        "df_concatenated = pd.concat([df_concatenated, df_euclidean], axis=1)\n",
        "\n",
        "# Apply str.replace() only to string columns\n",
        "for col in df_concatenated.columns:\n",
        "    if df_concatenated[col].dtype == 'object':\n",
        "        df_concatenated[col] = df_concatenated[col].str.replace('[','').str.replace(']','')\n",
        "df_concatenated = df_concatenated.drop(['cosine_29','cosine_30'],axis=1)\n",
        "df_concatenated;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLGIKll-2kYB"
      },
      "outputs": [],
      "source": [
        "# replace empty strings with NaN and drop rows containing NaN\n",
        "df_concatenated.replace('', np.nan, inplace=True)\n",
        "df_concatenated.dropna(inplace=True)\n",
        "\n",
        "# print the updated dataframe\n",
        "df_concatenated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3p2_Q7e2kYC"
      },
      "outputs": [],
      "source": [
        "# select all columns except 'col3' and 'col4'\n",
        "cols_to_exclude = ['id', 'BDI_score']\n",
        "X = df_concatenated.loc[:, ~df_concatenated.columns.isin(cols_to_exclude)]\n",
        "\n",
        "y= df_concatenated['BDI_score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmT3wsoQ2kYC"
      },
      "outputs": [],
      "source": [
        "# Convert all columns to numeric, raising errors on non-numeric values\n",
        "X = X.apply(pd.to_numeric, errors='raise')\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9Pt1Q8X2kYC"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Create an SVM model and fit it to the training data\n",
        "svm = SVR(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Use the model to make predictions on the test data\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Compute R^2 and RMSE\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print('R^2:', r2)\n",
        "print('RMSE:', rmse)\n",
        "\n",
        "#just with cosine\n",
        "# R^2: 0.09913889255840802\n",
        "# RMSE: 11.043210965336455\n",
        "\n",
        "#with just euclidean\n",
        "# R^2: 0.23771517212190119\n",
        "# RMSE: 8.960692041903147"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLafZiGq2kYD"
      },
      "source": [
        "حدود 24  درصد از واریانس در داده ها توسط مدل توضیح داده شده است"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izn8IM4P2kYD"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Create a table comparing actual and predicted scores\n",
        "comparison_table = pd.DataFrame({'Actual BDI Score': y_test, 'Predicted BDI Score': y_pred})\n",
        "table = tabulate(comparison_table, headers='keys', tablefmt='psql')\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLWuMSuw2kYD"
      },
      "source": [
        "modify the size and color an size of the points based on Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NevZIJzF2kYD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_test_series = np.array(y_test)\n",
        "subset_data = data.loc[pd.DataFrame(y_test).index]\n",
        "\n",
        "# Plot actual and predicted scores with size variation\n",
        "plt.scatter(y_test, y_pred,  alpha=0.5) #s=subset_data['length_seconds']/60\n",
        "plt.plot([min(y_test_series), max(y_test_series)], [min(y_test_series), max(y_test_series)], color='red', linestyle='--')\n",
        "# plt.colorbar(label='length_seconds')\n",
        "plt.xlabel('Actual BDI Score')\n",
        "plt.ylabel('Predicted BDI Score')\n",
        "plt.title('Actual vs Predicted BDI Score with euclidean')\n",
        "plt.show();\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W4nMnlPxq3op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3nbppqHmEid"
      },
      "source": [
        "# prediction on 28*28 tocken based"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## euclidean - tf-idf"
      ],
      "metadata": {
        "id": "kgDx2JGjrJmk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVg8McuFmEif"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDvcb0tXmEig"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "# from transformers import AutoTokenizer, AutoModel\n",
        "from torch import nn\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ7P4SAamEig"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/28x-tfidf.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rW2zAZVmEih"
      },
      "outputs": [],
      "source": [
        "groupbyid_df = pd.read_csv('/content/gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd_ComputedLength_MergedTranscripted_all.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWyrJAXwmEii"
      },
      "outputs": [],
      "source": [
        "merg = pd.merge(groupbyid_df[['id', 'BDI_score']], data, on=['id']).drop(['Unnamed: 0'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2jriwscmEii"
      },
      "outputs": [],
      "source": [
        "grouped = merg.groupby('id')\n",
        "df = pd.DataFrame(grouped.first())\n",
        "df= df.reset_index()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w2ci_W5mEij"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Split euclidean values into lists of numbers\n",
        "df['28xeuclidean'] = df['28xeuclidean'].str.split()\n",
        "\n",
        "# Apply pd.Series to euclidean column\n",
        "df_euclidean = df['28xeuclidean'].apply(pd.Series)\n",
        "\n",
        "# Rename columns\n",
        "df_euclidean.columns = ['euclidean_{}'.format(i+1) for i in range(len(df_euclidean.columns))]\n",
        "\n",
        "# Concatenate original DataFrame and new euclidean DataFrame\n",
        "df_concatenated = pd.concat([df[['id', 'BDI_score']], df_euclidean], axis=1)\n",
        "\n",
        "\n",
        "# Apply str.replace() only to string columns\n",
        "for col in df_concatenated.columns:\n",
        "    if df_concatenated[col].dtype == 'object':\n",
        "        df_concatenated[col] = df_concatenated[col].str.replace('[','').str.replace(']','')\n",
        "df_concatenated = df_concatenated.drop(['euclidean_29'],axis=1)\n",
        "df_concatenated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnqAvetnmEik"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # Split euclidean values into lists of numbers\n",
        "# df['28xcosine'] = df['28xcosine'].str.split()\n",
        "\n",
        "# # Apply pd.Series to euclidean column\n",
        "# df_euclidean = df['28xcosine'].apply(pd.Series)\n",
        "\n",
        "# # Rename columns\n",
        "# df_euclidean.columns = ['cosine_{}'.format(i+1) for i in range(len(df_euclidean.columns))]\n",
        "\n",
        "# # Concatenate original DataFrame and new euclidean DataFrame\n",
        "# df_concatenated = pd.concat([df_concatenated, df_euclidean], axis=1)\n",
        "\n",
        "# # Apply str.replace() only to string columns\n",
        "# for col in df_concatenated.columns:\n",
        "#     if df_concatenated[col].dtype == 'object':\n",
        "#         df_concatenated[col] = df_concatenated[col].str.replace('[','').str.replace(']','')\n",
        "# df_concatenated = df_concatenated.drop(['cosine_29','cosine_30'],axis=1)\n",
        "# df_concatenated;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBTiLIhOmEil"
      },
      "outputs": [],
      "source": [
        "# replace empty strings with NaN and drop rows containing NaN\n",
        "df_concatenated.replace('', np.nan, inplace=True)\n",
        "df_concatenated.dropna(inplace=True)\n",
        "\n",
        "# print the updated dataframe\n",
        "df_concatenated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGEXNDvWmEim"
      },
      "outputs": [],
      "source": [
        "# select all columns except 'col3' and 'col4'\n",
        "cols_to_exclude = ['id', 'BDI_score']\n",
        "X = df_concatenated.loc[:, ~df_concatenated.columns.isin(cols_to_exclude)]\n",
        "\n",
        "y= df_concatenated['BDI_score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtAGFxlNmEim"
      },
      "outputs": [],
      "source": [
        "# Convert all columns to numeric, raising errors on non-numeric values\n",
        "X = X.apply(pd.to_numeric, errors='raise')\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic32xuramEim"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Create an SVM model and fit it to the training data\n",
        "svm = SVR(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Use the model to make predictions on the test data\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Compute R^2 and RMSE\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print('R^2:', r2)\n",
        "print('RMSE:', rmse)\n",
        "\n",
        "#just with cosine\n",
        "# R^2: 0.09913889255840802\n",
        "# RMSE: 11.043210965336455\n",
        "\n",
        "#with just euclidean\n",
        "# R^2: 0.23771517212190119\n",
        "# RMSE: 8.960692041903147"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk4j31PqmEio"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Create a table comparing actual and predicted scores\n",
        "comparison_table = pd.DataFrame({'Actual BDI Score': y_test, 'Predicted BDI Score': y_pred})\n",
        "table = tabulate(comparison_table, headers='keys', tablefmt='psql')\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDSKoJP-mEip"
      },
      "source": [
        "modify the size and color an size of the points based on Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6yJB82xmEip"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_test_series = np.array(y_test)\n",
        "subset_data = data.loc[pd.DataFrame(y_test).index]\n",
        "\n",
        "# Plot actual and predicted scores with size variation\n",
        "plt.scatter(y_test, y_pred,  alpha=0.5) #s=subset_data['length_seconds']/60\n",
        "plt.plot([min(y_test_series), max(y_test_series)], [min(y_test_series), max(y_test_series)], color='red', linestyle='--')\n",
        "# plt.colorbar(label='length_seconds')\n",
        "plt.xlabel('Actual BDI Score')\n",
        "plt.ylabel('Predicted BDI Score')\n",
        "plt.title('Actual vs Predicted BDI Score with cosine')\n",
        "plt.show();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPCe_Xj1v3N7"
      },
      "source": [
        "## cosine - tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idDXPVgcv3N9"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6nUXUElv3N-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "# from transformers import AutoTokenizer, AutoModel\n",
        "from torch import nn\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFIGqm2tv3N_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/28x-tfidf.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_csv('/content/gdrive/MyDrive/28x.csv')"
      ],
      "metadata": {
        "id": "0e8Dim2awC1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tswsthZmv3OA"
      },
      "outputs": [],
      "source": [
        "groupbyid_df = pd.read_csv('/content/gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd_ComputedLength_MergedTranscripted_all.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp6PcPozv3OB"
      },
      "outputs": [],
      "source": [
        "merg = pd.merge(groupbyid_df[['id', 'BDI_score']], data, on=['id']).drop(['Unnamed: 0'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezVqGjBYv3OB"
      },
      "outputs": [],
      "source": [
        "grouped = merg.groupby('id')\n",
        "df = pd.DataFrame(grouped.first())\n",
        "df= df.reset_index()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Split euclidean values into lists of numbers\n",
        "df['28xcosine'] = df['28xcosine'].str.split()\n",
        "\n",
        "# Apply pd.Series to euclidean column\n",
        "df_euclidean = df['28xcosine'].apply(pd.Series)\n",
        "\n",
        "# Rename columns\n",
        "df_euclidean.columns = ['cosine_{}'.format(i+1) for i in range(len(df_euclidean.columns))]\n",
        "\n",
        "# Concatenate original DataFrame and new euclidean DataFrame\n",
        "df_concatenated = pd.concat([df_concatenated, df_euclidean], axis=1)\n"
      ],
      "metadata": {
        "id": "0teEHVsjw7Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_concatenated = df_concatenated.drop(['cosine_29','cosine_28','cosine_1'],axis=1)\n",
        "df_concatenated"
      ],
      "metadata": {
        "id": "r9N0Lnl3w8no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uKBpYLSv3OC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Apply str.replace() only to string columns\n",
        "# for col in df_concatenated.columns:\n",
        "#     if df_concatenated[col].dtype == 'object':\n",
        "#         df_concatenated[col] = df_concatenated[col].str.replace('[','').str.replace(']','')\n",
        "# ;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yostroBAv3OD"
      },
      "outputs": [],
      "source": [
        "# replace empty strings with NaN and drop rows containing NaN\n",
        "df_concatenated.replace('', np.nan, inplace=True)\n",
        "df_concatenated.dropna(inplace=True)\n",
        "\n",
        "# print the updated dataframe\n",
        "df_concatenated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2Ac3FfBv3OE"
      },
      "outputs": [],
      "source": [
        "# select all columns except 'col3' and 'col4'\n",
        "cols_to_exclude = ['id', 'BDI_score']\n",
        "X = df_concatenated.loc[:, ~df_concatenated.columns.isin(cols_to_exclude)]\n",
        "\n",
        "y= df['BDI_score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWwSn8Adv3OF"
      },
      "outputs": [],
      "source": [
        "# Convert all columns to numeric, raising errors on non-numeric values\n",
        "X = X.apply(pd.to_numeric, errors='raise')\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yEFJvBqv3OG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an SVM model and fit it to the training data\n",
        "svm = SVR(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Use the model to make predictions on the test data\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Compute R^2 and RMSE\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print('R^2:', r2)\n",
        "print('RMSE:', rmse)\n",
        "\n",
        "#just with cosine\n",
        "# R^2: 0.09913889255840802\n",
        "# RMSE: 11.043210965336455\n",
        "\n",
        "#with just euclidean\n",
        "# R^2: 0.23771517212190119\n",
        "# RMSE: 8.960692041903147"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kIkJKA9v3OI"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Create a table comparing actual and predicted scores\n",
        "comparison_table = pd.DataFrame({'Actual BDI Score': y_test, 'Predicted BDI Score': y_pred})\n",
        "table = tabulate(comparison_table, headers='keys', tablefmt='psql')\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4l3F-SAv3OJ"
      },
      "source": [
        "modify the size and color an size of the points based on Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFgfiVu6v3OJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_test_series = np.array(y_test)\n",
        "subset_data = data.loc[pd.DataFrame(y_test).index]\n",
        "\n",
        "# Plot actual and predicted scores with size variation\n",
        "plt.scatter(y_test, y_pred,  alpha=0.5) #s=subset_data['length_seconds']/60\n",
        "plt.plot([min(y_test_series), max(y_test_series)], [min(y_test_series), max(y_test_series)], color='red', linestyle='--')\n",
        "# plt.colorbar(label='length_seconds')\n",
        "plt.xlabel('Actual BDI Score')\n",
        "plt.ylabel('Predicted BDI Score')\n",
        "plt.title('Actual vs Predicted BDI Score with euclidean')\n",
        "plt.show();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMDDtpBMEh3g"
      },
      "source": [
        "#prediction on 28*28 mean word based\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## euclidean - parsbert"
      ],
      "metadata": {
        "id": "NW13AlPSEh3x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1vTwkGoEh3x"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmckkWPVEh3y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "# from transformers import AutoTokenizer, AutoModel\n",
        "from torch import nn\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvRkl7hmEh3z"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/28parsbertx.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpQqw3wcEh30"
      },
      "outputs": [],
      "source": [
        "groupbyid_df = pd.read_csv('/content/gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd_ComputedLength_MergedTranscripted_all.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CwKczMqEh31"
      },
      "outputs": [],
      "source": [
        "merg = pd.merge(groupbyid_df[['id', 'BDI_score']], data, on=['id']).drop(['Unnamed: 0'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sdds1GFFEh31"
      },
      "outputs": [],
      "source": [
        "grouped = merg.groupby('id')\n",
        "df = pd.DataFrame(grouped.first())\n",
        "df= df.reset_index()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGXXrtOCEh32"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Split euclidean values into lists of numbers\n",
        "df['28xeuclidean'] = df['28xeuclidean'].str.split()\n",
        "\n",
        "# Apply pd.Series to euclidean column\n",
        "df_euclidean = df['28xeuclidean'].apply(pd.Series)\n",
        "\n",
        "# Rename columns\n",
        "df_euclidean.columns = ['euclidean_{}'.format(i+1) for i in range(len(df_euclidean.columns))]\n",
        "\n",
        "# Concatenate original DataFrame and new euclidean DataFrame\n",
        "df_concatenated = pd.concat([df[['id', 'BDI_score']], df_euclidean], axis=1)\n",
        "\n",
        "\n",
        "# Apply str.replace() only to string columns\n",
        "for col in df_concatenated.columns:\n",
        "    if df_concatenated[col].dtype == 'object':\n",
        "        df_concatenated[col] = df_concatenated[col].str.replace('[','').str.replace(']','')\n",
        "df_concatenated = df_concatenated.drop(['euclidean_29'],axis=1)\n",
        "df_concatenated;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLFMFiIUEh33"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # Split euclidean values into lists of numbers\n",
        "# df['28xcosine'] = df['28xcosine'].str.split()\n",
        "\n",
        "# # Apply pd.Series to euclidean column\n",
        "# df_euclidean = df['28xcosine'].apply(pd.Series)\n",
        "\n",
        "# # Rename columns\n",
        "# df_euclidean.columns = ['cosine_{}'.format(i+1) for i in range(len(df_euclidean.columns))]\n",
        "\n",
        "# # Concatenate original DataFrame and new euclidean DataFrame\n",
        "# df_concatenated = pd.concat([df_concatenated, df_euclidean], axis=1)\n",
        "\n",
        "# # Apply str.replace() only to string columns\n",
        "# for col in df_concatenated.columns:\n",
        "#     if df_concatenated[col].dtype == 'object':\n",
        "#         df_concatenated[col] = df_concatenated[col].str.replace('[','').str.replace(']','')\n",
        "# df_concatenated = df_concatenated.drop(['cosine_29','cosine_30'],axis=1)\n",
        "# df_concatenated;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gei2LUDEh33"
      },
      "outputs": [],
      "source": [
        "# replace empty strings with NaN and drop rows containing NaN\n",
        "df_concatenated.replace('', np.nan, inplace=True)\n",
        "df_concatenated.dropna(inplace=True)\n",
        "\n",
        "# print the updated dataframe\n",
        "df_concatenated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdmYrXaYEh34"
      },
      "outputs": [],
      "source": [
        "# select all columns except 'col3' and 'col4'\n",
        "cols_to_exclude = ['id', 'BDI_score']\n",
        "X = df_concatenated.loc[:, ~df_concatenated.columns.isin(cols_to_exclude)]\n",
        "\n",
        "y= df_concatenated['BDI_score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "To1xyAqjEh35"
      },
      "outputs": [],
      "source": [
        "# Convert all columns to numeric, raising errors on non-numeric values\n",
        "X = X.apply(pd.to_numeric, errors='raise')\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noeMU8sOEh36"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "# Create an SVM model and fit it to the training data\n",
        "svm = SVR(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Use the model to make predictions on the test data\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Compute R^2 and RMSE\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print('R^2:', r2)\n",
        "print('RMSE:', rmse)\n",
        "\n",
        "#just with cosine\n",
        "# R^2:\n",
        "# RMSE:\n",
        "\n",
        "#with just euclidean\n",
        "# R^2:-0.011684696294692376\n",
        "# RMSE: 9.803214807389821"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-npv8qoEh38"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Create a table comparing actual and predicted scores\n",
        "comparison_table = pd.DataFrame({'Actual BDI Score': y_test, 'Predicted BDI Score': y_pred})\n",
        "table = tabulate(comparison_table, headers='keys', tablefmt='psql')\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwvAJBFCEh39"
      },
      "source": [
        "modify the size and color an size of the points based on Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbcZP_wrEh39"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_test_series = np.array(y_test)\n",
        "subset_data = data.loc[pd.DataFrame(y_test).index]\n",
        "\n",
        "# Plot actual and predicted scores with size variation\n",
        "plt.scatter(y_test, y_pred,  alpha=0.5) #s=subset_data['length_seconds']/60\n",
        "plt.plot([min(y_test_series), max(y_test_series)], [min(y_test_series), max(y_test_series)], color='red', linestyle='--')\n",
        "# plt.colorbar(label='length_seconds')\n",
        "plt.xlabel('Actual BDI Score')\n",
        "plt.ylabel('Predicted BDI Score')\n",
        "plt.title('Actual vs Predicted BDI Score with cosine')\n",
        "plt.show();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbwdtUAsEh3-"
      },
      "source": [
        "## cosine - parsbert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaevYWdZEh3_"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyGKvJMjEh3_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "# from transformers import AutoTokenizer, AutoModel\n",
        "from torch import nn\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLUDTO4PEh4A"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/28parsbertx.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk6mbddREh4A"
      },
      "outputs": [],
      "source": [
        "groupbyid_df = pd.read_csv('/content/gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd_ComputedLength_MergedTranscripted_all.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTQoCmYUEh4B"
      },
      "outputs": [],
      "source": [
        "merg = pd.merge(groupbyid_df[['id', 'BDI_score']], data, on=['id']).drop(['Unnamed: 0'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RM_WLUqJEh4B"
      },
      "outputs": [],
      "source": [
        "grouped = merg.groupby('id')\n",
        "df = pd.DataFrame(grouped.first())\n",
        "df= df.reset_index()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-d_tN5sVEh4C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Split euclidean values into lists of numbers\n",
        "df['28xcosine'] = df['28xcosine'].str.split()\n",
        "\n",
        "# Apply pd.Series to euclidean column\n",
        "df_euclidean = df['28xcosine'].apply(pd.Series)\n",
        "\n",
        "# Rename columns\n",
        "df_euclidean.columns = ['cosine_{}'.format(i+1) for i in range(len(df_euclidean.columns))]\n",
        "\n",
        "# Concatenate original DataFrame and new euclidean DataFrame\n",
        "df_concatenated = pd.concat([df_concatenated, df_euclidean], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Apply str.replace() only to string columns\n",
        "# for col in df_concatenated.columns:\n",
        "#     if df_concatenated[col].dtype == 'object':\n",
        "#         df_concatenated[col] = df_concatenated[col].str.replace('[','').str.replace(']','')\n",
        "# df_concatenated = df_concatenated.drop(['cosine_29'],axis=1)\n",
        "\n",
        "df_concatenated = df_concatenated.drop(['cosine_28','cosine_29','cosine_1'],axis=1)\n",
        "df_concatenated"
      ],
      "metadata": {
        "id": "ckFFm1IkGvri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt5PGXKPEh4D"
      },
      "outputs": [],
      "source": [
        "# replace empty strings with NaN and drop rows containing NaN\n",
        "df_concatenated.replace('', np.nan, inplace=True)\n",
        "df_concatenated.dropna(inplace=True)\n",
        "\n",
        "# print the updated dataframe\n",
        "df_concatenated\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosX = df_concatenated.iloc[:,-26:]"
      ],
      "metadata": {
        "id": "snslNlTZHlD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8Jm3qOmEh4D"
      },
      "outputs": [],
      "source": [
        "# select all columns except 'col3' and 'col4'\n",
        "cols_to_exclude = ['id', 'BDI_score']\n",
        "X = cosX.loc[:, ~cosX.columns.isin(cols_to_exclude)]\n",
        "\n",
        "y= df_concatenated['BDI_score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-THPt4kEh4E"
      },
      "outputs": [],
      "source": [
        "# Convert all columns to numeric, raising errors on non-numeric values\n",
        "X = X.apply(pd.to_numeric, errors='raise')\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0nr9EdNEh4F"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an SVM model and fit it to the training data\n",
        "svm = SVR(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Use the model to make predictions on the test data\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Compute R^2 and RMSE\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print('R^2:', r2)\n",
        "print('RMSE:', rmse)\n",
        "\n",
        "#just with cosine\n",
        "# R^2: -0.39951795615041186\n",
        "# RMSE: 13.76434937057281\n",
        "\n",
        "#with just euclidean\n",
        "# R^2:\n",
        "# RMSE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avJUy659Eh4H"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Create a table comparing actual and predicted scores\n",
        "comparison_table = pd.DataFrame({'Actual BDI Score': y_test, 'Predicted BDI Score': y_pred})\n",
        "table = tabulate(comparison_table, headers='keys', tablefmt='psql')\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNrLOjz4Eh4I"
      },
      "source": [
        "modify the size and color an size of the points based on Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEaozwZzEh4J"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_test_series = np.array(y_test)\n",
        "subset_data = data.loc[pd.DataFrame(y_test).index]\n",
        "\n",
        "# Plot actual and predicted scores with size variation\n",
        "plt.scatter(y_test, y_pred,  alpha=0.5) #s=subset_data['length_seconds']/60\n",
        "plt.plot([min(y_test_series), max(y_test_series)], [min(y_test_series), max(y_test_series)], color='red', linestyle='--')\n",
        "# plt.colorbar(label='length_seconds')\n",
        "plt.xlabel('Actual BDI Score')\n",
        "plt.ylabel('Predicted BDI Score')\n",
        "plt.title('Actual vs Predicted BDI Score with euclidean')\n",
        "plt.show();\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CEliXE8c2sCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#14. prediction : Linear regression"
      ],
      "metadata": {
        "id": "TDHkhg-f2sgA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqrnSalL2sgB"
      },
      "source": [
        "#prediction on 28*28 sentence based\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## euclidean - labse (Updated)"
      ],
      "metadata": {
        "id": "XSB5vEQt2sgC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOgv3tQ92sgC"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0mIh5GK2sgD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "# from transformers import AutoTokenizer, AutoModel\n",
        "from torch import nn\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5Q1NFoq2sgE"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "# data = pd.read_csv('/content/gdrive/MyDrive/28x.csv')\n",
        "data = pd.read_csv('gdrive/MyDrive/28x.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TQ0mmXI2sgF"
      },
      "outputs": [],
      "source": [
        "# groupbyid_df = pd.read_csv('/content/gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd_ComputedLength_MergedTranscripted_all.csv')\n",
        "groupbyid_df = pd.read_csv('gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd_ComputedLength_MergedTranscripted_all.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNj8EYN-2sgF"
      },
      "outputs": [],
      "source": [
        "merg = pd.merge(groupbyid_df[['id', 'BDI_score']], data, on=['id']).drop(['Unnamed: 0'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pwJ5_kz2sgG"
      },
      "outputs": [],
      "source": [
        "grouped = merg.groupby('id')\n",
        "df = pd.DataFrame(grouped.first())\n",
        "df= df.reset_index()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgDBj-nm2sgH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Split euclidean values into lists of numbers\n",
        "df['28xeuclidean'] = df['28xeuclidean'].str.split()\n",
        "\n",
        "# Apply pd.Series to euclidean column\n",
        "df_euclidean = df['28xeuclidean'].apply(pd.Series)\n",
        "\n",
        "# Rename columns\n",
        "df_euclidean.columns = ['euclidean_{}'.format(i+1) for i in range(len(df_euclidean.columns))]\n",
        "\n",
        "# Concatenate original DataFrame and new euclidean DataFrame\n",
        "df_concatenated = pd.concat([df[['id', 'BDI_score']], df_euclidean], axis=1)\n",
        "\n",
        "\n",
        "# Apply str.replace() only to string columns\n",
        "for col in df_concatenated.columns:\n",
        "    if df_concatenated[col].dtype == 'object':\n",
        "        df_concatenated[col] = df_concatenated[col].str.replace('[','').str.replace(']','')\n",
        "df_concatenated = df_concatenated.drop(['euclidean_29'],axis=1)\n",
        "df_concatenated;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-vO6huq2sgJ"
      },
      "outputs": [],
      "source": [
        "# replace empty strings with NaN and drop rows containing NaN\n",
        "df_concatenated.replace('', np.nan, inplace=True)\n",
        "df_concatenated.dropna(inplace=True)\n",
        "\n",
        "# print the updated dataframe\n",
        "df_concatenated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Xnh6Gq_2sgJ"
      },
      "outputs": [],
      "source": [
        "# select all columns except 'col3' and 'col4'\n",
        "cols_to_exclude = ['id', 'BDI_score']\n",
        "X = df_concatenated.loc[:, ~df_concatenated.columns.isin(cols_to_exclude)]\n",
        "\n",
        "y= df_concatenated['BDI_score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SS5qeZ9z2sgK"
      },
      "outputs": [],
      "source": [
        "# Convert all columns to numeric, raising errors on non-numeric values\n",
        "X = X.apply(pd.to_numeric, errors='raise')\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ifnmu4uD2sgL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an SVM model and fit it to the training data\n",
        "# svm = SVR(kernel='linear')\n",
        "# svm.fit(X_train, y_train)\n",
        "\n",
        "# # Use the model to make predictions on the test data\n",
        "# y_pred = svm.predict(X_test)\n",
        "\n",
        "# # Compute R^2 and RMSE\n",
        "\n",
        "\n",
        "from sklearn import linear_model\n",
        "\n",
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(X, y)\n",
        "y_pred = reg.predict(X)\n",
        "\n",
        "r2 = r2_score(y, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
        "\n",
        "print('R^2:', r2)\n",
        "print('RMSE:', rmse)\n",
        "\n",
        "#just with cosine\n",
        "# R^2: 0.09913889255840802\n",
        "# RMSE: 11.043210965336455\n",
        "\n",
        "#with just euclidean\n",
        "# R^2: 0.23771517212190119\n",
        "# RMSE: 8.960692041903147\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_WJgV6V2sgL"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Create a table comparing actual and predicted scores\n",
        "comparison_table = pd.DataFrame({'Actual BDI Score': y, 'Predicted BDI Score': y_pred})\n",
        "table = tabulate(comparison_table, headers='keys', tablefmt='psql')\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TilVND9D2sgM"
      },
      "source": [
        "modify the size and color an size of the points based on Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRwZMlGu2sgN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_test_series = np.array(y)\n",
        "subset_data = data.loc[pd.DataFrame(y).index]\n",
        "\n",
        "# Plot actual and predicted scores with size variation\n",
        "plt.scatter(y, y_pred,  alpha=0.5) #s=subset_data['length_seconds']/60\n",
        "plt.plot([min(y_test_series), max(y_test_series)], [min(y_test_series), max(y_test_series)], color='red', linestyle='--')\n",
        "# plt.colorbar(label='length_seconds')\n",
        "plt.xlabel('Actual BDI Score')\n",
        "plt.ylabel('Predicted BDI Score')\n",
        "plt.title('Actual vs Predicted BDI Score with eucl')\n",
        "plt.show();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjpneHyR2sgO"
      },
      "source": [
        "## cosine - labse (Updated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UY72rNAv2sgO"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-KQvHWk2sgO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "# from transformers import AutoTokenizer, AutoModel\n",
        "from torch import nn\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAcD-qEd2sgP"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/28x.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoYSMYOX2sgP"
      },
      "outputs": [],
      "source": [
        "groupbyid_df = pd.read_csv('/content/gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd_ComputedLength_MergedTranscripted_all.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGW2uZXL2sgQ"
      },
      "outputs": [],
      "source": [
        "merg = pd.merge(groupbyid_df[['id', 'BDI_score']], data, on=['id']).drop(['Unnamed: 0'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YF0igrr62sgQ"
      },
      "outputs": [],
      "source": [
        "grouped = merg.groupby('id')\n",
        "df = pd.DataFrame(grouped.first())\n",
        "df= df.reset_index()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1_QEU482sgR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Split euclidean values into lists of numbers\n",
        "df['28xcosine'] = df['28xcosine'].str.split()\n",
        "\n",
        "# Apply pd.Series to euclidean column\n",
        "df_euclidean = df['28xcosine'].apply(pd.Series)\n",
        "\n",
        "# Rename columns\n",
        "df_euclidean.columns = ['cosine_{}'.format(i+1) for i in range(len(df_euclidean.columns))]\n",
        "\n",
        "# Concatenate original DataFrame and new euclidean DataFrame\n",
        "df_concatenated = pd.concat([df_concatenated, df_euclidean], axis=1)\n",
        "\n",
        "# Apply str.replace() only to string columns\n",
        "for col in df_concatenated.columns:\n",
        "    if df_concatenated[col].dtype == 'object':\n",
        "        df_concatenated[col] = df_concatenated[col].str.replace('[','').str.replace(']','')\n",
        "df_concatenated = df_concatenated.drop(['cosine_29','cosine_30'],axis=1)\n",
        "df_concatenated;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ugs35pe2sgR"
      },
      "outputs": [],
      "source": [
        "# replace empty strings with NaN and drop rows containing NaN\n",
        "df_concatenated.replace('', np.nan, inplace=True)\n",
        "df_concatenated.dropna(inplace=True)\n",
        "\n",
        "# print the updated dataframe\n",
        "df_concatenated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU-BxHzP2sgU"
      },
      "outputs": [],
      "source": [
        "# select all columns except 'col3' and 'col4'\n",
        "cols_to_exclude = ['id', 'BDI_score']\n",
        "X = df_concatenated.loc[:, ~df_concatenated.columns.isin(cols_to_exclude)]\n",
        "\n",
        "y= df_concatenated['BDI_score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Udn6UoJO2sgV"
      },
      "outputs": [],
      "source": [
        "# Convert all columns to numeric, raising errors on non-numeric values\n",
        "X = X.apply(pd.to_numeric, errors='raise')\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFGhMQMl2sgV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# # Create an SVM model and fit it to the training data\n",
        "# svm = SVR(kernel='linear')\n",
        "# svm.fit(X_train, y_train)\n",
        "\n",
        "# # Use the model to make predictions on the test data\n",
        "# y_pred = svm.predict(X_test)\n",
        "\n",
        "# # Compute R^2 and RMSE\n",
        "# r2 = r2_score(y_test, y_pred)\n",
        "# rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "# print('R^2:', r2)\n",
        "# print('RMSE:', rmse)\n",
        "\n",
        "\n",
        "from sklearn import linear_model\n",
        "\n",
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(X, y)\n",
        "y_pred = reg.predict(X)\n",
        "\n",
        "r2 = r2_score(y, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
        "\n",
        "print('R^2:', r2)\n",
        "print('RMSE:', rmse)\n",
        "\n",
        "#just with cosine\n",
        "# R^2: 0.09913889255840802\n",
        "# RMSE: 11.043210965336455\n",
        "\n",
        "#with just euclidean\n",
        "# R^2: 0.23771517212190119\n",
        "# RMSE: 8.960692041903147"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyf8Jd7N2sgX"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Create a table comparing actual and predicted scores\n",
        "comparison_table = pd.DataFrame({'Actual BDI Score': y, 'Predicted BDI Score': y_pred})\n",
        "table = tabulate(comparison_table, headers='keys', tablefmt='psql')\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzmzqQnt2sgX"
      },
      "source": [
        "modify the size and color an size of the points based on Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XO3wLat2sgX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_test_series = np.array(y)\n",
        "subset_data = data.loc[pd.DataFrame(y).index]\n",
        "\n",
        "# Plot actual and predicted scores with size variation\n",
        "plt.scatter(y, y_pred,  alpha=0.5) #s=subset_data['length_seconds']/60\n",
        "plt.plot([min(y_test_series), max(y_test_series)], [min(y_test_series), max(y_test_series)], color='red', linestyle='--')\n",
        "# plt.colorbar(label='length_seconds')\n",
        "plt.xlabel('Actual BDI Score')\n",
        "plt.ylabel('Predicted BDI Score')\n",
        "plt.title('Actual vs Predicted BDI Score with cos')\n",
        "plt.show();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRX9NHK02sgY"
      },
      "source": [
        "# prediction on 28*28 tocken based"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## euclidean - tf-idf"
      ],
      "metadata": {
        "id": "sy_1Mvzv2sgY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th9-UzTT2sgZ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8hfD6F52sgZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "# from transformers import AutoTokenizer, AutoModel\n",
        "from torch import nn\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlL2nqnp2sgZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/28x-tfidf.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2DUqg1m2sga"
      },
      "outputs": [],
      "source": [
        "groupbyid_df = pd.read_csv('/content/gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd_ComputedLength_MergedTranscripted_all.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8stD8nY2sgb"
      },
      "outputs": [],
      "source": [
        "merg = pd.merge(groupbyid_df[['id', 'BDI_score']], data, on=['id']).drop(['Unnamed: 0'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_GcgDP02sgb"
      },
      "outputs": [],
      "source": [
        "grouped = merg.groupby('id')\n",
        "df = pd.DataFrame(grouped.first())\n",
        "df= df.reset_index()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUnborND2sge"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Split euclidean values into lists of numbers\n",
        "df['28xeuclidean'] = df['28xeuclidean'].str.split()\n",
        "\n",
        "# Apply pd.Series to euclidean column\n",
        "df_euclidean = df['28xeuclidean'].apply(pd.Series)\n",
        "\n",
        "# Rename columns\n",
        "df_euclidean.columns = ['euclidean_{}'.format(i+1) for i in range(len(df_euclidean.columns))]\n",
        "\n",
        "# Concatenate original DataFrame and new euclidean DataFrame\n",
        "df_concatenated = pd.concat([df[['id', 'BDI_score']], df_euclidean], axis=1)\n",
        "\n",
        "\n",
        "# Apply str.replace() only to string columns\n",
        "for col in df_concatenated.columns:\n",
        "    if df_concatenated[col].dtype == 'object':\n",
        "        df_concatenated[col] = df_concatenated[col].str.replace('[','').str.replace(']','')\n",
        "df_concatenated = df_concatenated.drop(['euclidean_29'],axis=1)\n",
        "df_concatenated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30gSYzqv2sgf"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # Split euclidean values into lists of numbers\n",
        "# df['28xcosine'] = df['28xcosine'].str.split()\n",
        "\n",
        "# # Apply pd.Series to euclidean column\n",
        "# df_euclidean = df['28xcosine'].apply(pd.Series)\n",
        "\n",
        "# # Rename columns\n",
        "# df_euclidean.columns = ['cosine_{}'.format(i+1) for i in range(len(df_euclidean.columns))]\n",
        "\n",
        "# # Concatenate original DataFrame and new euclidean DataFrame\n",
        "# df_concatenated = pd.concat([df_concatenated, df_euclidean], axis=1)\n",
        "\n",
        "# # Apply str.replace() only to string columns\n",
        "# for col in df_concatenated.columns:\n",
        "#     if df_concatenated[col].dtype == 'object':\n",
        "#         df_concatenated[col] = df_concatenated[col].str.replace('[','').str.replace(']','')\n",
        "# df_concatenated = df_concatenated.drop(['cosine_29','cosine_30'],axis=1)\n",
        "# df_concatenated;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc0QNC-R2sgg"
      },
      "outputs": [],
      "source": [
        "# replace empty strings with NaN and drop rows containing NaN\n",
        "df_concatenated.replace('', np.nan, inplace=True)\n",
        "df_concatenated.dropna(inplace=True)\n",
        "\n",
        "# print the updated dataframe\n",
        "df_concatenated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1bD106i2sgh"
      },
      "outputs": [],
      "source": [
        "# select all columns except 'col3' and 'col4'\n",
        "cols_to_exclude = ['id', 'BDI_score']\n",
        "X = df_concatenated.loc[:, ~df_concatenated.columns.isin(cols_to_exclude)]\n",
        "\n",
        "y= df_concatenated['BDI_score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlcJUCNH2sgh"
      },
      "outputs": [],
      "source": [
        "# Convert all columns to numeric, raising errors on non-numeric values\n",
        "X = X.apply(pd.to_numeric, errors='raise')\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwGFeDsO2sgi"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# # Create an SVM model and fit it to the training data\n",
        "# svm = SVR(kernel='linear')\n",
        "# svm.fit(X_train, y_train)\n",
        "\n",
        "# # Use the model to make predictions on the test data\n",
        "# y_pred = svm.predict(X_test)\n",
        "\n",
        "# # Compute R^2 and RMSE\n",
        "# r2 = r2_score(y_test, y_pred)\n",
        "# rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "# print('R^2:', r2)\n",
        "# print('RMSE:', rmse)\n",
        "\n",
        "from sklearn import linear_model\n",
        "\n",
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(X, y)\n",
        "y_pred = reg.predict(X)\n",
        "\n",
        "r2 = r2_score(y, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
        "\n",
        "print('R^2:', r2)\n",
        "print('RMSE:', rmse)\n",
        "\n",
        "#just with cosine\n",
        "# R^2: 0.09913889255840802\n",
        "# RMSE: 11.043210965336455\n",
        "\n",
        "#with just euclidean\n",
        "# R^2: 0.23771517212190119\n",
        "# RMSE: 8.960692041903147"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0X5W74E-2sgn"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Create a table comparing actual and predicted scores\n",
        "comparison_table = pd.DataFrame({'Actual BDI Score': y, 'Predicted BDI Score': y_pred})\n",
        "table = tabulate(comparison_table, headers='keys', tablefmt='psql')\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KgHAJ6o2sgo"
      },
      "source": [
        "modify the size and color an size of the points based on Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkws9ehl2sgo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_test_series = np.array(y)\n",
        "subset_data = data.loc[pd.DataFrame(y).index]\n",
        "\n",
        "# Plot actual and predicted scores with size variation\n",
        "plt.scatter(y, y_pred,  alpha=0.5) #s=subset_data['length_seconds']/60\n",
        "plt.plot([min(y_test_series), max(y_test_series)], [min(y_test_series), max(y_test_series)], color='red', linestyle='--')\n",
        "# plt.colorbar(label='length_seconds')\n",
        "plt.xlabel('Actual BDI Score')\n",
        "plt.ylabel('Predicted BDI Score')\n",
        "plt.title('Actual vs Predicted BDI Score with eucl')\n",
        "plt.show();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqt9B3qD2sgp"
      },
      "source": [
        "## cosine - tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jwa4yt1t2sgp"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtImD18E2sgp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "# from transformers import AutoTokenizer, AutoModel\n",
        "from torch import nn\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2y-D2nq32sgq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/28x-tfidf.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMG3hcIq2sgq"
      },
      "outputs": [],
      "source": [
        "groupbyid_df = pd.read_csv('/content/gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd_ComputedLength_MergedTranscripted_all.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X78eLx4a2sgq"
      },
      "outputs": [],
      "source": [
        "merg = pd.merge(groupbyid_df[['id', 'BDI_score']], data, on=['id']).drop(['Unnamed: 0'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrLr9y7J2sgq"
      },
      "outputs": [],
      "source": [
        "grouped = merg.groupby('id')\n",
        "df = pd.DataFrame(grouped.first())\n",
        "df= df.reset_index()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Split euclidean values into lists of numbers\n",
        "df['28xcosine'] = df['28xcosine'].str.replace('\\n', '')\n",
        "df['28xcosine'] = df['28xcosine'].str.replace('[', '')\n",
        "df['28xcosine'] = df['28xcosine'].str.replace(']', '')\n",
        "\n",
        "df['28xcosine'] = df['28xcosine'].str.split()\n",
        "\n",
        "# Apply pd.Series to euclidean column\n",
        "df_euclidean = df['28xcosine'].apply(pd.Series)\n",
        "\n",
        "# Rename columns\n",
        "df_euclidean.columns = ['cosine_{}'.format(i+1) for i in range(len(df_euclidean.columns))]\n",
        "\n",
        "# Concatenate original DataFrame and new euclidean DataFrame\n",
        "df_concatenated = pd.concat([df_concatenated, df_euclidean], axis=1)\n",
        "df_concatenated"
      ],
      "metadata": {
        "id": "aXb_jkOr2sgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddbYmGhi2sgr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Apply str.replace() only to string columns\n",
        "# for col in df_concatenated.columns:\n",
        "#     if df_concatenated[col].dtype == 'object':\n",
        "#         df_concatenated[col] = df_concatenated[col].str.replace('[','').str.replace(']','')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CPy-ao82sgr"
      },
      "outputs": [],
      "source": [
        "# replace empty strings with NaN and drop rows containing NaN\n",
        "df_concatenated.replace('', np.nan, inplace=True)\n",
        "df_concatenated.dropna(inplace=True)\n",
        "\n",
        "# print the updated dataframe\n",
        "df_concatenated\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_concatenated = df_concatenated.drop(['cosine_28','cosine_1'],axis=1)\n",
        "df_concatenated"
      ],
      "metadata": {
        "id": "mVReWqpE7eDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELE62jpP2sgs"
      },
      "outputs": [],
      "source": [
        "# select all columns except 'col3' and 'col4'\n",
        "cols_to_exclude = ['id', 'BDI_score']\n",
        "X = df_concatenated.loc[:, ~df_concatenated.columns.isin(cols_to_exclude)]\n",
        "\n",
        "y= df['BDI_score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PoBlefI2sgs"
      },
      "outputs": [],
      "source": [
        "# Convert all columns to numeric, raising errors on non-numeric values\n",
        "# X = X.apply(pd.to_numeric, errors='raise')\n",
        "# X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "725exwfB2sgv"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Create an SVM model and fit it to the training data\n",
        "# svm = SVR(kernel='linear')\n",
        "# svm.fit(X_train, y_train)\n",
        "\n",
        "# # Use the model to make predictions on the test data\n",
        "# y_pred = svm.predict(X_test)\n",
        "\n",
        "# # Compute R^2 and RMSE\n",
        "# r2 = r2_score(y_test, y_pred)\n",
        "# rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "# print('R^2:', r2)\n",
        "# print('RMSE:', rmse)\n",
        "\n",
        "from sklearn import linear_model\n",
        "\n",
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(X, y)\n",
        "y_pred = reg.predict(X)\n",
        "\n",
        "r2 = r2_score(y, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
        "\n",
        "print('R^2:', r2)\n",
        "print('RMSE:', rmse)\n",
        "\n",
        "#just with cosine\n",
        "# R^2: 0.09913889255840802\n",
        "# RMSE: 11.043210965336455\n",
        "\n",
        "#with just euclidean\n",
        "# R^2: 0.23771517212190119\n",
        "# RMSE: 8.960692041903147"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFaVsXVm2sgw"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Create a table comparing actual and predicted scores\n",
        "comparison_table = pd.DataFrame({'Actual BDI Score': y, 'Predicted BDI Score': y_pred})\n",
        "table = tabulate(comparison_table, headers='keys', tablefmt='psql')\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sPJj9aX2sgw"
      },
      "source": [
        "modify the size and color an size of the points based on Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKYGCbeO2sgw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_test_series = np.array(y)\n",
        "subset_data = data.loc[pd.DataFrame(y).index]\n",
        "\n",
        "# Plot actual and predicted scores with size variation\n",
        "plt.scatter(y, y_pred,  alpha=0.5) #s=subset_data['length_seconds']/60\n",
        "plt.plot([min(y_test_series), max(y_test_series)], [min(y_test_series), max(y_test_series)], color='red', linestyle='--')\n",
        "# plt.colorbar(label='length_seconds')\n",
        "plt.xlabel('Actual BDI Score')\n",
        "plt.ylabel('Predicted BDI Score')\n",
        "plt.title('Actual vs Predicted BDI Score with cos')\n",
        "plt.show();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZpVH-4Q2sgx"
      },
      "source": [
        "#prediction on 28*28 mean word based\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## euclidean - parsbert"
      ],
      "metadata": {
        "id": "eKjWNcqp2sgx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e01cn8R02sgx"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-UAqm8m2sgx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "# from transformers import AutoTokenizer, AutoModel\n",
        "from torch import nn\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sY-uH4A2sgy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/28parsbertx.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOHFz9e72sgy"
      },
      "outputs": [],
      "source": [
        "groupbyid_df = pd.read_csv('/content/gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd_ComputedLength_MergedTranscripted_all.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUbAVDmC2sgy"
      },
      "outputs": [],
      "source": [
        "merg = pd.merge(groupbyid_df[['id', 'BDI_score']], data, on=['id']).drop(['Unnamed: 0'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThJbIjeQ2sgy"
      },
      "outputs": [],
      "source": [
        "grouped = merg.groupby('id')\n",
        "df = pd.DataFrame(grouped.first())\n",
        "df= df.reset_index()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvcURE212sgz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Split euclidean values into lists of numbers\n",
        "df['28xeuclidean'] = df['28xeuclidean'].str.split()\n",
        "\n",
        "# Apply pd.Series to euclidean column\n",
        "df_euclidean = df['28xeuclidean'].apply(pd.Series)\n",
        "\n",
        "# Rename columns\n",
        "df_euclidean.columns = ['euclidean_{}'.format(i+1) for i in range(len(df_euclidean.columns))]\n",
        "\n",
        "# Concatenate original DataFrame and new euclidean DataFrame\n",
        "df_concatenated = pd.concat([df[['id', 'BDI_score']], df_euclidean], axis=1)\n",
        "\n",
        "\n",
        "# Apply str.replace() only to string columns\n",
        "for col in df_concatenated.columns:\n",
        "    if df_concatenated[col].dtype == 'object':\n",
        "        df_concatenated[col] = df_concatenated[col].str.replace('[','').str.replace(']','')\n",
        "df_concatenated = df_concatenated.drop(['euclidean_29'],axis=1)\n",
        "df_concatenated;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfZgX4iE2sgz"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # Split euclidean values into lists of numbers\n",
        "# df['28xcosine'] = df['28xcosine'].str.split()\n",
        "\n",
        "# # Apply pd.Series to euclidean column\n",
        "# df_euclidean = df['28xcosine'].apply(pd.Series)\n",
        "\n",
        "# # Rename columns\n",
        "# df_euclidean.columns = ['cosine_{}'.format(i+1) for i in range(len(df_euclidean.columns))]\n",
        "\n",
        "# # Concatenate original DataFrame and new euclidean DataFrame\n",
        "# df_concatenated = pd.concat([df_concatenated, df_euclidean], axis=1)\n",
        "\n",
        "# # Apply str.replace() only to string columns\n",
        "# for col in df_concatenated.columns:\n",
        "#     if df_concatenated[col].dtype == 'object':\n",
        "#         df_concatenated[col] = df_concatenated[col].str.replace('[','').str.replace(']','')\n",
        "# df_concatenated = df_concatenated.drop(['cosine_29','cosine_30'],axis=1)\n",
        "# df_concatenated;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDiai_-y2sgz"
      },
      "outputs": [],
      "source": [
        "# replace empty strings with NaN and drop rows containing NaN\n",
        "df_concatenated.replace('', np.nan, inplace=True)\n",
        "df_concatenated.dropna(inplace=True)\n",
        "\n",
        "# print the updated dataframe\n",
        "df_concatenated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeLoIJOK2sg0"
      },
      "outputs": [],
      "source": [
        "# select all columns except 'col3' and 'col4'\n",
        "cols_to_exclude = ['id', 'BDI_score']\n",
        "X = df_concatenated.loc[:, ~df_concatenated.columns.isin(cols_to_exclude)]\n",
        "\n",
        "y= df_concatenated['BDI_score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjeyIAtb2sg0"
      },
      "outputs": [],
      "source": [
        "# Convert all columns to numeric, raising errors on non-numeric values\n",
        "X = X.apply(pd.to_numeric, errors='raise')\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkmCDDg72sg0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "# # Create an SVM model and fit it to the training data\n",
        "# svm = SVR(kernel='linear')\n",
        "# svm.fit(X_train, y_train)\n",
        "\n",
        "# # Use the model to make predictions on the test data\n",
        "# y_pred = svm.predict(X_test)\n",
        "\n",
        "# # Compute R^2 and RMSE\n",
        "# r2 = r2_score(y_test, y_pred)\n",
        "# rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "# print('R^2:', r2)\n",
        "# print('RMSE:', rmse)\n",
        "\n",
        "from sklearn import linear_model\n",
        "\n",
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(X, y)\n",
        "y_pred = reg.predict(X)\n",
        "\n",
        "r2 = r2_score(y, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
        "\n",
        "print('R^2:', r2)\n",
        "print('RMSE:', rmse)\n",
        "\n",
        "#just with cosine\n",
        "# R^2:\n",
        "# RMSE:\n",
        "\n",
        "#with just euclidean\n",
        "# R^2:-0.011684696294692376\n",
        "# RMSE: 9.803214807389821"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F5gn7Ga2sg1"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Create a table comparing actual and predicted scores\n",
        "comparison_table = pd.DataFrame({'Actual BDI Score': y, 'Predicted BDI Score': y_pred})\n",
        "table = tabulate(comparison_table, headers='keys', tablefmt='psql')\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lViyXRtH2sg1"
      },
      "source": [
        "modify the size and color an size of the points based on Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkGJBxW12sg1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_test_series = np.array(y)\n",
        "subset_data = data.loc[pd.DataFrame(y).index]\n",
        "\n",
        "# Plot actual and predicted scores with size variation\n",
        "plt.scatter(y, y_pred,  alpha=0.5) #s=subset_data['length_seconds']/60\n",
        "plt.plot([min(y_test_series), max(y_test_series)], [min(y_test_series), max(y_test_series)], color='red', linestyle='--')\n",
        "# plt.colorbar(label='length_seconds')\n",
        "plt.xlabel('Actual BDI Score')\n",
        "plt.ylabel('Predicted BDI Score')\n",
        "plt.title('Actual vs Predicted BDI Score with eucl')\n",
        "plt.show();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz5mK69K2sg2"
      },
      "source": [
        "## cosine - parsbert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIRDPusZ2sg2"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmGESJ8u2sg2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "# from transformers import AutoTokenizer, AutoModel\n",
        "from torch import nn\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDOZuRyb2sg3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/28parsbertx.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RrJNjOP2sg3"
      },
      "outputs": [],
      "source": [
        "groupbyid_df = pd.read_csv('/content/gdrive/MyDrive/project_dataset_with_BDIscores_categoriesd_ComputedLength_MergedTranscripted_all.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOTbx-K02sg3"
      },
      "outputs": [],
      "source": [
        "merg = pd.merge(groupbyid_df[['id', 'BDI_score']], data, on=['id']).drop(['Unnamed: 0'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APsb6UmE2sg3"
      },
      "outputs": [],
      "source": [
        "grouped = merg.groupby('id')\n",
        "df = pd.DataFrame(grouped.first())\n",
        "df= df.reset_index()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCC3Fd172sg4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Split euclidean values into lists of numbers\n",
        "df['28xcosine'] = df['28xcosine'].str.split()\n",
        "\n",
        "# Apply pd.Series to euclidean column\n",
        "df_euclidean = df['28xcosine'].apply(pd.Series)\n",
        "\n",
        "# Rename columns\n",
        "df_euclidean.columns = ['cosine_{}'.format(i+1) for i in range(len(df_euclidean.columns))]\n",
        "\n",
        "# Concatenate original DataFrame and new euclidean DataFrame\n",
        "df_concatenated = pd.concat([df_concatenated, df_euclidean], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Apply str.replace() only to string columns\n",
        "# for col in df_concatenated.columns:\n",
        "#     if df_concatenated[col].dtype == 'object':\n",
        "#         df_concatenated[col] = df_concatenated[col].str.replace('[','').str.replace(']','')\n",
        "# df_concatenated = df_concatenated.drop(['cosine_29'],axis=1)\n",
        "\n",
        "df_concatenated = df_concatenated.drop(['cosine_28','cosine_29','cosine_1'],axis=1)\n",
        "df_concatenated"
      ],
      "metadata": {
        "id": "mxfQuuKm2sg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfhVbSNB2sg4"
      },
      "outputs": [],
      "source": [
        "# replace empty strings with NaN and drop rows containing NaN\n",
        "df_concatenated.replace('', np.nan, inplace=True)\n",
        "df_concatenated.dropna(inplace=True)\n",
        "\n",
        "# print the updated dataframe\n",
        "df_concatenated\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosX = df_concatenated.iloc[:,-26:]"
      ],
      "metadata": {
        "id": "QlPvCLIg2sg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmL3FMi02sg5"
      },
      "outputs": [],
      "source": [
        "# select all columns except 'col3' and 'col4'\n",
        "cols_to_exclude = ['id', 'BDI_score']\n",
        "X = cosX.loc[:, ~cosX.columns.isin(cols_to_exclude)]\n",
        "\n",
        "y= df_concatenated['BDI_score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfqXg3N72sg5"
      },
      "outputs": [],
      "source": [
        "# Convert all columns to numeric, raising errors on non-numeric values\n",
        "X = X.apply(pd.to_numeric, errors='raise')\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HX1QNoOj2sg5"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Create an SVM model and fit it to the training data\n",
        "# svm = SVR(kernel='linear')\n",
        "# svm.fit(X_train, y_train)\n",
        "\n",
        "# # Use the model to make predictions on the test data\n",
        "# y_pred = svm.predict(X_test)\n",
        "\n",
        "# # Compute R^2 and RMSE\n",
        "# r2 = r2_score(y_test, y_pred)\n",
        "# rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "# print('R^2:', r2)\n",
        "# print('RMSE:', rmse)\n",
        "\n",
        "from sklearn import linear_model\n",
        "\n",
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(X, y)\n",
        "y_pred = reg.predict(X)\n",
        "\n",
        "r2 = r2_score(y, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
        "\n",
        "print('R^2:', r2)\n",
        "print('RMSE:', rmse)\n",
        "\n",
        "#just with cosine\n",
        "# R^2: -0.39951795615041186\n",
        "# RMSE: 13.76434937057281\n",
        "\n",
        "#with just euclidean\n",
        "# R^2:\n",
        "# RMSE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv8qPHQ-2sg6"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Create a table comparing actual and predicted scores\n",
        "comparison_table = pd.DataFrame({'Actual BDI Score': y, 'Predicted BDI Score': y_pred})\n",
        "table = tabulate(comparison_table, headers='keys', tablefmt='psql')\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbBIsOqs2sg6"
      },
      "source": [
        "modify the size and color an size of the points based on Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XD49orwd2sg6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_test_series = np.array(y)\n",
        "subset_data = data.loc[pd.DataFrame(y).index]\n",
        "\n",
        "# Plot actual and predicted scores with size variation\n",
        "plt.scatter(y, y_pred,  alpha=0.5) #s=subset_data['length_seconds']/60\n",
        "plt.plot([min(y_test_series), max(y_test_series)], [min(y_test_series), max(y_test_series)], color='red', linestyle='--')\n",
        "# plt.colorbar(label='length_seconds')\n",
        "plt.xlabel('Actual BDI Score')\n",
        "plt.ylabel('Predicted BDI Score')\n",
        "plt.title('Actual vs Predicted BDI Score with cos')\n",
        "plt.show();\n"
      ]
    }
  ]
}